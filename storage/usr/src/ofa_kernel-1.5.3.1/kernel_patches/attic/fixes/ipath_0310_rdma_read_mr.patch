diff -up a/drivers/infiniband/hw/ipath/ipath_keys.c b/drivers/infiniband/hw/ipath/ipath_keys.c
--- a/drivers/infiniband/hw/ipath/ipath_keys.c	2009-03-05 17:37:11.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_keys.c	2009-03-06 16:04:19.000000000 -0800
@@ -217,20 +217,17 @@ bail:
  *
  * Return 1 if successful, otherwise 0.
  */
-int ipath_rkey_ok(struct ipath_qp *qp, struct ipath_sge_state *ss,
+int ipath_rkey_ok(struct ipath_qp *qp, struct ipath_sge *sge,
 		  u32 len, u64 vaddr, u32 rkey, int acc)
 {
 	struct ipath_ibdev *dev = to_idev(qp->ibqp.device);
 	struct ipath_lkey_table *rkt = &dev->lk_table;
-	struct ipath_sge *sge = &ss->sge;
 	struct ipath_mregion *mr;
 	unsigned n, m;
 	size_t off;
 	int ret = 0;
 	unsigned long flags;
 
-	ss->num_sge = 0;
-
 	/*
 	 * We use RKEY == zero for kernel virtual addresses
 	 * (see ipath_get_dma_mr and ipath_dma.c).
@@ -251,8 +248,6 @@ int ipath_rkey_ok(struct ipath_qp *qp, s
 		sge->sge_length = len;
 		sge->m = 0;
 		sge->n = 0;
-		ss->sg_list = NULL;
-		ss->num_sge = 1;
 		goto ok;
 	}
 
@@ -284,8 +279,6 @@ int ipath_rkey_ok(struct ipath_qp *qp, s
 	sge->sge_length = len;
 	sge->m = m;
 	sge->n = n;
-	ss->sg_list = NULL;
-	ss->num_sge = 1;
 ok:
 	ret = 1;
 bail:
diff -up a/drivers/infiniband/hw/ipath/ipath_qp.c b/drivers/infiniband/hw/ipath/ipath_qp.c
--- a/drivers/infiniband/hw/ipath/ipath_qp.c	2009-03-05 17:37:22.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_qp.c	2009-03-09 15:41:46.000000000 -0700
@@ -390,26 +390,23 @@ static void clear_mr_refs(struct ipath_q
 			if (++qp->s_last >= qp->s_size)
 				qp->s_last = 0;
 		}
+		if (qp->s_rdma_mr) {
+			atomic_dec(&qp->s_rdma_mr->refcount);
+			qp->s_rdma_mr = NULL;
+		}
 	}
 
 	if (qp->ibqp.qp_type != IB_QPT_RC)
 		return;
 
-	/* XXX Need to be sure that none of these are actively being sent */
 	for (n = 0; n < ARRAY_SIZE(qp->s_ack_queue); n++) {
 		struct ipath_ack_entry *e = &qp->s_ack_queue[n];
-		unsigned i;
-
-		if (e->opcode != IB_OPCODE_RC_RDMA_READ_REQUEST)
-			continue;
-		for (i = 0; i < e->rdma_sge.num_sge; i++) {
-			struct ipath_sge *sge = i ?
-				&e->rdma_sge.sg_list[i - 1] : &e->rdma_sge.sge;
 
-			atomic_dec(&sge->mr->refcount);
+		if (e->opcode == IB_OPCODE_RC_RDMA_READ_REQUEST &&
+		    e->rdma_sge.mr) {
+			atomic_dec(&e->rdma_sge.mr->refcount);
+			e->rdma_sge.mr = NULL;
 		}
-		e->opcode = 0;
-		e->rdma_sge.num_sge = 0;
 	}
 }
 
@@ -855,7 +852,7 @@ struct ib_qp *ipath_create_qp(struct ib_
 		} else if (init_attr->cap.max_recv_sge > 1)
 			sg_list_sz = sizeof(*qp->r_sg_list) *
 				(init_attr->cap.max_recv_sge - 1);
-		qp = kmalloc(sz + sg_list_sz, GFP_KERNEL);
+		qp = kzalloc(sz + sg_list_sz, GFP_KERNEL);
 		if (!qp) {
 			ret = ERR_PTR(-ENOMEM);
 			goto bail_swq;
@@ -870,14 +867,9 @@ struct ib_qp *ipath_create_qp(struct ib_
 			}
 		} else
 			qp->r_ud_sg_list = NULL;
-		if (init_attr->srq) {
+		if (init_attr->srq)
 			sz = 0;
-			qp->r_rq.size = 0;
-			qp->r_rq.max_sge = 0;
-			qp->r_rq.wq = NULL;
-			init_attr->cap.max_recv_wr = 0;
-			init_attr->cap.max_recv_sge = 0;
-		} else {
+		else {
 			qp->r_rq.size = init_attr->cap.max_recv_wr + 1;
 			qp->r_rq.max_sge = init_attr->cap.max_recv_sge;
 			sz = (sizeof(struct ib_sge) * qp->r_rq.max_sge) +
@@ -910,8 +902,6 @@ struct ib_qp *ipath_create_qp(struct ib_
 		qp->s_max_sge = init_attr->cap.max_send_sge;
 		if (init_attr->sq_sig_type == IB_SIGNAL_REQ_WR)
 			qp->s_flags = IPATH_S_SIGNAL_REQ_WR;
-		else
-			qp->s_flags = 0;
 		dev = to_idev(ibpd->device);
 		err = ipath_alloc_qpn(&dev->qp_table, qp,
 				      init_attr->qp_type);
@@ -920,8 +910,6 @@ struct ib_qp *ipath_create_qp(struct ib_
 			vfree(qp->r_rq.wq);
 			goto bail_sg_list;
 		}
-		qp->ip = NULL;
-		qp->s_tx = NULL;
 		ipath_reset_qp(qp, init_attr->qp_type);
 		break;
 
@@ -1040,6 +1028,10 @@ int ipath_destroy_qp(struct ib_qp *ibqp)
 	ipath_free_qp(&dev->qp_table, qp);
 
 	if (qp->s_tx) {
+		if (qp->s_tx->mr) {
+			atomic_dec(&qp->s_tx->mr->refcount);
+			qp->s_tx->mr = NULL;
+		}
 		atomic_dec(&qp->refcount);
 		if (qp->s_tx->txreq.flags & IPATH_SDMA_TXREQ_F_FREEBUF)
 			kfree(qp->s_tx->txreq.map_addr);
diff -up a/drivers/infiniband/hw/ipath/ipath_rc.c b/drivers/infiniband/hw/ipath/ipath_rc.c
--- a/drivers/infiniband/hw/ipath/ipath_rc.c	2009-03-05 17:37:11.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_rc.c	2009-03-09 17:40:39.000000000 -0700
@@ -124,16 +124,31 @@ static int ipath_make_rc_ack(struct ipat
 
 		e = &qp->s_ack_queue[qp->s_tail_ack_queue];
 		if (e->opcode == OP(RDMA_READ_REQUEST)) {
+			/*
+			 * If a RDMA read response is being resent and
+			 * we haven't seen the duplicate request yet,
+			 * then stop sending the remaining responses the
+			 * responder has seen until the requester resends it.
+			 */
+			if (!e->rdma_sge.mr) {
+				qp->s_tail_ack_queue = qp->r_head_ack_queue;
+				qp->s_ack_state = OP(ACKNOWLEDGE);
+				goto bail;
+			}
 			/* Copy SGE state in case we need to resend */
-			qp->s_ack_rdma_sge = e->rdma_sge;
+			qp->s_rdma_mr = e->rdma_sge.mr;
+			qp->s_ack_rdma_sge.sge = e->rdma_sge;
+			qp->s_ack_rdma_sge.num_sge = 1;
 			qp->s_cur_sge = &qp->s_ack_rdma_sge;
-			len = e->rdma_sge.sge.sge_length;
+			len = e->rdma_sge.sge_length;
 			if (len > pmtu) {
 				len = pmtu;
 				qp->s_ack_state = OP(RDMA_READ_RESPONSE_FIRST);
+				atomic_inc(&qp->s_rdma_mr->refcount);
 			} else {
 				qp->s_ack_state = OP(RDMA_READ_RESPONSE_ONLY);
 				e->sent = 1;
+				e->rdma_sge.mr = NULL;
 			}
 			ohdr->u.aeth = ipath_compute_aeth(qp);
 			hwords++;
@@ -160,14 +175,19 @@ static int ipath_make_rc_ack(struct ipat
 		qp->s_ack_state = OP(RDMA_READ_RESPONSE_MIDDLE);
 		/* FALLTHROUGH */
 	case OP(RDMA_READ_RESPONSE_MIDDLE):
+		qp->s_cur_sge = &qp->s_ack_rdma_sge;
+		qp->s_rdma_mr = qp->s_ack_rdma_sge.sge.mr;
 		len = qp->s_ack_rdma_sge.sge.sge_length;
-		if (len > pmtu)
+		if (len > pmtu) {
 			len = pmtu;
-		else {
+			atomic_inc(&qp->s_rdma_mr->refcount);
+		} else {
 			ohdr->u.aeth = ipath_compute_aeth(qp);
 			hwords++;
 			qp->s_ack_state = OP(RDMA_READ_RESPONSE_LAST);
-			qp->s_ack_queue[qp->s_tail_ack_queue].sent = 1;
+			e = &qp->s_ack_queue[qp->s_tail_ack_queue];
+			e->sent = 1;
+			e->rdma_sge.mr = NULL;
 		}
 		bth0 = qp->s_ack_state << 24;
 		bth2 = qp->s_ack_rdma_psn++ & IPATH_PSN_MASK;
@@ -915,13 +935,12 @@ void ipath_rc_send_complete(struct ipath
 		ohdr = &hdr->u.l.oth;
 
 	opcode = be32_to_cpu(ohdr->bth[0]) >> 24;
+	psn = be32_to_cpu(ohdr->bth[2]);
+
 	if (opcode >= OP(RDMA_READ_RESPONSE_FIRST) &&
-	    opcode <= OP(ATOMIC_ACKNOWLEDGE)) {
-		/* XXX Need to handle MR refcount similar to requester */
+	    opcode <= OP(ATOMIC_ACKNOWLEDGE))
 		return;
-	}
 
-	psn = be32_to_cpu(ohdr->bth[2]);
 	reset_sending_psn(qp, psn);
 
 	while (qp->s_last != qp->s_acked) {
@@ -1587,13 +1606,11 @@ static inline int ipath_rc_rcv_error(str
 		offset = ((psn - e->psn) & IPATH_PSN_MASK) *
 			ib_mtu_enum_to_int(qp->path_mtu);
 		len = be32_to_cpu(reth->length);
-		if (unlikely(offset + len > e->rdma_sge.sge.sge_length))
+		if (unlikely(offset + len > e->rdma_sge.sge_length))
 			goto unlock_done;
-		for (i = 0; i < e->rdma_sge.num_sge; i++) {
-			struct ipath_sge *sge = i ?
-				&e->rdma_sge.sg_list[i - 1] : &e->rdma_sge.sge;
-
-			atomic_dec(&sge->mr->refcount);
+		if (e->rdma_sge.mr) {
+			atomic_dec(&e->rdma_sge.mr->refcount);
+			e->rdma_sge.mr = NULL;
 		}
 		if (len != 0) {
 			u32 rkey = be32_to_cpu(reth->rkey);
@@ -1606,12 +1623,9 @@ static inline int ipath_rc_rcv_error(str
 			if (unlikely(!ok))
 				goto unlock_done;
 		} else {
-			e->rdma_sge.sg_list = NULL;
-			e->rdma_sge.num_sge = 0;
-			e->rdma_sge.sge.mr = NULL;
-			e->rdma_sge.sge.vaddr = NULL;
-			e->rdma_sge.sge.length = 0;
-			e->rdma_sge.sge.sge_length = 0;
+			e->rdma_sge.vaddr = NULL;
+			e->rdma_sge.length = 0;
+			e->rdma_sge.sge_length = 0;
 		}
 		e->psn = psn;
 		qp->s_ack_state = OP(ACKNOWLEDGE);
@@ -1705,10 +1719,8 @@ static inline void ipath_update_ack_queu
 	next = n + 1;
 	if (next > IPATH_MAX_RDMA_ATOMIC)
 		next = 0;
-	if (n == qp->s_tail_ack_queue) {
-		qp->s_tail_ack_queue = next;
-		qp->s_ack_state = OP(ACKNOWLEDGE);
-	}
+	qp->s_tail_ack_queue = next;
+	qp->s_ack_state = OP(ACKNOWLEDGE);
 }
 
 /**
@@ -1925,19 +1937,20 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 		hdrsize += sizeof(*reth);
 		qp->r_len = be32_to_cpu(reth->length);
 		qp->r_rcv_len = 0;
+		qp->r_sge.sg_list = NULL;
 		if (qp->r_len != 0) {
 			u32 rkey = be32_to_cpu(reth->rkey);
 			u64 vaddr = be64_to_cpu(reth->vaddr);
 			int ok;
 
 			/* Check rkey & NAK */
-			ok = ipath_rkey_ok(qp, &qp->r_sge,
+			ok = ipath_rkey_ok(qp, &qp->r_sge.sge,
 					   qp->r_len, vaddr, rkey,
 					   IB_ACCESS_REMOTE_WRITE);
 			if (unlikely(!ok))
 				goto nack_acc;
+			qp->r_sge.num_sge = 1;
 		} else {
-			qp->r_sge.sg_list = NULL;
 			qp->r_sge.num_sge = 0;
 			qp->r_sge.sge.mr = NULL;
 			qp->r_sge.sge.vaddr = NULL;
@@ -1973,17 +1986,9 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 			ipath_update_ack_queue(qp, next);
 		}
 		e = &qp->s_ack_queue[qp->r_head_ack_queue];
-		if (e->opcode == OP(RDMA_READ_REQUEST)) {
-			unsigned i;
-
-			for (i = 0; i < e->rdma_sge.num_sge; i++) {
-				struct ipath_sge *sge = i ?
-					&e->rdma_sge.sg_list[i - 1] :
-					&e->rdma_sge.sge;
-
-				atomic_dec(&sge->mr->refcount);
-			}
-			e->opcode = 0;
+		if (e->opcode == OP(RDMA_READ_REQUEST) && e->rdma_sge.mr) {
+			atomic_dec(&e->rdma_sge.mr->refcount);
+			e->rdma_sge.mr = NULL;
 		}
 		/* RETH comes after BTH */
 		if (!header_in_data)
@@ -2010,12 +2015,10 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 			if (len > pmtu)
 				qp->r_psn += (len - 1) / pmtu;
 		} else {
-			e->rdma_sge.sg_list = NULL;
-			e->rdma_sge.num_sge = 0;
-			e->rdma_sge.sge.mr = NULL;
-			e->rdma_sge.sge.vaddr = NULL;
-			e->rdma_sge.sge.length = 0;
-			e->rdma_sge.sge.sge_length = 0;
+			e->rdma_sge.mr = NULL;
+			e->rdma_sge.vaddr = NULL;
+			e->rdma_sge.length = 0;
+			e->rdma_sge.sge_length = 0;
 		}
 		e->opcode = opcode;
 		e->sent = 0;
@@ -2063,17 +2066,9 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 			ipath_update_ack_queue(qp, next);
 		}
 		e = &qp->s_ack_queue[qp->r_head_ack_queue];
-		if (e->opcode == OP(RDMA_READ_REQUEST)) {
-			unsigned i;
-
-			for (i = 0; i < e->rdma_sge.num_sge; i++) {
-				struct ipath_sge *sge = i ?
-					&e->rdma_sge.sg_list[i - 1] :
-					&e->rdma_sge.sge;
-
-				atomic_dec(&sge->mr->refcount);
-			}
-			e->opcode = 0;
+		if (e->opcode == OP(RDMA_READ_REQUEST) && e->rdma_sge.mr) {
+			atomic_dec(&e->rdma_sge.mr->refcount);
+			e->rdma_sge.mr = NULL;
 		}
 		if (!header_in_data)
 			ateth = &ohdr->u.atomic_eth;
@@ -2085,7 +2080,7 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 			goto nack_inv_unlck;
 		rkey = be32_to_cpu(ateth->rkey);
 		/* Check rkey & NAK */
-		if (unlikely(!ipath_rkey_ok(qp, &qp->r_sge,
+		if (unlikely(!ipath_rkey_ok(qp, &qp->r_sge.sge,
 					    sizeof(u64), vaddr, rkey,
 					    IB_ACCESS_REMOTE_ATOMIC)))
 			goto nack_acc_unlck;
diff -up a/drivers/infiniband/hw/ipath/ipath_ruc.c b/drivers/infiniband/hw/ipath/ipath_ruc.c
--- a/drivers/infiniband/hw/ipath/ipath_ruc.c	2009-03-05 17:37:11.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_ruc.c	2009-03-06 16:08:18.000000000 -0800
@@ -367,23 +367,27 @@ again:
 			goto inv_err;
 		if (wqe->length == 0)
 			break;
-		if (unlikely(!ipath_rkey_ok(qp, &qp->r_sge, wqe->length,
+		if (unlikely(!ipath_rkey_ok(qp, &qp->r_sge.sge, wqe->length,
 					    wqe->wr.wr.rdma.remote_addr,
 					    wqe->wr.wr.rdma.rkey,
 					    IB_ACCESS_REMOTE_WRITE)))
 			goto acc_err;
+		qp->r_sge.sg_list = NULL;
+		qp->r_sge.num_sge = 1;
 		qp->r_sge.total_len = wqe->length;
 		break;
 
 	case IB_WR_RDMA_READ:
 		if (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_READ)))
 			goto inv_err;
-		if (unlikely(!ipath_rkey_ok(qp, &sqp->s_sge, wqe->length,
+		if (unlikely(!ipath_rkey_ok(qp, &sqp->s_sge.sge, wqe->length,
 					    wqe->wr.wr.rdma.remote_addr,
 					    wqe->wr.wr.rdma.rkey,
 					    IB_ACCESS_REMOTE_READ)))
 			goto acc_err;
 		release = 0;
+		sqp->s_sge.sg_list = NULL;
+		sqp->s_sge.num_sge = 1;
 		qp->r_sge.sge = wqe->sg_list[0];
 		qp->r_sge.sg_list = wqe->sg_list + 1;
 		qp->r_sge.num_sge = wqe->wr.num_sge;
@@ -394,7 +398,7 @@ again:
 	case IB_WR_ATOMIC_FETCH_AND_ADD:
 		if (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_ATOMIC)))
 			goto inv_err;
-		if (unlikely(!ipath_rkey_ok(qp, &qp->r_sge, sizeof(u64),
+		if (unlikely(!ipath_rkey_ok(qp, &qp->r_sge.sge, sizeof(u64),
 					    wqe->wr.wr.atomic.remote_addr,
 					    wqe->wr.wr.atomic.rkey,
 					    IB_ACCESS_REMOTE_ATOMIC)))
diff -up a/drivers/infiniband/hw/ipath/ipath_uc.c b/drivers/infiniband/hw/ipath/ipath_uc.c
--- a/drivers/infiniband/hw/ipath/ipath_uc.c	2009-03-05 17:37:09.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_uc.c	2009-03-06 16:09:21.000000000 -0800
@@ -467,21 +467,22 @@ void ipath_uc_rcv(struct ipath_ibdev *de
 		hdrsize += sizeof(*reth);
 		qp->r_len = be32_to_cpu(reth->length);
 		qp->r_rcv_len = 0;
+		qp->r_sge.sg_list = NULL;
 		if (qp->r_len != 0) {
 			u32 rkey = be32_to_cpu(reth->rkey);
 			u64 vaddr = be64_to_cpu(reth->vaddr);
 			int ok;
 
 			/* Check rkey */
-			ok = ipath_rkey_ok(qp, &qp->r_sge, qp->r_len,
+			ok = ipath_rkey_ok(qp, &qp->r_sge.sge, qp->r_len,
 					   vaddr, rkey,
 					   IB_ACCESS_REMOTE_WRITE);
 			if (unlikely(!ok)) {
 				dev->n_pkt_drops++;
 				goto done;
 			}
+			qp->r_sge.num_sge = 1;
 		} else {
-			qp->r_sge.sg_list = NULL;
 			qp->r_sge.num_sge = 0;
 			qp->r_sge.sge.mr = NULL;
 			qp->r_sge.sge.vaddr = NULL;
diff -up a/drivers/infiniband/hw/ipath/ipath_verbs.c b/drivers/infiniband/hw/ipath/ipath_verbs.c
--- a/drivers/infiniband/hw/ipath/ipath_verbs.c	2009-03-05 17:37:22.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_verbs.c	2009-03-09 16:18:22.000000000 -0700
@@ -1077,6 +1077,10 @@ static void sdma_complete(void *cookie, 
 		spin_unlock_irqrestore(&qp->s_lock, flags);
 	}
 
+	if (tx->mr) {
+		atomic_dec(&tx->mr->refcount);
+		tx->mr = NULL;
+	}
 	if (tx->txreq.flags & IPATH_SDMA_TXREQ_F_FREEBUF)
 		kfree(tx->txreq.map_addr);
 	put_txreq(dev, tx);
@@ -1166,6 +1170,9 @@ static int ipath_verbs_send_dma(struct i
 	tx->qp = qp;
 	atomic_inc(&qp->refcount);
 	tx->wqe = qp->s_wqe;
+	tx->mr = qp->s_rdma_mr;
+	if (qp->s_rdma_mr)
+		qp->s_rdma_mr = NULL;
 	tx->txreq.callback = sdma_complete;
 	tx->txreq.callback_cookie = tx;
 	tx->txreq.flags = IPATH_SDMA_TXREQ_F_HEADTOHOST |
@@ -1242,6 +1249,10 @@ static int ipath_verbs_send_dma(struct i
 	goto bail;
 
 err_tx:
+	if (tx->mr) {
+		atomic_dec(&tx->mr->refcount);
+		tx->mr = NULL;
+	}
 	if (atomic_dec_and_test(&qp->refcount))
 		wake_up(&qp->wait);
 	put_txreq(dev, tx);
@@ -1331,6 +1342,10 @@ static int ipath_verbs_send_pio(struct i
 	}
 	copy_io(piobuf, ss, len, flush_wc);
 done:
+	if (qp->s_rdma_mr) {
+		atomic_dec(&qp->s_rdma_mr->refcount);
+		qp->s_rdma_mr = NULL;
+	}
 	if (qp->s_wqe) {
 		spin_lock_irqsave(&qp->s_lock, flags);
 		ipath_send_complete(qp, qp->s_wqe, IB_WC_SUCCESS);
diff -up a/drivers/infiniband/hw/ipath/ipath_verbs.h b/drivers/infiniband/hw/ipath/ipath_verbs.h
--- a/drivers/infiniband/hw/ipath/ipath_verbs.h	2009-03-05 17:37:09.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_verbs.h	2009-03-09 17:46:15.000000000 -0700
@@ -331,7 +331,6 @@ struct ipath_sge_state {
 	struct ipath_sge sge;   /* progress state for the current SGE */
 	u32 total_len;
 	u8 num_sge;
-	u8 static_rate;
 };
 
 /*
@@ -343,7 +342,7 @@ struct ipath_ack_entry {
 	u8 sent;
 	u32 psn;
 	union {
-		struct ipath_sge_state rdma_sge;
+		struct ipath_sge rdma_sge;
 		u64 atomic_data;
 	};
 };
@@ -372,6 +371,7 @@ struct ipath_qp {
 	struct ipath_mmap_info *ip;
 	struct ipath_sge_state *s_cur_sge;
 	struct ipath_verbs_txreq *s_tx;
+	struct ipath_mregion *s_rdma_mr;
 	struct ipath_sge_state s_sge;	/* current send request data */
 	struct ipath_ack_entry s_ack_queue[IPATH_MAX_RDMA_ATOMIC + 1];
 	struct ipath_sge_state s_ack_rdma_sge;
@@ -654,6 +654,7 @@ struct ipath_verbs_txreq {
 	struct ipath_swqe       *wqe;
 	u32                      map_len;
 	u32                      len;
+	struct ipath_mregion	*mr;
 	struct ipath_sge_state  *ss;
 	struct ipath_pio_header  hdr;
 	struct ipath_sdma_txreq  txreq;
@@ -795,7 +796,7 @@ int ipath_free_lkey(struct ipath_ibdev *
 int ipath_lkey_ok(struct ipath_qp *qp, struct ipath_sge *isge,
 		  struct ib_sge *sge, int acc);
 
-int ipath_rkey_ok(struct ipath_qp *qp, struct ipath_sge_state *ss,
+int ipath_rkey_ok(struct ipath_qp *qp, struct ipath_sge *sge,
 		  u32 len, u64 vaddr, u32 rkey, int acc);
 
 int ipath_post_srq_receive(struct ib_srq *ibsrq, struct ib_recv_wr *wr,
