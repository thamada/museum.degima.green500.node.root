IB/ipath: Fix memory region reference counts for resent RC packets

The verbs functions for freeing a memory region don't prevent a user
process from freeing a memory region while work requests still reference
it. Also, if a RC QP work request is being resent, it has an implicit
reference to the memory region via the LKey. The ACK can come back
and generate a completion which could signal the verbs consumer that
it is safe to free a memory region when it is actually still being
referenced by the driver. This patch delays the completion entry
until such packets are finished being sent.

Signed-off-by: Ralph Campbell <ralph.campbell@qlogic.com>
diff -up a/drivers/infiniband/hw/ipath/ipath_keys.c b/drivers/infiniband/hw/ipath/ipath_keys.c
--- a/drivers/infiniband/hw/ipath/ipath_keys.c	2008-12-10 18:04:07.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_keys.c	2008-12-10 18:07:56.000000000 -0800
@@ -93,17 +93,35 @@ bail:
  * @rkt: table from which to free the lkey
  * @lkey: lkey id to free
  */
-void ipath_free_lkey(struct ipath_lkey_table *rkt, u32 lkey)
+int ipath_free_lkey(struct ipath_ibdev *dev, struct ipath_mregion *mr)
 {
 	unsigned long flags;
+	u32 lkey = mr->lkey;
 	u32 r;
+	int ret;
 
-	if (lkey == 0)
-		return;
-	r = lkey >> (32 - ib_ipath_lkey_table_size);
-	spin_lock_irqsave(&rkt->lock, flags);
-	rkt->table[r] = NULL;
-	spin_unlock_irqrestore(&rkt->lock, flags);
+	spin_lock_irqsave(&dev->lk_table.lock, flags);
+	if (lkey == 0) {
+		if (dev->dma_mr) {
+			ret = atomic_read(&dev->dma_mr->refcount);
+			if (!ret && dev->dma_mr == mr)
+				dev->dma_mr = NULL;
+		} else
+			ret = 0;
+	} else {
+		r = lkey >> (32 - ib_ipath_lkey_table_size);
+		ret = atomic_read(&dev->lk_table.table[r]->refcount);
+		if (!ret)
+			dev->lk_table.table[r] = NULL;
+	}
+	spin_unlock_irqrestore(&dev->lk_table.lock, flags);
+
+	if (ret) {
+		ipath_dbg("ipath_free_lkey: MR busy (LKEY %x cnt %u)\n",
+			  lkey, ret);
+		ret = -EBUSY;
+	}
+	return ret;
 }
 
 /**
@@ -125,41 +143,41 @@ int ipath_lkey_ok(struct ipath_qp *qp, s
 	struct ipath_mregion *mr;
 	unsigned n, m;
 	size_t off;
-	int ret;
+	int ret = 0;
+	unsigned long flags;
 
 	/*
 	 * We use LKEY == zero for kernel virtual addresses
 	 * (see ipath_get_dma_mr and ipath_dma.c).
 	 */
+	spin_lock_irqsave(&rkt->lock, flags);
 	if (sge->lkey == 0) {
-		/* always a kernel port, no locking needed */
 		struct ipath_pd *pd = to_ipd(qp->ibqp.pd);
+		struct ipath_ibdev *dev = to_idev(pd->ibpd.device);
 
-		if (pd->user) {
-			ret = 0;
+		if (pd->user)
 			goto bail;
-		}
-		isge->mr = NULL;
+		if (!dev->dma_mr)
+			goto bail;
+		atomic_inc(&dev->dma_mr->refcount);
+		isge->mr = dev->dma_mr;
 		isge->vaddr = (void *) sge->addr;
 		isge->length = sge->length;
 		isge->sge_length = sge->length;
-		ret = 1;
-		goto bail;
+		isge->m = 0;
+		isge->n = 0;
+		goto ok;
 	}
 	mr = rkt->table[(sge->lkey >> (32 - ib_ipath_lkey_table_size))];
 	if (unlikely(mr == NULL || mr->lkey != sge->lkey ||
-		     qp->ibqp.pd != mr->pd)) {
-		ret = 0;
+		     qp->ibqp.pd != mr->pd))
 		goto bail;
-	}
 
 	off = sge->addr - mr->user_base;
 	if (unlikely(sge->addr < mr->user_base ||
 		     off + sge->length > mr->length ||
-		     (mr->access_flags & acc) != acc)) {
-		ret = 0;
+		     (mr->access_flags & acc) != acc))
 		goto bail;
-	}
 
 	off += mr->offset;
 	m = 0;
@@ -172,16 +190,17 @@ int ipath_lkey_ok(struct ipath_qp *qp, s
 			n = 0;
 		}
 	}
+	atomic_inc(&mr->refcount);
 	isge->mr = mr;
 	isge->vaddr = mr->map[m]->segs[n].vaddr + off;
 	isge->length = mr->map[m]->segs[n].length - off;
 	isge->sge_length = sge->length;
 	isge->m = m;
 	isge->n = n;
-
+ok:
 	ret = 1;
-
 bail:
+	spin_unlock_irqrestore(&rkt->lock, flags);
 	return ret;
 }
 
@@ -205,43 +224,45 @@ int ipath_rkey_ok(struct ipath_qp *qp, s
 	struct ipath_mregion *mr;
 	unsigned n, m;
 	size_t off;
-	int ret;
+	int ret = 0;
+	unsigned long flags;
+
+	ss->num_sge = 0;
 
 	/*
 	 * We use RKEY == zero for kernel virtual addresses
 	 * (see ipath_get_dma_mr and ipath_dma.c).
 	 */
+	spin_lock_irqsave(&rkt->lock, flags);
 	if (rkey == 0) {
-		/* always a kernel port, no locking needed */
 		struct ipath_pd *pd = to_ipd(qp->ibqp.pd);
+		struct ipath_ibdev *dev = to_idev(pd->ibpd.device);
 
-		if (pd->user) {
-			ret = 0;
+		if (pd->user)
 			goto bail;
-		}
-		sge->mr = NULL;
+		if (!dev->dma_mr)
+			goto bail;
+		atomic_inc(&dev->dma_mr->refcount);
+		sge->mr = dev->dma_mr;
 		sge->vaddr = (void *) vaddr;
 		sge->length = len;
 		sge->sge_length = len;
+		sge->m = 0;
+		sge->n = 0;
 		ss->sg_list = NULL;
 		ss->num_sge = 1;
-		ret = 1;
-		goto bail;
+		goto ok;
 	}
 
 	mr = rkt->table[(rkey >> (32 - ib_ipath_lkey_table_size))];
 	if (unlikely(mr == NULL || mr->lkey != rkey ||
-		     qp->ibqp.pd != mr->pd)) {
-		ret = 0;
+		     qp->ibqp.pd != mr->pd))
 		goto bail;
-	}
 
 	off = vaddr - mr->iova;
 	if (unlikely(vaddr < mr->iova || off + len > mr->length ||
-		     (mr->access_flags & acc) == 0)) {
-		ret = 0;
+		     (mr->access_flags & acc) == 0))
 		goto bail;
-	}
 
 	off += mr->offset;
 	m = 0;
@@ -254,6 +275,7 @@ int ipath_rkey_ok(struct ipath_qp *qp, s
 			n = 0;
 		}
 	}
+	atomic_inc(&mr->refcount);
 	sge->mr = mr;
 	sge->vaddr = mr->map[m]->segs[n].vaddr + off;
 	sge->length = mr->map[m]->segs[n].length - off;
@@ -262,9 +284,9 @@ int ipath_rkey_ok(struct ipath_qp *qp, s
 	sge->n = n;
 	ss->sg_list = NULL;
 	ss->num_sge = 1;
-
+ok:
 	ret = 1;
-
 bail:
+	spin_unlock_irqrestore(&rkt->lock, flags);
 	return ret;
 }
diff -up a/drivers/infiniband/hw/ipath/ipath_mr.c b/drivers/infiniband/hw/ipath/ipath_mr.c
--- a/drivers/infiniband/hw/ipath/ipath_mr.c	2008-12-10 18:04:03.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_mr.c	2008-12-10 18:07:56.000000000 -0800
@@ -35,6 +35,7 @@
 #include <rdma/ib_pack.h>
 #include <rdma/ib_smi.h>
 
+#include "ipath_kernel.h"
 #include "ipath_verbs.h"
 
 /* Fast memory region */
@@ -60,8 +61,15 @@ static inline struct ipath_fmr *to_ifmr(
  */
 struct ib_mr *ipath_get_dma_mr(struct ib_pd *pd, int acc)
 {
+	struct ipath_ibdev *dev = to_idev(pd->device);
 	struct ipath_mr *mr;
 	struct ib_mr *ret;
+	unsigned long flags;
+
+	if (to_ipd(pd)->user) {
+		ret = ERR_PTR(-EPERM);
+		goto bail;
+	}
 
 	mr = kzalloc(sizeof *mr, GFP_KERNEL);
 	if (!mr) {
@@ -70,6 +78,13 @@ struct ib_mr *ipath_get_dma_mr(struct ib
 	}
 
 	mr->mr.access_flags = acc;
+	atomic_set(&mr->mr.refcount, 0);
+
+	spin_lock_irqsave(&dev->lk_table.lock, flags);
+	if (!dev->dma_mr)
+		dev->dma_mr = &mr->mr;
+	spin_unlock_irqrestore(&dev->lk_table.lock, flags);
+
 	ret = &mr->ibmr;
 
 bail:
@@ -104,6 +119,7 @@ static struct ipath_mr *alloc_mr(int cou
 		goto bail;
 	mr->ibmr.rkey = mr->ibmr.lkey = mr->mr.lkey;
 
+	atomic_set(&mr->mr.refcount, 0);
 	goto done;
 
 bail:
@@ -258,9 +274,14 @@ bail:
 int ipath_dereg_mr(struct ib_mr *ibmr)
 {
 	struct ipath_mr *mr = to_imr(ibmr);
+	struct ipath_ibdev *dev = to_idev(ibmr->device);
+	int ret;
 	int i;
 
-	ipath_free_lkey(&to_idev(ibmr->device)->lk_table, ibmr->lkey);
+	ret = ipath_free_lkey(dev, &mr->mr);
+	if (ret)
+		return ret;
+
 	i = mr->mr.mapsz;
 	while (i) {
 		i--;
@@ -324,6 +345,7 @@ struct ib_fmr *ipath_alloc_fmr(struct ib
 	fmr->mr.max_segs = fmr_attr->max_pages;
 	fmr->page_shift = fmr_attr->page_shift;
 
+	atomic_set(&fmr->mr.refcount, 0);
 	ret = &fmr->ibfmr;
 	goto done;
 
@@ -357,6 +379,12 @@ int ipath_map_phys_fmr(struct ib_fmr *ib
 	u32 ps;
 	int ret;
 
+	if (atomic_read(&fmr->mr.refcount)) {
+		ipath_dbg("FMR modified when busy (LKEY %x cnt %u)\n",
+			  fmr->mr.lkey, atomic_read(&fmr->mr.refcount));
+		return -EBUSY;
+	}
+
 	if (list_len > fmr->mr.max_segs) {
 		ret = -EINVAL;
 		goto bail;
@@ -400,6 +428,10 @@ int ipath_unmap_fmr(struct list_head *fm
 	list_for_each_entry(fmr, fmr_list, ibfmr.list) {
 		rkt = &to_idev(fmr->ibfmr.device)->lk_table;
 		spin_lock_irqsave(&rkt->lock, flags);
+		if (atomic_read(&fmr->mr.refcount))
+			ipath_dbg("FMR busy (LKEY %x cnt %u)\n",
+				  fmr->mr.lkey, atomic_read(&fmr->mr.refcount));
+
 		fmr->mr.user_base = 0;
 		fmr->mr.iova = 0;
 		fmr->mr.length = 0;
@@ -417,9 +449,13 @@ int ipath_unmap_fmr(struct list_head *fm
 int ipath_dealloc_fmr(struct ib_fmr *ibfmr)
 {
 	struct ipath_fmr *fmr = to_ifmr(ibfmr);
+	int ret;
 	int i;
 
-	ipath_free_lkey(&to_idev(ibfmr->device)->lk_table, ibfmr->lkey);
+	ret = ipath_free_lkey(to_idev(ibfmr->device), &fmr->mr);
+	if (ret)
+		return ret;
+
 	i = fmr->mr.mapsz;
 	while (i)
 		kfree(fmr->mr.map[--i]);
diff -up a/drivers/infiniband/hw/ipath/ipath_qp.c b/drivers/infiniband/hw/ipath/ipath_qp.c
--- a/drivers/infiniband/hw/ipath/ipath_qp.c	2008-12-10 18:04:07.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_qp.c	2008-12-10 18:07:56.000000000 -0800
@@ -330,6 +330,10 @@ static void ipath_reset_qp(struct ipath_
 	qp->s_wqe = NULL;
 	qp->s_pkt_delay = 0;
 	qp->s_draining = 0;
+	qp->s_next_psn = 0;
+	qp->s_last_psn = 0;
+	qp->s_sending_psn = 0;
+	qp->s_sending_hpsn = 0;
 	qp->s_psn = 0;
 	qp->r_psn = 0;
 	qp->r_msn = 0;
@@ -348,6 +352,7 @@ static void ipath_reset_qp(struct ipath_
 	qp->s_head = 0;
 	qp->s_tail = 0;
 	qp->s_cur = 0;
+	qp->s_acked = 0;
 	qp->s_last = 0;
 	qp->s_ssn = 1;
 	qp->s_lsn = 0;
@@ -359,6 +364,54 @@ static void ipath_reset_qp(struct ipath_
 		qp->r_rq.wq->head = 0;
 		qp->r_rq.wq->tail = 0;
 	}
+	qp->r_sge.num_sge = 0;
+}
+
+static void clear_mr_refs(struct ipath_qp *qp, int clr_sends)
+{
+	unsigned n;
+
+	while (qp->r_sge.num_sge) {
+		atomic_dec(&qp->r_sge.sge.mr->refcount);
+		if (--qp->r_sge.num_sge)
+			qp->r_sge.sge = *qp->r_sge.sg_list++;
+	}
+
+	if (clr_sends) {
+		n = qp->s_last <= qp->s_head ? qp->s_head - qp->s_last :
+			qp->s_size - qp->s_last + qp->s_head;
+
+		while (qp->s_last != qp->s_head) {
+			struct ipath_swqe *wqe = get_swqe_ptr(qp, qp->s_last);
+			unsigned i;
+
+			for (i = 0; i < wqe->wr.num_sge; i++) {
+				struct ipath_sge *sge = &wqe->sg_list[i];
+
+				atomic_dec(&sge->mr->refcount);
+			}
+			if (++qp->s_last >= qp->s_size)
+				qp->s_last = 0;
+		}
+	}
+
+	if (qp->ibqp.qp_type != IB_QPT_RC)
+		return;
+
+	/* XXX Need to be sure that none of these are actively being sent */
+	for (n = 0; n < ARRAY_SIZE(qp->s_ack_queue); n++) {
+		struct ipath_ack_entry *e = &qp->s_ack_queue[n];
+		unsigned i;
+
+		if (e->opcode != IB_OPCODE_RC_RDMA_READ_REQUEST)
+			continue;
+		for (i = 0; i < e->rdma_sge.num_sge; i++) {
+			struct ipath_sge *sge = i ?
+				&e->rdma_sge.sg_list[i - 1] : &e->rdma_sge.sge;
+
+			atomic_dec(&sge->mr->refcount);
+		}
+	}
 }
 
 /**
@@ -394,6 +447,8 @@ int ipath_error_qp(struct ipath_qp *qp, 
 	if (qp->s_last != qp->s_head)
 		ipath_schedule_send(qp);
 
+	clear_mr_refs(qp, 0);
+
 	memset(&wc, 0, sizeof(wc));
 	wc.qp = &qp->ibqp;
 	wc.opcode = IB_WC_RECV;
@@ -552,8 +607,8 @@ int ipath_modify_qp(struct ib_qp *ibqp, 
 		qp->remote_qpn = attr->dest_qp_num;
 
 	if (attr_mask & IB_QP_SQ_PSN) {
-		qp->s_psn = qp->s_next_psn = attr->sq_psn;
-		qp->s_last_psn = qp->s_next_psn - 1;
+		qp->s_sending_psn = qp->s_psn = qp->s_next_psn = attr->sq_psn;
+		qp->s_sending_hpsn = qp->s_last_psn = qp->s_next_psn - 1;
 	}
 
 	if (attr_mask & IB_QP_RQ_PSN)
@@ -994,6 +1049,8 @@ int ipath_destroy_qp(struct ib_qp *ibqp)
 
 	wait_event(qp->wait, !atomic_read(&qp->refcount));
 
+	clear_mr_refs(qp, 1);
+
 	/* all user's cleaned up, mark it available */
 	free_qpn(&dev->qp_table, qp->ibqp.qp_num);
 	spin_lock(&dev->n_qps_lock);
@@ -1067,12 +1124,4 @@ void ipath_get_credit(struct ipath_qp *q
 		if (ipath_cmp24(credit, qp->s_lsn) > 0)
 			qp->s_lsn = credit;
 	}
-
-	/* Restart sending if it was blocked due to lack of credits. */
-	if ((qp->s_flags & IPATH_S_WAIT_SSN_CREDIT) &&
-	    qp->s_cur != qp->s_head &&
-	    (qp->s_lsn == (u32) -1 ||
-	     ipath_cmp24(get_swqe_ptr(qp, qp->s_cur)->ssn,
-			 qp->s_lsn + 1) <= 0))
-		ipath_schedule_send(qp);
 }
diff -up a/drivers/infiniband/hw/ipath/ipath_rc.c b/drivers/infiniband/hw/ipath/ipath_rc.c
--- a/drivers/infiniband/hw/ipath/ipath_rc.c	2008-12-10 18:04:07.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_rc.c	2008-12-10 18:14:56.000000000 -0800
@@ -49,7 +49,7 @@ static u32 restart_sge(struct ipath_sge_
 	ss->sg_list = wqe->sg_list + 1;
 	ss->num_sge = wqe->wr.num_sge;
 	ss->total_len = wqe->length;
-	ipath_skip_sge(ss, len);
+	ipath_skip_sge(ss, len, 0);
 	return wqe->length - len;
 }
 
@@ -225,6 +225,7 @@ int ipath_make_rc_req(struct ipath_qp *q
 	char newreq;
 	unsigned long flags;
 	int ret = 0;
+	int delta;
 
 	ohdr = &qp->s_hdr.u.oth;
 	if (qp->remote_ah_attr.ah_flags & IB_AH_GRH)
@@ -255,6 +256,12 @@ int ipath_make_rc_req(struct ipath_qp *q
 			goto bail;
 		}
 		wqe = get_swqe_ptr(qp, qp->s_last);
+		while (qp->s_last != qp->s_acked) {
+			ipath_send_complete(qp, wqe, IB_WC_SUCCESS);
+			if (++qp->s_last >= qp->s_size)
+				qp->s_last = 0;
+			wqe = get_swqe_ptr(qp, qp->s_last);
+		}
 		ipath_send_complete(qp, wqe, IB_WC_WR_FLUSH_ERR);
 		goto done;
 	}
@@ -265,6 +272,15 @@ int ipath_make_rc_req(struct ipath_qp *q
 		goto bail;
 	}
 
+	/*
+	 * Leave BUSY set until sdma queue drains so we don't send
+	 * the same PSN multiple times.
+	 */
+	if (ipath_cmp24(qp->s_psn, qp->s_sending_hpsn) <= 0) {
+		qp->s_flags |= IPATH_S_WAITING;
+		goto bail;
+	}
+
 	/* header size in 32-bit words LRH+BTH = (8+12)/4. */
 	hwords = 5;
 	bth0 = 1 << 22; /* Set M bit */
@@ -575,9 +591,8 @@ int ipath_make_rc_req(struct ipath_qp *q
 		ohdr->u.rc.reth.length = cpu_to_be32(qp->s_len);
 		qp->s_state = OP(RDMA_READ_REQUEST);
 		hwords += sizeof(ohdr->u.rc.reth) / sizeof(u32);
-		bth2 = qp->s_psn++ & IPATH_PSN_MASK;
-		if (ipath_cmp24(qp->s_psn, qp->s_next_psn) > 0)
-			qp->s_next_psn = qp->s_psn;
+		bth2 = qp->s_psn & IPATH_PSN_MASK;
+		qp->s_psn = wqe->lpsn + 1;
 		ss = NULL;
 		len = 0;
 		qp->s_cur++;
@@ -585,7 +600,9 @@ int ipath_make_rc_req(struct ipath_qp *q
 			qp->s_cur = 0;
 		break;
 	}
-	if (ipath_cmp24(qp->s_psn, qp->s_last_psn + IPATH_PSN_CREDIT - 1) >= 0)
+	qp->s_sending_hpsn = bth2;
+	delta = (((int) bth2 - (int) wqe->psn) << 8) >> 8;
+	if (delta && delta % IPATH_PSN_CREDIT == 0)
 		bth2 |= 1 << 31;	/* Request ACK. */
 	qp->s_len -= len;
 	qp->s_hdrwords = hwords;
@@ -725,7 +742,7 @@ done:
  */
 static void reset_psn(struct ipath_qp *qp, u32 psn)
 {
-	u32 n = qp->s_last;
+	u32 n = qp->s_acked;
 	struct ipath_swqe *wqe = get_swqe_ptr(qp, n);
 	u32 opcode;
 
@@ -794,6 +811,10 @@ static void reset_psn(struct ipath_qp *q
 	}
 done:
 	qp->s_psn = psn;
+	if (ipath_cmp24(qp->s_sending_psn, qp->s_sending_hpsn) > 0) {
+		qp->s_sending_psn = psn;
+		qp->s_sending_hpsn = psn - 1;
+	}
 }
 
 /**
@@ -806,12 +827,17 @@ done:
  */
 void ipath_restart_rc(struct ipath_qp *qp, u32 psn)
 {
-	struct ipath_swqe *wqe = get_swqe_ptr(qp, qp->s_last);
+	struct ipath_swqe *wqe = get_swqe_ptr(qp, qp->s_acked);
 	struct ipath_ibdev *dev;
 
 	if (qp->s_retry == 0) {
-		ipath_send_complete(qp, wqe, IB_WC_RETRY_EXC_ERR);
-		ipath_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+		if (qp->s_last == qp->s_acked) {
+			ipath_send_complete(qp, wqe, IB_WC_RETRY_EXC_ERR);
+			ipath_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+		} else {
+			/* XXX need to handle delayed completion */
+			ipath_dbg("Delayed too many retries\n");
+		}
 		goto bail;
 	}
 	qp->s_retry--;
@@ -840,6 +866,99 @@ bail:
 	return;
 }
 
+/*
+ * Set qp->s_sending_psn to the next PSN after the given one.
+ * This would be psn+1 except when RDMA reads are present.
+ */
+static void reset_sending_psn(struct ipath_qp *qp, u32 psn)
+{
+	struct ipath_swqe *wqe;
+	u32 n = qp->s_last;
+
+	/* Find the work request corresponding to the given PSN. */
+	for (;;) {
+		wqe = get_swqe_ptr(qp, n);
+		if (ipath_cmp24(psn, wqe->lpsn) <= 0) {
+			if (wqe->wr.opcode == IB_WR_RDMA_READ)
+				qp->s_sending_psn = wqe->lpsn + 1;
+			else
+				qp->s_sending_psn = psn + 1;
+			break;
+		}
+		if (++n == qp->s_size)
+			n = 0;
+		if (n == qp->s_tail)
+			break;
+	}
+}
+
+/*
+ * This should be called with the QP s_lock held and interrupts disabled.
+ */
+void ipath_rc_send_complete(struct ipath_qp *qp, struct ipath_ib_header *hdr)
+{
+	struct ipath_other_headers *ohdr;
+	struct ipath_swqe *wqe;
+	struct ib_wc wc;
+	unsigned i;
+	u32 opcode;
+	u32 psn;
+
+	if (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_OR_FLUSH_SEND))
+		return;
+
+	/* Find out where the BTH is */
+	if ((be16_to_cpu(hdr->lrh[0]) & 3) == IPATH_LRH_BTH)
+		ohdr = &hdr->u.oth;
+	else
+		ohdr = &hdr->u.l.oth;
+
+	opcode = be32_to_cpu(ohdr->bth[0]) >> 24;
+	if (opcode >= OP(RDMA_READ_RESPONSE_FIRST) &&
+	    opcode <= OP(ATOMIC_ACKNOWLEDGE)) {
+		/* XXX Need to handle MR refcount similar to requester */
+		return;
+	}
+
+	psn = be32_to_cpu(ohdr->bth[2]);
+	reset_sending_psn(qp, psn);
+
+	while (qp->s_last != qp->s_acked) {
+		wqe = get_swqe_ptr(qp, qp->s_last);
+		if (ipath_cmp24(wqe->lpsn, qp->s_sending_psn) >= 0 &&
+		    ipath_cmp24(qp->s_sending_psn, qp->s_sending_hpsn) <= 0)
+			break;
+		for (i = 0; i < wqe->wr.num_sge; i++) {
+			struct ipath_sge *sge = &wqe->sg_list[i];
+
+			atomic_dec(&sge->mr->refcount);
+		}
+		/* Post a send completion queue entry if requested. */
+		if (!(qp->s_flags & IPATH_S_SIGNAL_REQ_WR) ||
+		    (wqe->wr.send_flags & IB_SEND_SIGNALED)) {
+			memset(&wc, 0, sizeof wc);
+			wc.wr_id = wqe->wr.wr_id;
+			wc.status = IB_WC_SUCCESS;
+			wc.opcode = ib_ipath_wc_opcode[wqe->wr.opcode];
+			wc.qp = &qp->ibqp;
+			ipath_cq_enter(to_icq(qp->ibqp.send_cq), &wc, 0);
+		}
+		if (++qp->s_last >= qp->s_size)
+			qp->s_last = 0;
+	}
+	/*
+	 * If we were waiting for sends to complete before resending,
+	 * and they are now complete, restart sending.
+	 */
+	if (qp->s_acked != qp->s_head &&
+	    ipath_cmp24(qp->s_sending_psn, qp->s_sending_hpsn) > 0 &&
+	    ipath_cmp24(qp->s_psn, qp->s_sending_hpsn) <= 0) {
+		qp->s_sending_psn = qp->s_psn;
+		qp->s_sending_hpsn = qp->s_psn - 1;
+		ipath_schedule_send(qp);
+	}
+}
+
 static inline void update_last_psn(struct ipath_qp *qp, u32 psn)
 {
 	qp->s_last_psn = psn;
@@ -866,6 +985,7 @@ static int do_rc_ack(struct ipath_qp *qp
 	int ret = 0;
 	u32 ack_psn;
 	int diff;
+	unsigned i;
 
 	/*
 	 * Remove the QP from the timeout queue (or RNR timeout queue).
@@ -887,7 +1007,7 @@ static int do_rc_ack(struct ipath_qp *qp
 	ack_psn = psn;
 	if (aeth >> 29)
 		ack_psn--;
-	wqe = get_swqe_ptr(qp, qp->s_last);
+	wqe = get_swqe_ptr(qp, qp->s_acked);
 
 	/*
 	 * The MSN might be for a later WQE than the PSN indicates so
@@ -947,65 +1067,78 @@ static int do_rc_ack(struct ipath_qp *qp
 			    qp->s_flags & IPATH_S_RDMAR_PENDING)
 				ipath_schedule_send(qp);
 		}
-		/* Post a send completion queue entry if requested. */
-		if (!(qp->s_flags & IPATH_S_SIGNAL_REQ_WR) ||
-		    (wqe->wr.send_flags & IB_SEND_SIGNALED)) {
-			memset(&wc, 0, sizeof wc);
-			wc.wr_id = wqe->wr.wr_id;
-			wc.status = IB_WC_SUCCESS;
-			wc.opcode = ib_ipath_wc_opcode[wqe->wr.opcode];
-			wc.byte_len = wqe->length;
-			wc.qp = &qp->ibqp;
-			wc.src_qp = qp->remote_qpn;
-			wc.slid = qp->remote_ah_attr.dlid;
-			wc.sl = qp->remote_ah_attr.sl;
-			ipath_cq_enter(to_icq(qp->ibqp.send_cq), &wc, 0);
-		}
+		/*
+		 * Don't decrement refcount and don't generate a
+		 * completion if the WQE is being resent until the send
+		 * is finished.
+		 */
+		if (ipath_cmp24(wqe->lpsn, qp->s_sending_psn) < 0 ||
+		    ipath_cmp24(qp->s_sending_psn, qp->s_sending_hpsn) > 0) {
+			for (i = 0; i < wqe->wr.num_sge; i++) {
+				struct ipath_sge *sge = &wqe->sg_list[i];
+
+				atomic_dec(&sge->mr->refcount);
+			}
+			/* Post a send completion queue entry if requested. */
+			if (!(qp->s_flags & IPATH_S_SIGNAL_REQ_WR) ||
+			    (wqe->wr.send_flags & IB_SEND_SIGNALED)) {
+				memset(&wc, 0, sizeof wc);
+				wc.wr_id = wqe->wr.wr_id;
+				wc.status = IB_WC_SUCCESS;
+				wc.opcode = ib_ipath_wc_opcode[wqe->wr.opcode];
+				wc.qp = &qp->ibqp;
+				ipath_cq_enter(to_icq(qp->ibqp.send_cq), &wc,
+						0);
+			}
+			if (++qp->s_last >= qp->s_size)
+				qp->s_last = 0;
+		} else
+			dev->n_rc_delayed_comp++;
 		qp->s_retry = qp->s_retry_cnt;
 		/*
 		 * If we are completing a request which is in the process of
 		 * being resent, we can stop resending it since we know the
 		 * responder has already seen it.
 		 */
-		if (qp->s_last == qp->s_cur) {
+		if (qp->s_acked == qp->s_cur) {
 			if (++qp->s_cur >= qp->s_size)
 				qp->s_cur = 0;
-			qp->s_last = qp->s_cur;
-			if (qp->s_last == qp->s_tail)
+			qp->s_acked = qp->s_cur;
+			if (qp->s_acked == qp->s_tail)
 				break;
 			wqe = get_swqe_ptr(qp, qp->s_cur);
 			qp->s_state = OP(SEND_LAST);
 			qp->s_psn = wqe->psn;
 		} else {
-			if (++qp->s_last >= qp->s_size)
-				qp->s_last = 0;
-			if (qp->state == IB_QPS_SQD && qp->s_last == qp->s_cur)
+			if (++qp->s_acked >= qp->s_size)
+				qp->s_acked = 0;
+			if (qp->state == IB_QPS_SQD && qp->s_acked == qp->s_cur)
 				qp->s_draining = 0;
-			if (qp->s_last == qp->s_tail)
+			if (qp->s_acked == qp->s_tail)
 				break;
-			wqe = get_swqe_ptr(qp, qp->s_last);
+			wqe = get_swqe_ptr(qp, qp->s_acked);
 		}
 	}
 
 	switch (aeth >> 29) {
 	case 0:		/* ACK */
 		dev->n_rc_acks++;
-		/* If this is a partial ACK, reset the retransmit timer. */
-		if (qp->s_last != qp->s_tail) {
+		if (qp->s_acked != qp->s_tail) {
+			/*
+			 * We got a partial ACK for a resent operation so
+			 * reset the retransmit timer.
+			 */
 			spin_lock(&dev->pending_lock);
 			if (list_empty(&qp->timerwait))
 				list_add_tail(&qp->timerwait,
 					&dev->pending[dev->pending_index]);
 			spin_unlock(&dev->pending_lock);
 			/*
-			 * If we get a partial ACK for a resent operation,
-			 * we can stop resending the earlier packets and
+			 * We can stop resending the earlier packets and
 			 * continue with the next packet the receiver wants.
 			 */
-			if (ipath_cmp24(qp->s_psn, psn) <= 0) {
+			if (ipath_cmp24(qp->s_psn, psn) <= 0)
 				reset_psn(qp, psn + 1);
-				ipath_schedule_send(qp);
-			}
 		} else if (ipath_cmp24(qp->s_psn, psn) <= 0) {
 			qp->s_state = OP(SEND_LAST);
 			qp->s_psn = psn + 1;
@@ -1014,12 +1147,16 @@ static int do_rc_ack(struct ipath_qp *qp
 		qp->s_rnr_retry = qp->s_rnr_retry_cnt;
 		qp->s_retry = qp->s_retry_cnt;
 		update_last_psn(qp, psn);
+		if (qp->s_acked != qp->s_head)
+			ipath_schedule_send(qp);
+		else
+			qp->s_flags &= ~IPATH_S_WAITING;
 		ret = 1;
 		goto bail;
 
 	case 1:		/* RNR NAK */
 		dev->n_rnr_naks++;
-		if (qp->s_last == qp->s_tail)
+		if (qp->s_acked == qp->s_tail)
 			goto bail;
 		if (qp->s_rnr_retry == 0) {
 			status = IB_WC_RNR_RETRY_EXC_ERR;
@@ -1047,7 +1184,7 @@ static int do_rc_ack(struct ipath_qp *qp
 		goto bail;
 
 	case 3:		/* NAK */
-		if (qp->s_last == qp->s_tail)
+		if (qp->s_acked == qp->s_tail)
 			goto bail;
 		/* The last valid PSN is the previous PSN. */
 		update_last_psn(qp, psn - 1);
@@ -1078,8 +1215,13 @@ static int do_rc_ack(struct ipath_qp *qp
 			status = IB_WC_REM_OP_ERR;
 			dev->n_other_naks++;
 		class_b:
-			ipath_send_complete(qp, wqe, status);
-			ipath_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+			if (qp->s_last == qp->s_acked) {
+				ipath_send_complete(qp, wqe, status);
+				ipath_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+			} else {
+				/* XXX need to handle delayed completion */
+				ipath_dbg("Delayed error %d\n", status);
+			}
 			break;
 
 		default:
@@ -1159,9 +1301,9 @@ static inline void ipath_rc_rcv_resp(str
 		goto ack_done;
 	}
 
-	if (unlikely(qp->s_last == qp->s_tail))
+	if (unlikely(qp->s_acked == qp->s_tail))
 		goto ack_done;
-	wqe = get_swqe_ptr(qp, qp->s_last);
+	wqe = get_swqe_ptr(qp, qp->s_acked);
 	status = IB_WC_SUCCESS;
 
 	switch (opcode) {
@@ -1188,7 +1330,7 @@ static inline void ipath_rc_rcv_resp(str
 		    opcode != OP(RDMA_READ_RESPONSE_FIRST))
 			goto ack_done;
 		hdrsize += 4;
-		wqe = get_swqe_ptr(qp, qp->s_last);
+		wqe = get_swqe_ptr(qp, qp->s_acked);
 		if (unlikely(wqe->wr.opcode != IB_WR_RDMA_READ))
 			goto ack_op_err;
 		qp->r_flags &= ~IPATH_R_RDMAR_SEQ;
@@ -1236,7 +1378,7 @@ static inline void ipath_rc_rcv_resp(str
 		qp->s_rdma_read_len -= pmtu;
 		update_last_psn(qp, psn);
 		spin_unlock_irqrestore(&qp->s_lock, flags);
-		ipath_copy_sge(&qp->s_rdma_read_sge, data, pmtu);
+		ipath_copy_sge(&qp->s_rdma_read_sge, data, pmtu, 0);
 		goto bail;
 
 	case OP(RDMA_READ_RESPONSE_ONLY):
@@ -1260,7 +1402,7 @@ static inline void ipath_rc_rcv_resp(str
 		 * have to be careful to copy the data to the right
 		 * location.
 		 */
-		wqe = get_swqe_ptr(qp, qp->s_last);
+		wqe = get_swqe_ptr(qp, qp->s_acked);
 		qp->s_rdma_read_len = restart_sge(&qp->s_rdma_read_sge,
 						  wqe, psn, pmtu);
 		goto read_last;
@@ -1296,7 +1438,8 @@ static inline void ipath_rc_rcv_resp(str
 			aeth = be32_to_cpu(((__be32 *) data)[0]);
 			data += sizeof(__be32);
 		}
-		ipath_copy_sge(&qp->s_rdma_read_sge, data, tlen);
+		ipath_copy_sge(&qp->s_rdma_read_sge, data, tlen, 0);
+		WARN_ON(qp->s_rdma_read_sge.num_sge);
 		(void) do_rc_ack(qp, aeth, psn,
 				 OP(RDMA_READ_RESPONSE_LAST), 0);
 		goto ack_done;
@@ -1309,8 +1452,13 @@ ack_op_err:
 ack_len_err:
 	status = IB_WC_LOC_LEN_ERR;
 ack_err:
-	ipath_send_complete(qp, wqe, status);
-	ipath_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+	if (qp->s_last == qp->s_acked) {
+		ipath_send_complete(qp, wqe, status);
+		ipath_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+	} else {
+		/* XXX need to handle delayed completion */
+		ipath_dbg("Delayed error %d\n", status);
+	}
 ack_done:
 	spin_unlock_irqrestore(&qp->s_lock, flags);
 bail:
@@ -1440,6 +1588,12 @@ static inline int ipath_rc_rcv_error(str
 		len = be32_to_cpu(reth->length);
 		if (unlikely(offset + len > e->rdma_sge.sge.sge_length))
 			goto unlock_done;
+		for (i = 0; i < e->rdma_sge.num_sge; i++) {
+			struct ipath_sge *sge = i ?
+				&e->rdma_sge.sg_list[i - 1] : &e->rdma_sge.sge;
+
+			atomic_dec(&sge->mr->refcount);
+		}
 		if (len != 0) {
 			u32 rkey = be32_to_cpu(reth->rkey);
 			u64 vaddr = be64_to_cpu(reth->vaddr);
@@ -1685,7 +1839,7 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 		qp->r_rcv_len += pmtu;
 		if (unlikely(qp->r_rcv_len > qp->r_len))
 			goto nack_inv;
-		ipath_copy_sge(&qp->r_sge, data, pmtu);
+		ipath_copy_sge(&qp->r_sge, data, pmtu, 1);
 		break;
 
 	case OP(RDMA_WRITE_LAST_WITH_IMMEDIATE):
@@ -1728,7 +1882,12 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 		wc.byte_len = tlen + qp->r_rcv_len;
 		if (unlikely(wc.byte_len > qp->r_len))
 			goto nack_inv;
-		ipath_copy_sge(&qp->r_sge, data, tlen);
+		ipath_copy_sge(&qp->r_sge, data, tlen, 1);
+		while (qp->r_sge.num_sge) {
+			atomic_dec(&qp->r_sge.sge.mr->refcount);
+			if (--qp->r_sge.num_sge)
+				qp->r_sge.sge = *qp->r_sge.sg_list++;
+		}
 		qp->r_msn++;
 		if (!test_and_clear_bit(IPATH_R_WRID_VALID, &qp->r_aflags))
 			break;
@@ -1779,6 +1938,7 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 				goto nack_acc;
 		} else {
 			qp->r_sge.sg_list = NULL;
+			qp->r_sge.num_sge = 0;
 			qp->r_sge.sge.mr = NULL;
 			qp->r_sge.sge.vaddr = NULL;
 			qp->r_sge.sge.length = 0;
@@ -1813,6 +1973,18 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 			ipath_update_ack_queue(qp, next);
 		}
 		e = &qp->s_ack_queue[qp->r_head_ack_queue];
+		if (e->opcode == OP(RDMA_READ_REQUEST)) {
+			unsigned i;
+
+			for (i = 0; i < e->rdma_sge.num_sge; i++) {
+				struct ipath_sge *sge = i ?
+					&e->rdma_sge.sg_list[i - 1] :
+					&e->rdma_sge.sge;
+
+				atomic_dec(&sge->mr->refcount);
+			}
+			e->opcode = 0;
+		}
 		/* RETH comes after BTH */
 		if (!header_in_data)
 			reth = &ohdr->u.rc.reth;
@@ -1890,6 +2062,19 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 				goto nack_inv_unlck;
 			ipath_update_ack_queue(qp, next);
 		}
+		e = &qp->s_ack_queue[qp->r_head_ack_queue];
+		if (e->opcode == OP(RDMA_READ_REQUEST)) {
+			unsigned i;
+
+			for (i = 0; i < e->rdma_sge.num_sge; i++) {
+				struct ipath_sge *sge = i ?
+					&e->rdma_sge.sg_list[i - 1] :
+					&e->rdma_sge.sge;
+
+				atomic_dec(&sge->mr->refcount);
+			}
+			e->opcode = 0;
+		}
 		if (!header_in_data)
 			ateth = &ohdr->u.atomic_eth;
 		else
@@ -1907,12 +2092,13 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 		/* Perform atomic OP and save result. */
 		maddr = (atomic64_t *) qp->r_sge.sge.vaddr;
 		sdata = be64_to_cpu(ateth->swap_data);
-		e = &qp->s_ack_queue[qp->r_head_ack_queue];
 		e->atomic_data = (opcode == OP(FETCH_ADD)) ?
 			(u64) atomic64_add_return(sdata, maddr) - sdata :
 			(u64) cmpxchg((u64 *) qp->r_sge.sge.vaddr,
 				      be64_to_cpu(ateth->compare_data),
 				      sdata);
+		atomic_dec(&qp->r_sge.sge.mr->refcount);
+		qp->r_sge.num_sge = 0;
 		e->opcode = opcode;
 		e->sent = 0;
 		e->psn = psn & IPATH_PSN_MASK;
diff -up a/drivers/infiniband/hw/ipath/ipath_ruc.c b/drivers/infiniband/hw/ipath/ipath_ruc.c
--- a/drivers/infiniband/hw/ipath/ipath_ruc.c	2008-12-10 18:04:07.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_ruc.c	2008-12-10 18:07:56.000000000 -0800
@@ -142,6 +142,12 @@ int ipath_init_sge(struct ipath_qp *qp, 
 	goto bail;
 
 bad_lkey:
+	while (j) {
+		struct ipath_sge *sge = --j ? &ss->sg_list[j - 1] : &ss->sge;
+
+		atomic_dec(&sge->mr->refcount);
+	}
+	ss->num_sge = 0;
 	memset(&wc, 0, sizeof(wc));
 	wc.wr_id = wqe->wr_id;
 	wc.status = IB_WC_LOC_PROT_ERR;
@@ -268,6 +274,7 @@ static void ipath_ruc_loopback(struct ip
 	u64 sdata;
 	atomic64_t *maddr;
 	enum ib_wc_status send_status;
+	int release;
 
 	/*
 	 * Note that we check the responder QP state after
@@ -325,6 +332,7 @@ again:
 	memset(&wc, 0, sizeof wc);
 	send_status = IB_WC_SUCCESS;
 
+	release = 1;
 	sqp->s_sge.sge = wqe->sg_list[0];
 	sqp->s_sge.sg_list = wqe->sg_list + 1;
 	sqp->s_sge.num_sge = wqe->wr.num_sge;
@@ -368,6 +376,7 @@ again:
 					    wqe->wr.wr.rdma.rkey,
 					    IB_ACCESS_REMOTE_READ)))
 			goto acc_err;
+		release = 0;
 		qp->r_sge.sge = wqe->sg_list[0];
 		qp->r_sge.sg_list = wqe->sg_list + 1;
 		qp->r_sge.num_sge = wqe->wr.num_sge;
@@ -391,6 +400,8 @@ again:
 			(u64) atomic64_add_return(sdata, maddr) - sdata :
 			(u64) cmpxchg((u64 *) qp->r_sge.sge.vaddr,
 				      sdata, wqe->wr.wr.atomic.swap);
+		atomic_dec(&qp->r_sge.sge.mr->refcount);
+		qp->r_sge.num_sge = 0;
 		goto send_comp;
 
 	default:
@@ -407,14 +418,16 @@ again:
 		if (len > sge->sge_length)
 			len = sge->sge_length;
 		BUG_ON(len == 0);
-		ipath_copy_sge(&qp->r_sge, sge->vaddr, len);
+		ipath_copy_sge(&qp->r_sge, sge->vaddr, len, release);
 		sge->vaddr += len;
 		sge->length -= len;
 		sge->sge_length -= len;
 		if (sge->sge_length == 0) {
+			if (!release)
+				atomic_dec(&sge->mr->refcount);
 			if (--sqp->s_sge.num_sge)
 				*sge = *sqp->s_sge.sg_list++;
-		} else if (sge->length == 0 && sge->mr != NULL) {
+		} else if (sge->length == 0 && sge->mr->lkey) {
 			if (++sge->n >= IPATH_SEGSZ) {
 				if (++sge->m >= sge->mr->mapsz)
 					break;
@@ -427,6 +440,12 @@ again:
 		}
 		sqp->s_len -= len;
 	}
+	if (release)
+		while (qp->r_sge.num_sge) {
+			atomic_dec(&qp->r_sge.sge.mr->refcount);
+			if (--qp->r_sge.num_sge)
+				qp->r_sge.sge = *qp->r_sge.sg_list++;
+		}
 
 	if (!test_and_clear_bit(IPATH_R_WRID_VALID, &qp->r_aflags))
 		goto send_comp;
@@ -701,10 +720,16 @@ void ipath_send_complete(struct ipath_qp
 			 enum ib_wc_status status)
 {
 	u32 old_last, last;
+	unsigned i;
 
 	if (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_OR_FLUSH_SEND))
 		return;
 
+	for (i = 0; i < wqe->wr.num_sge; i++) {
+		struct ipath_sge *sge = &wqe->sg_list[i];
+
+		atomic_dec(&sge->mr->refcount);
+	}
 	/* See ch. 11.2.4.1 and 10.7.3.1 */
 	if (!(qp->s_flags & IPATH_S_SIGNAL_REQ_WR) ||
 	    (wqe->wr.send_flags & IB_SEND_SIGNALED) ||
@@ -726,6 +751,8 @@ void ipath_send_complete(struct ipath_qp
 	if (++last >= qp->s_size)
 		last = 0;
 	qp->s_last = last;
+	if (qp->s_acked == old_last)
+		qp->s_acked = last;
 	if (qp->s_cur == old_last)
 		qp->s_cur = last;
 	if (qp->s_tail == old_last)
diff -up a/drivers/infiniband/hw/ipath/ipath_sdma.c b/drivers/infiniband/hw/ipath/ipath_sdma.c
--- a/drivers/infiniband/hw/ipath/ipath_sdma.c	2008-12-10 18:04:03.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_sdma.c	2008-12-10 18:16:39.000000000 -0800
@@ -698,10 +698,8 @@ retry:
 
 	addr = dma_map_single(&dd->pcidev->dev, tx->txreq.map_addr,
 			      tx->map_len, DMA_TO_DEVICE);
-	if (dma_mapping_error(&dd->pcidev->dev, addr)) {
-		ret = -EIO;
-		goto unlock;
-	}
+	if (dma_mapping_error(&dd->pcidev->dev, addr))
+		goto ioerr;
 
 	dwoffset = tx->map_len >> 2;
 	make_sdma_desc(dd, sdmadesc, (u64) addr, dwoffset, 0);
@@ -741,6 +739,8 @@ retry:
 		dw = (len + 3) >> 2;
 		addr = dma_map_single(&dd->pcidev->dev, sge->vaddr, dw << 2,
 				      DMA_TO_DEVICE);
+		if (dma_mapping_error(addr))
+			goto unmap;
 		make_sdma_desc(dd, sdmadesc, (u64) addr, dw, dwoffset);
 		/* SDmaUseLargeBuf has to be set in every descriptor */
 		if (tx->txreq.flags & IPATH_SDMA_TXREQ_F_USELARGEBUF)
@@ -761,7 +761,7 @@ retry:
 		if (sge->sge_length == 0) {
 			if (--ss->num_sge)
 				*sge = *ss->sg_list++;
-		} else if (sge->length == 0 && sge->mr != NULL) {
+		} else if (sge->length == 0 && sge->mr->lkey) {
 			if (++sge->n >= IPATH_SEGSZ) {
 				if (++sge->m >= sge->mr->mapsz)
 					break;
@@ -798,7 +798,18 @@ retry:
 	list_add_tail(&tx->txreq.list, &dd->ipath_sdma_activelist);
 	if (tx->txreq.flags & IPATH_SDMA_TXREQ_F_VL15)
 		vl15_watchdog_enq(dd);
+	goto unlock;
 
+unmap:
+	while (tail != dd->ipath_sdma_descq_tail) {
+		if (!tail)
+			tail = dd->ipath_sdma_descq_cnt - 1;
+		else
+			tail--;
+		unmap_desc(dd, tail);
+	}
+ioerr:
+	ret = -EIO;
 unlock:
 	spin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);
 fail:
diff -up a/drivers/infiniband/hw/ipath/ipath_uc.c b/drivers/infiniband/hw/ipath/ipath_uc.c
--- a/drivers/infiniband/hw/ipath/ipath_uc.c	2008-12-10 18:04:07.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_uc.c	2008-12-10 18:07:56.000000000 -0800
@@ -225,6 +225,26 @@ unlock:
 	return ret;
 }
 
+static void fix_mr_refcount(struct ipath_qp *qp)
+{
+	unsigned i;
+
+	if (qp->r_sge.num_sge == qp->s_rdma_read_sge.num_sge)
+		return;
+	while (qp->r_sge.num_sge) {
+		atomic_dec(&qp->r_sge.sge.mr->refcount);
+		if (--qp->r_sge.num_sge)
+			qp->r_sge.sge = *qp->r_sge.sg_list++;
+	}
+	for (i = 0; i < qp->s_rdma_read_sge.num_sge; i++) {
+		struct ipath_sge *sge = i ?
+			&qp->s_rdma_read_sge.sg_list[i - 1] :
+			&qp->s_rdma_read_sge.sge;
+
+		atomic_inc(&sge->mr->refcount);
+	}
+}
+
 /**
  * ipath_uc_rcv - handle an incoming UC packet
  * @dev: the device the packet came in on
@@ -293,6 +313,11 @@ void ipath_uc_rcv(struct ipath_ibdev *de
 		 */
 		qp->r_psn = psn;
 	inv:
+		while (qp->r_sge.num_sge) {
+			atomic_dec(&qp->r_sge.sge.mr->refcount);
+			if (--qp->r_sge.num_sge)
+				qp->r_sge.sge = *qp->r_sge.sg_list++;
+		}
 		qp->r_state = OP(SEND_LAST);
 		switch (opcode) {
 		case OP(SEND_FIRST):
@@ -348,13 +373,13 @@ void ipath_uc_rcv(struct ipath_ibdev *de
 	send_first:
 		if (qp->r_flags & IPATH_R_REUSE_SGE) {
 			qp->r_flags &= ~IPATH_R_REUSE_SGE;
+			fix_mr_refcount(qp);
 			qp->r_sge = qp->s_rdma_read_sge;
 		} else if (!ipath_get_rwqe(qp, 0)) {
 			dev->n_pkt_drops++;
 			goto done;
-		}
-		/* Save the WQE so we can reuse it in case of an error. */
-		qp->s_rdma_read_sge = qp->r_sge;
+		} else
+			qp->s_rdma_read_sge = qp->r_sge;
 		qp->r_rcv_len = 0;
 		if (opcode == OP(SEND_ONLY))
 			goto send_last;
@@ -374,7 +399,7 @@ void ipath_uc_rcv(struct ipath_ibdev *de
 			dev->n_pkt_drops++;
 			goto done;
 		}
-		ipath_copy_sge(&qp->r_sge, data, pmtu);
+		ipath_copy_sge(&qp->r_sge, data, pmtu, 1);
 		break;
 
 	case OP(SEND_LAST_WITH_IMMEDIATE):
@@ -410,7 +435,12 @@ void ipath_uc_rcv(struct ipath_ibdev *de
 		}
 		wc.opcode = IB_WC_RECV;
 	last_imm:
-		ipath_copy_sge(&qp->r_sge, data, tlen);
+		ipath_copy_sge(&qp->r_sge, data, tlen, 1);
+		while (qp->r_sge.num_sge) {
+			atomic_dec(&qp->r_sge.sge.mr->refcount);
+			if (--qp->r_sge.num_sge)
+				qp->r_sge.sge = *qp->r_sge.sg_list++;
+		}
 		wc.wr_id = qp->r_wr_id;
 		wc.status = IB_WC_SUCCESS;
 		wc.qp = &qp->ibqp;
@@ -452,6 +482,7 @@ void ipath_uc_rcv(struct ipath_ibdev *de
 			}
 		} else {
 			qp->r_sge.sg_list = NULL;
+			qp->r_sge.num_sge = 0;
 			qp->r_sge.sge.mr = NULL;
 			qp->r_sge.sge.vaddr = NULL;
 			qp->r_sge.sge.length = 0;
@@ -478,7 +509,7 @@ void ipath_uc_rcv(struct ipath_ibdev *de
 			dev->n_pkt_drops++;
 			goto done;
 		}
-		ipath_copy_sge(&qp->r_sge, data, pmtu);
+		ipath_copy_sge(&qp->r_sge, data, pmtu, 1);
 		break;
 
 	case OP(RDMA_WRITE_LAST_WITH_IMMEDIATE):
@@ -533,7 +564,12 @@ void ipath_uc_rcv(struct ipath_ibdev *de
 			dev->n_pkt_drops++;
 			goto done;
 		}
-		ipath_copy_sge(&qp->r_sge, data, tlen);
+		ipath_copy_sge(&qp->r_sge, data, tlen, 1);
+		while (qp->r_sge.num_sge) {
+			atomic_dec(&qp->r_sge.sge.mr->refcount);
+			if (--qp->r_sge.num_sge)
+				qp->r_sge.sge = *qp->r_sge.sg_list++;
+		}
 		break;
 
 	default:
diff -up a/drivers/infiniband/hw/ipath/ipath_ud.c b/drivers/infiniband/hw/ipath/ipath_ud.c
--- a/drivers/infiniband/hw/ipath/ipath_ud.c	2008-12-10 18:04:07.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_ud.c	2008-12-10 18:07:56.000000000 -0800
@@ -132,14 +132,20 @@ static void ipath_ud_loopback(struct ipa
 	}
 	wqe = get_rwqe_ptr(rq, tail);
 	rsge.sg_list = qp->r_ud_sg_list;
-	if (!ipath_init_sge(qp, wqe, &rlen, &rsge)) {
+	if (unlikely(!ipath_init_sge(qp, wqe, &rlen, &rsge))) {
 		spin_unlock_irqrestore(&rq->lock, flags);
 		dev->n_pkt_drops++;
 		goto drop;
 	}
 	/* Silently drop packets which are too big. */
-	if (wc.byte_len > rlen) {
+	if (unlikely(wc.byte_len > rlen)) {
+		unsigned i;
+
 		spin_unlock_irqrestore(&rq->lock, flags);
+		for (i = 0; i < rsge.num_sge; i++) {
+			sge = i ? &rsge.sg_list[i - 1] : &rsge.sge;
+			atomic_dec(&sge->mr->refcount);
+		}
 		dev->n_pkt_drops++;
 		goto drop;
 	}
@@ -177,10 +183,10 @@ static void ipath_ud_loopback(struct ipa
 
 	ah_attr = &to_iah(swqe->wr.wr.ud.ah)->attr;
 	if (ah_attr->ah_flags & IB_AH_GRH) {
-		ipath_copy_sge(&rsge, &ah_attr->grh, sizeof(struct ib_grh));
+		ipath_copy_sge(&rsge, &ah_attr->grh, sizeof(struct ib_grh), 1);
 		wc.wc_flags |= IB_WC_GRH;
 	} else
-		ipath_skip_sge(&rsge, sizeof(struct ib_grh));
+		ipath_skip_sge(&rsge, sizeof(struct ib_grh), 1);
 	ssge.sg_list = swqe->sg_list + 1;
 	ssge.sge = *swqe->sg_list;
 	ssge.num_sge = swqe->wr.num_sge;
@@ -193,14 +199,14 @@ static void ipath_ud_loopback(struct ipa
 		if (len > sge->sge_length)
 			len = sge->sge_length;
 		BUG_ON(len == 0);
-		ipath_copy_sge(&rsge, sge->vaddr, len);
+		ipath_copy_sge(&rsge, sge->vaddr, len, 1);
 		sge->vaddr += len;
 		sge->length -= len;
 		sge->sge_length -= len;
 		if (sge->sge_length == 0) {
 			if (--ssge.num_sge)
 				*sge = *ssge.sg_list++;
-		} else if (sge->length == 0 && sge->mr != NULL) {
+		} else if (sge->length == 0 && sge->mr->lkey) {
 			if (++sge->n >= IPATH_SEGSZ) {
 				if (++sge->m >= sge->mr->mapsz)
 					break;
@@ -213,6 +219,11 @@ static void ipath_ud_loopback(struct ipa
 		}
 		length -= len;
 	}
+	while (rsge.num_sge) {
+		atomic_dec(&rsge.sge.mr->refcount);
+		if (--rsge.num_sge)
+			rsge.sge = *rsge.sg_list++;
+	}
 	wc.status = IB_WC_SUCCESS;
 	wc.opcode = IB_WC_RECV;
 	wc.qp = &qp->ibqp;
@@ -550,12 +561,17 @@ void ipath_ud_rcv(struct ipath_ibdev *de
 	}
 	if (has_grh) {
 		ipath_copy_sge(&qp->r_sge, &hdr->u.l.grh,
-			       sizeof(struct ib_grh));
+			       sizeof(struct ib_grh), 1);
 		wc.wc_flags |= IB_WC_GRH;
 	} else
-		ipath_skip_sge(&qp->r_sge, sizeof(struct ib_grh));
+		ipath_skip_sge(&qp->r_sge, sizeof(struct ib_grh), 1);
 	ipath_copy_sge(&qp->r_sge, data,
-		       wc.byte_len - sizeof(struct ib_grh));
+		       wc.byte_len - sizeof(struct ib_grh), 1);
+	while (qp->r_sge.num_sge) {
+		atomic_dec(&qp->r_sge.sge.mr->refcount);
+		if (--qp->r_sge.num_sge)
+			qp->r_sge.sge = *qp->r_sge.sg_list++;
+	}
 	if (!test_and_clear_bit(IPATH_R_WRID_VALID, &qp->r_aflags))
 		goto bail;
 	wc.wr_id = qp->r_wr_id;
diff -up a/drivers/infiniband/hw/ipath/ipath_verbs.c b/drivers/infiniband/hw/ipath/ipath_verbs.c
--- a/drivers/infiniband/hw/ipath/ipath_verbs.c	2008-12-10 18:04:07.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_verbs.c	2008-12-10 18:22:23.000000000 -0800
@@ -174,7 +174,8 @@ static __be64 sys_image_guid;
  * @data: the data to copy
  * @length: the length of the data
  */
-void ipath_copy_sge(struct ipath_sge_state *ss, void *data, u32 length)
+void ipath_copy_sge(struct ipath_sge_state *ss, void *data, u32 length,
+		    int release)
 {
 	struct ipath_sge *sge = &ss->sge;
 
@@ -194,9 +195,11 @@ void ipath_copy_sge(struct ipath_sge_sta
 		sge->length -= len;
 		sge->sge_length -= len;
 		if (sge->sge_length == 0) {
+			if (release)
+				atomic_dec(&sge->mr->refcount);
 			if (--ss->num_sge)
 				*sge = *ss->sg_list++;
-		} else if (sge->length == 0 && sge->mr != NULL) {
+		} else if (sge->length == 0 && sge->mr->lkey) {
 			if (++sge->n >= IPATH_SEGSZ) {
 				if (++sge->m >= sge->mr->mapsz)
 					break;
@@ -217,7 +220,7 @@ void ipath_copy_sge(struct ipath_sge_sta
  * @ss: the SGE state
  * @length: the number of bytes to skip
  */
-void ipath_skip_sge(struct ipath_sge_state *ss, u32 length)
+void ipath_skip_sge(struct ipath_sge_state *ss, u32 length, int release)
 {
 	struct ipath_sge *sge = &ss->sge;
 
@@ -233,9 +236,11 @@ void ipath_skip_sge(struct ipath_sge_sta
 		sge->length -= len;
 		sge->sge_length -= len;
 		if (sge->sge_length == 0) {
+			if (release)
+				atomic_dec(&sge->mr->refcount);
 			if (--ss->num_sge)
 				*sge = *ss->sg_list++;
-		} else if (sge->length == 0 && sge->mr != NULL) {
+		} else if (sge->length == 0 && sge->mr->lkey) {
 			if (++sge->n >= IPATH_SEGSZ) {
 				if (++sge->m >= sge->mr->mapsz)
 					break;
@@ -282,7 +287,7 @@ static u32 ipath_count_sge(struct ipath_
 		if (sge.sge_length == 0) {
 			if (--num_sge)
 				sge = *sg_list++;
-		} else if (sge.length == 0 && sge.mr != NULL) {
+		} else if (sge.length == 0 && sge.mr->lkey) {
 			if (++sge.n >= IPATH_SEGSZ) {
 				if (++sge.m >= sge.mr->mapsz)
 					break;
@@ -321,7 +326,7 @@ static void ipath_copy_from_sge(void *da
 		if (sge->sge_length == 0) {
 			if (--ss->num_sge)
 				*sge = *ss->sg_list++;
-		} else if (sge->length == 0 && sge->mr != NULL) {
+		} else if (sge->length == 0 && sge->mr->lkey) {
 			if (++sge->n >= IPATH_SEGSZ) {
 				if (++sge->m >= sge->mr->mapsz)
 					break;
@@ -406,10 +411,11 @@ static int ipath_post_one_send(struct ip
 	wqe = get_swqe_ptr(qp, qp->s_head);
 	wqe->wr = *wr;
 	wqe->length = 0;
+	j = 0;
 	if (wr->num_sge) {
 		acc = wr->opcode >= IB_WR_RDMA_READ ?
 			IB_ACCESS_LOCAL_WRITE : 0;
-		for (i = 0, j = 0; i < wr->num_sge; i++) {
+		for (i = 0; i < wr->num_sge; i++) {
 			u32 length = wr->sg_list[i].length;
 			int ok;
 
@@ -418,7 +424,7 @@ static int ipath_post_one_send(struct ip
 			ok = ipath_lkey_ok(qp, &wqe->sg_list[j],
 					   &wr->sg_list[i], acc);
 			if (!ok)
-				goto bail_inval;
+				goto bail_inval_free;
 			wqe->length += length;
 			j++;
 		}
@@ -427,15 +433,21 @@ static int ipath_post_one_send(struct ip
 	if (qp->ibqp.qp_type == IB_QPT_UC ||
 	    qp->ibqp.qp_type == IB_QPT_RC) {
 		if (wqe->length > 0x80000000U)
-			goto bail_inval;
+			goto bail_inval_free;
 	} else if (wqe->length > to_idev(qp->ibqp.device)->dd->ipath_ibmtu)
-		goto bail_inval;
+		goto bail_inval_free;
 	wqe->ssn = qp->s_ssn++;
 	qp->s_head = next;
 
 	ret = 0;
 	goto bail;
 
+bail_inval_free:
+	while (j) {
+		struct ipath_sge *sge = &wqe->sg_list[--j];
+
+		atomic_dec(&sge->mr->refcount);
+	}
 bail_inval:
 	ret = -EINVAL;
 bail:
@@ -760,7 +772,7 @@ static void ipath_ib_timer(struct ipath_
 		resend = qp->timer_next;
 
 		spin_lock_irqsave(&qp->s_lock, flags);
-		if (qp->s_last != qp->s_tail &&
+		if (qp->s_acked != qp->s_tail &&
 		    ib_ipath_state_ops[qp->state] & IPATH_PROCESS_SEND_OK) {
 			dev->n_timeouts++;
 			ipath_restart_rc(qp, qp->s_last_psn + 1);
@@ -796,7 +808,7 @@ static void update_sge(struct ipath_sge_
 	if (sge->sge_length == 0) {
 		if (--ss->num_sge)
 			*sge = *ss->sg_list++;
-	} else if (sge->length == 0 && sge->mr != NULL) {
+	} else if (sge->length == 0 && sge->mr->lkey) {
 		if (++sge->n >= IPATH_SEGSZ) {
 			if (++sge->m >= sge->mr->mapsz)
 				return;
@@ -1047,6 +1059,8 @@ static void sdma_complete(void *cookie, 
 		spin_lock_irqsave(&qp->s_lock, flags);
 		if (tx->wqe)
 			ipath_send_complete(qp, tx->wqe, ibs);
+		else if (qp->ibqp.qp_type == IB_QPT_RC)
+			ipath_rc_send_complete(qp, &tx->hdr.hdr);
 		if ((ib_ipath_state_ops[qp->state] & IPATH_FLUSH_SEND &&
 		     qp->s_last != qp->s_head) ||
 		    (qp->s_flags & IPATH_S_WAIT_DMA))
@@ -1057,6 +1071,10 @@ static void sdma_complete(void *cookie, 
 		spin_lock_irqsave(&qp->s_lock, flags);
 		ipath_send_complete(qp, tx->wqe, ibs);
 		spin_unlock_irqrestore(&qp->s_lock, flags);
+	} else if (qp->ibqp.qp_type == IB_QPT_RC) {
+		spin_lock_irqsave(&qp->s_lock, flags);
+		ipath_rc_send_complete(qp, &tx->hdr.hdr);
+		spin_unlock_irqrestore(&qp->s_lock, flags);
 	}
 
 	if (tx->txreq.flags & IPATH_SDMA_TXREQ_F_FREEBUF)
@@ -1314,6 +1332,10 @@ done:
 		spin_lock_irqsave(&qp->s_lock, flags);
 		ipath_send_complete(qp, qp->s_wqe, IB_WC_SUCCESS);
 		spin_unlock_irqrestore(&qp->s_lock, flags);
+	} else if (qp->ibqp.qp_type == IB_QPT_RC) {
+		spin_lock_irqsave(&qp->s_lock, flags);
+		ipath_rc_send_complete(qp, ibhdr);
+		spin_unlock_irqrestore(&qp->s_lock, flags);
 	}
 	ret = 0;
 bail:
@@ -2238,6 +2260,8 @@ void ipath_unregister_ib_device(struct i
 		ipath_dev_err(dev->dd, "piowait list not empty!\n");
 	if (!list_empty(&dev->rnrwait))
 		ipath_dev_err(dev->dd, "rnrwait list not empty!\n");
+	if (dev->dma_mr)
+		ipath_dev_err(dev->dd, "DMA MR not NULL!\n");
 	if (!ipath_mcast_tree_empty())
 		ipath_dev_err(dev->dd, "multicast table memory leak!\n");
 	/*
@@ -2287,10 +2311,12 @@ static ssize_t show_stats(struct device 
 		container_of(device, struct ipath_ibdev, ibdev.dev);
 	int i;
 	int len;
+	struct ipath_qp_table *qpt;
+	unsigned long flags;
 
 	len = sprintf(buf,
 		      "RC resends  %d\n"
-		      "RC no QACK  %d\n"
+		      "RC QACKs    %d\n"
 		      "RC ACKs     %d\n"
 		      "RC SEQ NAKs %d\n"
 		      "RC RDMA seq %d\n"
@@ -2298,6 +2324,7 @@ static ssize_t show_stats(struct device 
 		      "RC OTH NAKs %d\n"
 		      "RC timeouts %d\n"
 		      "RC RDMA dup %d\n"
+		      "RC DComp    %d\n"
 		      "piobuf wait %d\n"
 		      "unaligned   %d\n"
 		      "PKT drops   %d\n"
@@ -2305,7 +2332,8 @@ static ssize_t show_stats(struct device 
 		      dev->n_rc_resends, dev->n_rc_qacks, dev->n_rc_acks,
 		      dev->n_seq_naks, dev->n_rdma_seq, dev->n_rnr_naks,
 		      dev->n_other_naks, dev->n_timeouts,
-		      dev->n_rdma_dup_busy, dev->n_piowait, dev->n_unaligned,
+		      dev->n_rdma_dup_busy, dev->n_rc_delayed_comp,
+		      dev->n_piowait, dev->n_unaligned,
 		      dev->n_pkt_drops, dev->n_wqe_errs);
 	for (i = 0; i < ARRAY_SIZE(dev->opstats); i++) {
 		const struct ipath_opcode_stats *si = &dev->opstats[i];
@@ -2316,6 +2344,25 @@ static ssize_t show_stats(struct device 
 			       (unsigned long long) si->n_packets,
 			       (unsigned long long) si->n_bytes);
 	}
+	qpt = &dev->qp_table;
+	spin_lock_irqsave(&qpt->lock, flags);
+	for (i = 0; i < qpt->max; i++) {
+		struct ipath_qp *qp;
+		for (qp = qpt->table[i]; qp != NULL; qp = qp->next) {
+			len += sprintf(buf + len,
+				"QP%u %x PSN %x %x %x %x %x (%u %u %u %u %u)\n",
+				qp->ibqp.qp_num,
+				qp->s_flags,
+				qp->s_last_psn,
+				qp->s_psn,
+				qp->s_next_psn,
+				qp->s_sending_psn,
+				qp->s_sending_hpsn,
+				qp->s_last, qp->s_acked, qp->s_cur,
+				qp->s_tail, qp->s_head);
+		}
+	}
+	spin_unlock_irqrestore(&qpt->lock, flags);
 	return len;
 }
 
diff -up a/drivers/infiniband/hw/ipath/ipath_verbs.h b/drivers/infiniband/hw/ipath/ipath_verbs.h
--- a/drivers/infiniband/hw/ipath/ipath_verbs.h	2008-12-10 18:04:07.000000000 -0800
+++ b/drivers/infiniband/hw/ipath/ipath_verbs.h	2008-12-10 18:07:56.000000000 -0800
@@ -248,6 +248,7 @@ struct ipath_mregion {
 	int access_flags;
 	u32 max_segs;		/* number of ipath_segs in all the arrays */
 	u32 mapsz;		/* size of the map array */
+	atomic_t refcount;
 	struct ipath_segarray *map[0];	/* the segments */
 };
 
@@ -385,6 +386,8 @@ struct ipath_qp {
 	u32 s_rdma_read_len;	/* total length of s_rdma_read_sge */
 	u32 s_next_psn;		/* PSN for next request */
 	u32 s_last_psn;		/* last response PSN processed */
+	u32 s_sending_psn;	/* lowest PSN that is being sent */
+	u32 s_sending_hpsn;	/* highest PSN that is being sent */
 	u32 s_psn;		/* current packet sequence number */
 	u32 s_ack_rdma_psn;	/* PSN for sending RDMA read responses */
 	u32 s_ack_psn;		/* PSN for acking sends and RDMA writes */
@@ -427,7 +430,8 @@ struct ipath_qp {
 	u32 s_head;		/* new entries added here */
 	u32 s_tail;		/* next entry to process */
 	u32 s_cur;		/* current work queue entry */
-	u32 s_last;		/* last un-ACK'ed entry */
+	u32 s_acked;		/* last un-ACK'ed entry */
+	u32 s_last;		/* last completed entry */
 	u32 s_ssn;		/* SSN of tail entry */
 	u32 s_lsn;		/* limit sequence number (credit) */
 	struct ipath_swqe *s_wq;	/* send work queue */
@@ -539,6 +543,7 @@ struct ipath_ibdev {
 	struct list_head pending_mmaps;
 	spinlock_t mmap_offset_lock;
 	u32 mmap_offset;
+	struct ipath_mregion *dma_mr;
 	int ib_unit;		/* This is the device number */
 	u16 sm_lid;		/* in host order */
 	u8 sm_sl;
@@ -601,6 +606,7 @@ struct ipath_ibdev {
 	u32 n_rc_resends;
 	u32 n_rc_acks;
 	u32 n_rc_qacks;
+	u32 n_rc_delayed_comp;
 	u32 n_seq_naks;
 	u32 n_rdma_seq;
 	u32 n_rnr_naks;
@@ -759,9 +765,10 @@ unsigned ipath_ib_rate_to_mult(enum ib_r
 int ipath_verbs_send(struct ipath_qp *qp, struct ipath_ib_header *hdr,
 		     u32 hdrwords, struct ipath_sge_state *ss, u32 len);
 
-void ipath_copy_sge(struct ipath_sge_state *ss, void *data, u32 length);
+void ipath_copy_sge(struct ipath_sge_state *ss, void *data, u32 length,
+		    int release);
 
-void ipath_skip_sge(struct ipath_sge_state *ss, u32 length);
+void ipath_skip_sge(struct ipath_sge_state *ss, u32 length, int release);
 
 void ipath_uc_rcv(struct ipath_ibdev *dev, struct ipath_ib_header *hdr,
 		  int has_grh, void *data, u32 tlen, struct ipath_qp *qp);
@@ -771,6 +778,8 @@ void ipath_rc_rcv(struct ipath_ibdev *de
 
 void ipath_restart_rc(struct ipath_qp *qp, u32 psn);
 
+void ipath_rc_send_complete(struct ipath_qp *qp, struct ipath_ib_header *hdr);
+
 void ipath_rc_error(struct ipath_qp *qp, enum ib_wc_status err);
 
 int ipath_post_ud_send(struct ipath_qp *qp, struct ib_send_wr *wr);
@@ -781,7 +790,7 @@ void ipath_ud_rcv(struct ipath_ibdev *de
 int ipath_alloc_lkey(struct ipath_lkey_table *rkt,
 		     struct ipath_mregion *mr);
 
-void ipath_free_lkey(struct ipath_lkey_table *rkt, u32 lkey);
+int ipath_free_lkey(struct ipath_ibdev *dev, struct ipath_mregion *mr);
 
 int ipath_lkey_ok(struct ipath_qp *qp, struct ipath_sge *isge,
 		  struct ib_sge *sge, int acc);
