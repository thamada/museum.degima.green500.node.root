---
 drivers/infiniband/ulp/sdp/sdp.h       |   26 --------------------------
 drivers/infiniband/ulp/sdp/sdp_bcopy.c |   24 ++++++++++++------------
 drivers/infiniband/ulp/sdp/sdp_cma.c   |    2 --
 drivers/infiniband/ulp/sdp/sdp_main.c  |   19 +++++++++----------
 4 files changed, 21 insertions(+), 50 deletions(-)

Index: ofed_kernel.last_bcopy/drivers/infiniband/ulp/sdp/sdp.h
===================================================================
--- ofed_kernel.last_bcopy.orig/drivers/infiniband/ulp/sdp/sdp.h
+++ ofed_kernel.last_bcopy/drivers/infiniband/ulp/sdp/sdp.h
@@ -605,30 +605,4 @@ static inline char *mid2str(int mid)
 	return mid2str[mid];
 }
 
-static inline struct sk_buff *sdp_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp)
-{
-	struct sk_buff *skb;
-
-	/* The TCP header must be at least 32-bit aligned.  */
-	size = ALIGN(size, 4);
-
-	skb = alloc_skb_fclone(size + sk->sk_prot->max_header, gfp);
-	if (skb) {
-		if (sk_wmem_schedule(sk, skb->truesize)) {
-			/*
-			 * Make sure that we have exactly size bytes
-			 * available to the caller, no more, no less.
-			 */
-			skb_reserve(skb, skb_tailroom(skb) - size);
-			return skb;
-		}
-		__kfree_skb(skb);
-	} else {
-		sk->sk_prot->enter_memory_pressure(sk);
-		sk_stream_moderate_sndbuf(sk);
-	}
-	return NULL;
-}
-
-
 #endif
Index: ofed_kernel.last_bcopy/drivers/infiniband/ulp/sdp/sdp_bcopy.c
===================================================================
--- ofed_kernel.last_bcopy.orig/drivers/infiniband/ulp/sdp/sdp_bcopy.c
+++ ofed_kernel.last_bcopy/drivers/infiniband/ulp/sdp/sdp_bcopy.c
@@ -174,7 +174,7 @@ int sdp_post_credits(struct sdp_sock *ss
 	if (likely(tx_credits(ssk) > 1) &&
 	    likely(sdp_tx_ring_slots_left(&ssk->tx_ring))) {
 		struct sk_buff *skb;
-		skb = sdp_stream_alloc_skb(&ssk->isk.sk,
+		skb = sk_stream_alloc_skb(&ssk->isk.sk,
 					  sizeof(struct sdp_bsdh),
 					  GFP_KERNEL);
 		if (!skb)
@@ -228,7 +228,7 @@ void _sdp_post_sends(const char *func, i
 	    sdp_tx_ring_slots_left(&ssk->tx_ring)) {
 		struct sdp_chrecvbuf *resp_size;
 		ssk->recv_request = 0;
-		skb = sdp_stream_alloc_skb(&ssk->isk.sk,
+		skb = sk_stream_alloc_skb(&ssk->isk.sk,
 					  sizeof(struct sdp_bsdh) +
 					  sizeof(*resp_size),
 					  gfp_page);
@@ -263,7 +263,7 @@ void _sdp_post_sends(const char *func, i
 	    ring_head(ssk->tx_ring) > ssk->sent_request_head + SDP_RESIZE_WAIT &&
 	    sdp_tx_ring_slots_left(&ssk->tx_ring)) {
 		struct sdp_chrecvbuf *req_size;
-		skb = sdp_stream_alloc_skb(&ssk->isk.sk,
+		skb = sk_stream_alloc_skb(&ssk->isk.sk,
 					  sizeof(struct sdp_bsdh) +
 					  sizeof(*req_size),
 					  gfp_page);
@@ -286,7 +286,7 @@ void _sdp_post_sends(const char *func, i
 	    likely(sdp_tx_ring_slots_left(&ssk->tx_ring)) &&
 	    likely((1 << ssk->isk.sk.sk_state) &
 		    (TCPF_ESTABLISHED | TCPF_FIN_WAIT1))) {
-		skb = sdp_stream_alloc_skb(&ssk->isk.sk,
+		skb = sk_stream_alloc_skb(&ssk->isk.sk,
 					  sizeof(struct sdp_bsdh),
 					  GFP_KERNEL);
 		/* FIXME */
@@ -305,7 +305,7 @@ void _sdp_post_sends(const char *func, i
 			!ssk->isk.sk.sk_send_head &&
 			tx_credits(ssk) > 1) {
 		ssk->sdp_disconnect = 0;
-		skb = sdp_stream_alloc_skb(&ssk->isk.sk,
+		skb = sk_stream_alloc_skb(&ssk->isk.sk,
 					  sizeof(struct sdp_bsdh),
 					  gfp_page);
 		/* FIXME */
Index: ofed_kernel.last_bcopy/drivers/infiniband/ulp/sdp/sdp_main.c
===================================================================
--- ofed_kernel.last_bcopy.orig/drivers/infiniband/ulp/sdp/sdp_main.c
+++ ofed_kernel.last_bcopy/drivers/infiniband/ulp/sdp/sdp_main.c
@@ -509,7 +509,7 @@ static void sdp_close(struct sock *sk, l
 		__kfree_skb(skb);
 	}
 
-	sk_mem_reclaim(sk);
+	sk_stream_mem_reclaim(sk);
 
 	/* As outlined in draft-ietf-tcpimpl-prob-03.txt, section
 	 * 3.10, we send a RST here because data was lost.  To
@@ -1214,7 +1214,7 @@ static inline void sdp_mark_urg(struct s
 {
 	if (unlikely(flags & MSG_OOB)) {
 		struct sk_buff *skb = sk->sk_write_queue.prev;
-		TCP_SKB_CB(skb)->flags |= TCPCB_FLAG_URG;
+		TCP_SKB_CB(skb)->flags |= TCPCB_URG;
 	}
 }
 
@@ -1230,8 +1230,7 @@ static inline void skb_entail(struct soc
 {
         skb_header_release(skb);
         __skb_queue_tail(&sk->sk_write_queue, skb);
-	sk->sk_wmem_queued += skb->truesize;
-        sk_mem_charge(sk, skb->truesize);
+	sk_charge_skb(sk, skb);
         if (!sk->sk_send_head)
                 sk->sk_send_head = skb;
         if (ssk->nonagle & TCP_NAGLE_PUSH)
@@ -1399,7 +1398,7 @@ static inline int sdp_bcopy_get(struct s
 		if (copy > PAGE_SIZE - off)
 			copy = PAGE_SIZE - off;
 
-		if (!sk_wmem_schedule(sk, copy))
+		if (!sk_stream_wmem_schedule(sk, copy))
 			return SDP_DO_WAIT_MEM;
 
 		if (!page) {
@@ -1474,7 +1473,7 @@ static inline int sdp_bzcopy_get(struct 
 		if (left <= this_page)
 			this_page = left;
 
-		if (!sk_wmem_schedule(sk, copy))
+		if (!sk_stream_wmem_schedule(sk, copy))
 			return SDP_DO_WAIT_MEM;
 
 		get_page(bz->pages[bz->cur_page]);
@@ -1682,7 +1681,7 @@ new_segment:
 						goto wait_for_sndbuf;
 				}
 
-				skb = sdp_stream_alloc_skb(sk, 0, sk->sk_allocation);
+ 				skb = sk_stream_alloc_skb(sk, 0, sk->sk_allocation);
 				if (!skb)
 					goto wait_for_memory;
 
@@ -1712,7 +1711,7 @@ new_segment:
 
 			/* OOB data byte should be the last byte of
 			   the data payload */
-			if (unlikely(TCP_SKB_CB(skb)->flags & TCPCB_FLAG_URG) &&
+			if (unlikely(TCP_SKB_CB(skb)->flags & TCPCB_URG) &&
 			    !(flags & MSG_OOB)) {
 				sdp_mark_push(ssk, skb);
 				goto new_segment;
@@ -1798,7 +1797,7 @@ do_fault:
 		if (sk->sk_send_head == skb)
 			sk->sk_send_head = NULL;
 		__skb_unlink(skb, &sk->sk_write_queue);
-		sk_wmem_free_skb(sk, skb);
+		sk_stream_free_skb(sk, skb);
 	}
 
 do_error:
Index: ofed_kernel.last_bcopy/drivers/infiniband/ulp/sdp/sdp_rx.c
===================================================================
--- ofed_kernel.last_bcopy.orig/drivers/infiniband/ulp/sdp/sdp_rx.c
+++ ofed_kernel.last_bcopy/drivers/infiniband/ulp/sdp/sdp_rx.c
@@ -143,7 +143,7 @@ static void sdp_fin(struct sock *sk)
 	}
 
 
-	sk_mem_reclaim(sk);
+	sk_stream_mem_reclaim(sk);
 
 	if (!sock_flag(sk, SOCK_DEAD)) {
 		sk->sk_state_change(sk);
@@ -179,11 +179,11 @@ static int sdp_post_recv(struct sdp_sock
 	/* TODO: allocate from cache */
 
 	if (unlikely(ssk->isk.sk.sk_allocation)) {
-		skb = sdp_stream_alloc_skb(&ssk->isk.sk, SDP_HEAD_SIZE,
+		skb = sk_stream_alloc_skb(&ssk->isk.sk, SDP_HEAD_SIZE,
 					  ssk->isk.sk.sk_allocation);
 		gfp_page = ssk->isk.sk.sk_allocation | __GFP_HIGHMEM;
 	} else {
-		skb = sdp_stream_alloc_skb(&ssk->isk.sk, SDP_HEAD_SIZE,
+		skb = sk_stream_alloc_skb(&ssk->isk.sk, SDP_HEAD_SIZE,
 					  GFP_KERNEL);
 		gfp_page = GFP_HIGHUSER;
 	}
Index: ofed_kernel.last_bcopy/drivers/infiniband/ulp/sdp/sdp_tx.c
===================================================================
--- ofed_kernel.last_bcopy.orig/drivers/infiniband/ulp/sdp/sdp_tx.c
+++ ofed_kernel.last_bcopy/drivers/infiniband/ulp/sdp/sdp_tx.c
@@ -82,7 +82,7 @@ void sdp_post_send(struct sdp_sock *ssk,
 	SDPSTATS_HIST(send_size, skb->len);
 
 	h->mid = mid;
-	if (unlikely(TCP_SKB_CB(skb)->flags & TCPCB_FLAG_URG))
+	if (unlikely(TCP_SKB_CB(skb)->flags & TCPCB_URG))
 		h->flags = SDP_OOB_PRES | SDP_OOB_PEND;
 	else
 		h->flags = 0;
@@ -130,7 +130,7 @@ void sdp_post_send(struct sdp_sock *ssk,
 	tx_wr.num_sge = frags + 1;
 	tx_wr.opcode = IB_WR_SEND;
 	tx_wr.send_flags = IB_SEND_SIGNALED;
-	if (unlikely(TCP_SKB_CB(skb)->flags & TCPCB_FLAG_URG))
+	if (unlikely(TCP_SKB_CB(skb)->flags & TCPCB_URG))
 		tx_wr.send_flags |= IB_SEND_SOLICITED;
 	
 	{
@@ -217,7 +217,7 @@ static int sdp_handle_send_comp(struct s
 		sdp_prf1(&ssk->isk.sk, skb, "tx completion. mseq:%d", ntohl(h->mseq));
 	}
 
-	sk_wmem_free_skb(&ssk->isk.sk, skb);
+	sk_stream_free_skb(&ssk->isk.sk, skb);
 
 	return 0;
 }
