diff --git a/drivers/net/cxgb3/adapter.h b/drivers/net/cxgb3/adapter.h
index 3b33ecb..21dad82 100644
--- a/drivers/net/cxgb3/adapter.h
+++ b/drivers/net/cxgb3/adapter.h
@@ -48,12 +48,10 @@
 
 struct vlan_group;
 struct adapter;
-struct sge_qset;
 
 struct port_info {
 	struct adapter *adapter;
 	struct vlan_group *vlan_grp;
-	struct sge_qset *qs;
 	u8 port_id;
 	u8 rx_csum_offload;
 	u8 nqsets;
@@ -183,8 +181,6 @@ enum {				/* per port SGE statistics */
 #define T3_MAX_LRO_MAX_PKTS 64
 
 struct sge_qset {		/* an SGE queue set */
-	struct adapter *adap;
-	struct napi_struct napi;
 	struct sge_rspq rspq;
 	struct sge_fl fl[SGE_RXQ_PER_SET];
 	struct sge_txq txq[SGE_TXQ_PER_SET];
@@ -195,7 +191,7 @@ struct sge_qset {		/* an SGE queue set */
 	int lro_enabled;
 	int lro_frag_len;
 	void *lro_va;
-	struct net_device *netdev;
+	struct net_device *netdev;	/* associated net device */
 	unsigned long txq_stopped;	/* which Tx queues are stopped */
 	struct timer_list tx_reclaim_timer;	/* reclaims TX buffers */
 	unsigned long port_stats[SGE_PSTAT_MAX];
@@ -240,6 +236,12 @@ struct adapter {
 	struct delayed_work adap_check_task;
 	struct work_struct ext_intr_handler_task;
 
+	/*
+	 * Dummy netdevices are needed when using multiple receive queues with
+	 * NAPI as each netdevice can service only one queue.
+	 */
+	struct net_device *dummy_netdev[SGE_QSETS - 1];
+
 	struct dentry *debugfs_root;
 
 	struct mutex mdio_lock;
@@ -266,6 +268,12 @@ static inline struct port_info *adap2pinfo(struct adapter *adap, int idx)
 	return netdev_priv(adap->port[idx]);
 }
 
+/*
+ * We use the spare atalk_ptr to map a net device to its SGE queue set.
+ * This is a macro so it can be used as l-value.
+ */
+#define dev2qset(netdev) ((netdev)->atalk_ptr)
+
 #define OFFLOAD_DEVMAP_BIT 15
 
 #define tdev2adap(d) container_of(d, struct adapter, tdev)
@@ -292,7 +300,7 @@ int t3_mgmt_tx(struct adapter *adap, struct sk_buff *skb);
 void t3_update_qset_coalesce(struct sge_qset *qs, const struct qset_params *p);
 int t3_sge_alloc_qset(struct adapter *adapter, unsigned int id, int nports,
 		      int irq_vec_idx, const struct qset_params *p,
-		      int ntxq, struct net_device *dev);
+		      int ntxq, struct net_device *netdev);
 int t3_get_desc(const struct sge_qset *qs, unsigned int qnum, unsigned int idx,
 		unsigned char *data);
 irqreturn_t t3_sge_intr_msix(int irq, void *cookie);
diff --git a/drivers/net/cxgb3/cxgb3_main.c b/drivers/net/cxgb3/cxgb3_main.c
index 0f4c694..342d441 100644
--- a/drivers/net/cxgb3/cxgb3_main.c
+++ b/drivers/net/cxgb3/cxgb3_main.c
@@ -435,17 +435,49 @@ static void setup_rss(struct adapter *adap)
 		      V_RRCPLCPUSIZE(6) | F_HASHTOEPLITZ, cpus, rspq_map);
 }
 
-static void init_napi(struct adapter *adap)
+/*
+ * If we have multiple receive queues per port serviced by NAPI we need one
+ * netdevice per queue as NAPI operates on netdevices.  We already have one
+ * netdevice, namely the one associated with the interface, so we use dummy
+ * ones for any additional queues.  Note that these netdevices exist purely
+ * so that NAPI has something to work with, they do not represent network
+ * ports and are not registered.
+ */
+static int init_dummy_netdevs(struct adapter *adap)
 {
-	int i;
+	int i, j, dummy_idx = 0;
+	struct net_device *nd;
+
+	for_each_port(adap, i) {
+		struct net_device *dev = adap->port[i];
+		const struct port_info *pi = netdev_priv(dev);
+
+		for (j = 0; j < pi->nqsets - 1; j++) {
+			if (!adap->dummy_netdev[dummy_idx]) {
+				struct port_info *p;
+
+				nd = alloc_netdev(sizeof(*p), "", ether_setup);
+				if (!nd)
+					goto free_all;
 
-	for (i = 0; i < SGE_QSETS; i++) {
-		struct sge_qset *qs = &adap->sge.qs[i];
+				p = netdev_priv(nd);
+				p->adapter = adap;
+				nd->weight = 64;
+				set_bit(__LINK_STATE_START, &nd->state);
+				adap->dummy_netdev[dummy_idx] = nd;
+			}
+			strcpy(adap->dummy_netdev[dummy_idx]->name, dev->name);
+			dummy_idx++;
+		}
+	}
+	return 0;
 
-		if (qs->adap)
-			netif_napi_add(qs->netdev, &qs->napi, qs->napi.poll,
-				       64);
+free_all:
+	while (--dummy_idx >= 0) {
+		free_netdev(adap->dummy_netdev[dummy_idx]);
+		adap->dummy_netdev[dummy_idx] = NULL;
 	}
+	return -ENOMEM;
 }
 
 /*
@@ -456,18 +488,20 @@ static void init_napi(struct adapter *adap)
 static void quiesce_rx(struct adapter *adap)
 {
 	int i;
+	struct net_device *dev;
 
-	for (i = 0; i < SGE_QSETS; i++)
-		if (adap->sge.qs[i].adap)
-			napi_disable(&adap->sge.qs[i].napi);
-}
+	for_each_port(adap, i) {
+		dev = adap->port[i];
+		while (test_bit(__LINK_STATE_RX_SCHED, &dev->state))
+			msleep(1);
+	}
 
-static void enable_all_napi(struct adapter *adap)
-{
-	int i;
-	for (i = 0; i < SGE_QSETS; i++)
-		if (adap->sge.qs[i].adap)
-			napi_enable(&adap->sge.qs[i].napi);
+	for (i = 0; i < ARRAY_SIZE(adap->dummy_netdev); i++) {
+		dev = adap->dummy_netdev[i];
+		if (dev)
+			while (test_bit(__LINK_STATE_RX_SCHED, &dev->state))
+				msleep(1);
+	}
 }
 
 /**
@@ -480,7 +514,7 @@ static void enable_all_napi(struct adapter *adap)
  */
 static int setup_sge_qsets(struct adapter *adap)
 {
-	int i, j, err, irq_idx = 0, qset_idx = 0;
+	int i, j, err, irq_idx = 0, qset_idx = 0, dummy_dev_idx = 0;
 	unsigned int ntxq = SGE_TXQ_PER_SET;
 
 	if (adap->params.rev > 0 && !(adap->flags & USING_MSI))
@@ -488,14 +522,15 @@ static int setup_sge_qsets(struct adapter *adap)
 
 	for_each_port(adap, i) {
 		struct net_device *dev = adap->port[i];
-		struct port_info *pi = netdev_priv(dev);
+		const struct port_info *pi = netdev_priv(dev);
 
-		pi->qs = &adap->sge.qs[pi->first_qset];
 		for (j = 0; j < pi->nqsets; ++j, ++qset_idx) {
 			err = t3_sge_alloc_qset(adap, qset_idx, 1,
 				(adap->flags & USING_MSIX) ? qset_idx + 1 :
 							     irq_idx,
-				&adap->params.sge.qset[qset_idx], ntxq, dev);
+				&adap->params.sge.qset[qset_idx], ntxq,
+				j == 0 ? dev :
+					 adap-> dummy_netdev[dummy_dev_idx++]);
 			if (err) {
 				t3_free_sge_resources(adap);
 				return err;
@@ -909,6 +944,10 @@ static int cxgb_up(struct adapter *adap)
 				goto out;
 		}
 
+ 		err = init_dummy_netdevs(adap);
+ 		if (err)
+ 			goto out;
+
 		err = t3_init_hw(adap, 0);
 		if (err)
 			goto out;
@@ -921,7 +960,6 @@ static int cxgb_up(struct adapter *adap)
 			goto out;
 
 		setup_rss(adap);
-		init_napi(adap);
 		adap->flags |= FULL_INIT_DONE;
 	}
 
@@ -949,7 +987,6 @@ static int cxgb_up(struct adapter *adap)
 				      adap->name, adap)))
 		goto irq_err;
 
-	enable_all_napi(adap);
 	t3_sge_start(adap);
 	t3_intr_enable(adap);
 
@@ -1086,10 +1123,8 @@ static int cxgb_open(struct net_device *dev)
 	int other_ports = adapter->open_device_map & PORT_MASK;
 	int err;
 
-	if (!adapter->open_device_map && (err = cxgb_up(adapter)) < 0) {
-		quiesce_rx(adapter);
+	if (!adapter->open_device_map && (err = cxgb_up(adapter)) < 0)
 		return err;
-	}
 
 	set_bit(pi->port_id, &adapter->open_device_map);
 	if (is_offload(adapter) && !ofld_disable) {
@@ -2736,6 +2771,7 @@ static int __devinit init_one(struct pci_dev *pdev,
 #ifdef CONFIG_NET_POLL_CONTROLLER
 		netdev->poll_controller = cxgb_netpoll;
 #endif
+		netdev->weight = 64;
 
 		SET_ETHTOOL_OPS(netdev, &cxgb_ethtool_ops);
 	}
@@ -2836,6 +2872,12 @@ static void __devexit remove_one(struct pci_dev *pdev)
 		t3_free_sge_resources(adapter);
 		cxgb_disable_msi(adapter);
 
+		for (i = 0; i < ARRAY_SIZE(adapter->dummy_netdev); i++)
+			if (adapter->dummy_netdev[i]) {
+				free_netdev(adapter->dummy_netdev[i]);
+				adapter->dummy_netdev[i] = NULL;
+			}
+
 		for_each_port(adapter, i)
 			if (adapter->port[i])
 				free_netdev(adapter->port[i]);
diff --git a/drivers/net/cxgb3/sge.c b/drivers/net/cxgb3/sge.c
index f6bc6fe..3bbf626 100644
--- a/drivers/net/cxgb3/sge.c
+++ b/drivers/net/cxgb3/sge.c
@@ -618,6 +618,9 @@ static void t3_free_qset(struct adapter *adapter, struct sge_qset *q)
 				  q->rspq.desc, q->rspq.phys_addr);
 	}
 
+	if (q->netdev)
+		q->netdev->atalk_ptr = NULL;
+
 	memset(q, 0, sizeof(*q));
 }
 
@@ -1116,7 +1119,7 @@ int t3_eth_xmit(struct sk_buff *skb, struct net_device *dev)
 	unsigned int ndesc, pidx, credits, gen, compl;
 	const struct port_info *pi = netdev_priv(dev);
 	struct adapter *adap = pi->adapter;
-	struct sge_qset *qs = pi->qs;
+	struct sge_qset *qs = dev2qset(dev);
 	struct sge_txq *q = &qs->txq[TXQ_ETH];
 
 	/*
@@ -1365,12 +1368,13 @@ static void restart_ctrlq(unsigned long data)
 	struct sk_buff *skb;
 	struct sge_qset *qs = (struct sge_qset *)data;
 	struct sge_txq *q = &qs->txq[TXQ_CTRL];
+	const struct port_info *pi = netdev_priv(qs->netdev);
+	struct adapter *adap = pi->adapter;
 
 	spin_lock(&q->lock);
       again:reclaim_completed_tx_imm(q);
 
-	while (q->in_use < q->size &&
-	       (skb = __skb_dequeue(&q->sendq)) != NULL) {
+	while (q->in_use < q->size && (skb = __skb_dequeue(&q->sendq)) != NULL) {
 
 		write_imm(&q->desc[q->pidx], skb, skb->len, q->gen);
 
@@ -1393,7 +1397,7 @@ static void restart_ctrlq(unsigned long data)
 
 	spin_unlock(&q->lock);
 	wmb();
-	t3_write_reg(qs->adap, A_SG_KDOORBELL,
+	t3_write_reg(adap, A_SG_KDOORBELL,
 		     F_SELEGRCNTX | V_EGRCNTX(q->cntxt_id));
 }
 
@@ -1683,7 +1687,8 @@ static inline void offload_enqueue(struct sge_rspq *q, struct sk_buff *skb)
 	else {
 		struct sge_qset *qs = rspq_to_qset(q);
 
-		napi_schedule(&qs->napi);
+		if (__netif_rx_schedule_prep(qs->netdev))
+			__netif_rx_schedule(qs->netdev);
 		q->rx_head = skb;
 	}
 	q->rx_tail = skb;
@@ -1719,30 +1724,34 @@ static inline void deliver_partial_bundle(struct t3cdev *tdev,
  *	receive handler.  Batches need to be of modest size as we do prefetches
  *	on the packets in each.
  */
-static int ofld_poll(struct napi_struct *napi, int budget)
+static int ofld_poll(struct net_device *dev, int *budget)
 {
-	struct sge_qset *qs = container_of(napi, struct sge_qset, napi);
+	const struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+	struct sge_qset *qs = dev2qset(dev);
 	struct sge_rspq *q = &qs->rspq;
-	struct adapter *adapter = qs->adap;
-	int work_done = 0;
+	int work_done, limit = min(*budget, dev->quota), avail = limit;
 
-	while (work_done < budget) {
+	while (avail) {
 		struct sk_buff *head, *tail, *skbs[RX_BUNDLE_SIZE];
 		int ngathered;
 
 		spin_lock_irq(&q->lock);
 		head = q->rx_head;
 		if (!head) {
-			napi_complete(napi);
+			work_done = limit - avail;
+			*budget -= work_done;
+			dev->quota -= work_done;
+			__netif_rx_complete(dev);
 			spin_unlock_irq(&q->lock);
-			return work_done;
+			return 0;
 		}
 
 		tail = q->rx_tail;
 		q->rx_head = q->rx_tail = NULL;
 		spin_unlock_irq(&q->lock);
 
-		for (ngathered = 0; work_done < budget && head; work_done++) {
+		for (ngathered = 0; avail && head; avail--) {
 			prefetch(head->data);
 			skbs[ngathered] = head;
 			head = head->next;
@@ -1764,8 +1773,10 @@ static int ofld_poll(struct napi_struct *napi, int budget)
 		}
 		deliver_partial_bundle(&adapter->tdev, q, skbs, ngathered);
 	}
-
-	return work_done;
+	work_done = limit - avail;
+	*budget -= work_done;
+	dev->quota -= work_done;
+	return 1;
 }
 
 /**
@@ -2325,47 +2336,50 @@ static inline int is_pure_response(const struct rsp_desc *r)
 
 /**
  *	napi_rx_handler - the NAPI handler for Rx processing
- *	@napi: the napi instance
+ *	@dev: the net device
  *	@budget: how many packets we can process in this round
  *
  *	Handler for new data events when using NAPI.
  */
-static int napi_rx_handler(struct napi_struct *napi, int budget)
+static int napi_rx_handler(struct net_device *dev, int *budget)
 {
-	struct sge_qset *qs = container_of(napi, struct sge_qset, napi);
-	struct adapter *adap = qs->adap;
-	int work_done = process_responses(adap, qs, budget);
+	const struct port_info *pi = netdev_priv(dev);
+	struct adapter *adap = pi->adapter;
+	struct sge_qset *qs = dev2qset(dev);
+	int effective_budget = min(*budget, dev->quota);
 
-	if (likely(work_done < budget)) {
-		napi_complete(napi);
+	int work_done = process_responses(adap, qs, effective_budget);
+	*budget -= work_done;
+	dev->quota -= work_done;
 
-		/*
-		 * Because we don't atomically flush the following
-		 * write it is possible that in very rare cases it can
-		 * reach the device in a way that races with a new
-		 * response being written plus an error interrupt
-		 * causing the NAPI interrupt handler below to return
-		 * unhandled status to the OS.  To protect against
-		 * this would require flushing the write and doing
-		 * both the write and the flush with interrupts off.
-		 * Way too expensive and unjustifiable given the
-		 * rarity of the race.
-		 *
-		 * The race cannot happen at all with MSI-X.
-		 */
-		t3_write_reg(adap, A_SG_GTS, V_RSPQ(qs->rspq.cntxt_id) |
-			     V_NEWTIMER(qs->rspq.next_holdoff) |
-			     V_NEWINDEX(qs->rspq.cidx));
-	}
-	return work_done;
+	if (work_done >= effective_budget)
+		return 1;
+
+	netif_rx_complete(dev);
+
+	/*
+	 * Because we don't atomically flush the following write it is
+	 * possible that in very rare cases it can reach the device in a way
+	 * that races with a new response being written plus an error interrupt
+	 * causing the NAPI interrupt handler below to return unhandled status
+	 * to the OS.  To protect against this would require flushing the write
+	 * and doing both the write and the flush with interrupts off.  Way too
+	 * expensive and unjustifiable given the rarity of the race.
+	 *
+	 * The race cannot happen at all with MSI-X.
+	 */
+	t3_write_reg(adap, A_SG_GTS, V_RSPQ(qs->rspq.cntxt_id) |
+		     V_NEWTIMER(qs->rspq.next_holdoff) |
+		     V_NEWINDEX(qs->rspq.cidx));
+	return 0;
 }
 
 /*
  * Returns true if the device is already scheduled for polling.
  */
-static inline int napi_is_scheduled(struct napi_struct *napi)
+static inline int napi_is_scheduled(struct net_device *dev)
 {
-	return test_bit(NAPI_STATE_SCHED, &napi->state);
+	return test_bit(__LINK_STATE_RX_SCHED, &dev->state);
 }
 
 /**
@@ -2448,7 +2462,8 @@ static inline int handle_responses(struct adapter *adap, struct sge_rspq *q)
 			     V_NEWTIMER(q->holdoff_tmr) | V_NEWINDEX(q->cidx));
 		return 0;
 	}
-	napi_schedule(&qs->napi);
+	if (likely(__netif_rx_schedule_prep(qs->netdev)))
+		__netif_rx_schedule(qs->netdev);
 	return 1;
 }
 
@@ -2459,7 +2474,8 @@ static inline int handle_responses(struct adapter *adap, struct sge_rspq *q)
 irqreturn_t t3_sge_intr_msix(int irq, void *cookie)
 {
 	struct sge_qset *qs = cookie;
-	struct adapter *adap = qs->adap;
+	const struct port_info *pi = netdev_priv(qs->netdev);
+	struct adapter *adap = pi->adapter;
 	struct sge_rspq *q = &qs->rspq;
 
 	spin_lock(&q->lock);
@@ -2478,11 +2494,13 @@ irqreturn_t t3_sge_intr_msix(int irq, void *cookie)
 static irqreturn_t t3_sge_intr_msix_napi(int irq, void *cookie)
 {
 	struct sge_qset *qs = cookie;
+	const struct port_info *pi = netdev_priv(qs->netdev);
+	struct adapter *adap = pi->adapter;
 	struct sge_rspq *q = &qs->rspq;
 
 	spin_lock(&q->lock);
 
-	if (handle_responses(qs->adap, q) < 0)
+	if (handle_responses(adap, q) < 0)
 		q->unhandled_irqs++;
 	spin_unlock(&q->lock);
 	return IRQ_HANDLED;
@@ -2525,13 +2543,11 @@ static irqreturn_t t3_intr_msi(int irq, void *cookie)
 	return IRQ_HANDLED;
 }
 
-static int rspq_check_napi(struct sge_qset *qs)
+static int rspq_check_napi(struct net_device *dev, struct sge_rspq *q)
 {
-	struct sge_rspq *q = &qs->rspq;
-
-	if (!napi_is_scheduled(&qs->napi) &&
-	    is_new_response(&q->desc[q->cidx], q)) {
-		napi_schedule(&qs->napi);
+	if (!napi_is_scheduled(dev) && is_new_response(&q->desc[q->cidx], q)) {
+		if (likely(__netif_rx_schedule_prep(dev)))
+			__netif_rx_schedule(dev);
 		return 1;
 	}
 	return 0;
@@ -2552,9 +2568,10 @@ static irqreturn_t t3_intr_msi_napi(int irq, void *cookie)
 
 	spin_lock(&q->lock);
 
-	new_packets = rspq_check_napi(&adap->sge.qs[0]);
+	new_packets = rspq_check_napi(adap->sge.qs[0].netdev, q);
 	if (adap->params.nports == 2)
-		new_packets += rspq_check_napi(&adap->sge.qs[1]);
+		new_packets += rspq_check_napi(adap->sge.qs[1].netdev,
+					       &adap->sge.qs[1].rspq);
 	if (!new_packets && t3_slow_intr_handler(adap) == 0)
 		q->unhandled_irqs++;
 
@@ -2657,9 +2674,9 @@ static irqreturn_t t3b_intr(int irq, void *cookie)
 static irqreturn_t t3b_intr_napi(int irq, void *cookie)
 {
 	u32 map;
+	struct net_device *dev;
 	struct adapter *adap = cookie;
-	struct sge_qset *qs0 = &adap->sge.qs[0];
-	struct sge_rspq *q0 = &qs0->rspq;
+	struct sge_rspq *q0 = &adap->sge.qs[0].rspq;
 
 	t3_write_reg(adap, A_PL_CLI, 0);
 	map = t3_read_reg(adap, A_SG_DATA_INTR);
@@ -2672,11 +2689,18 @@ static irqreturn_t t3b_intr_napi(int irq, void *cookie)
 	if (unlikely(map & F_ERRINTR))
 		t3_slow_intr_handler(adap);
 
-	if (likely(map & 1))
-		napi_schedule(&qs0->napi);
+	if (likely(map & 1)) {
+		dev = adap->sge.qs[0].netdev;
 
-	if (map & 2)
-		napi_schedule(&adap->sge.qs[1].napi);
+		if (likely(__netif_rx_schedule_prep(dev)))
+			__netif_rx_schedule(dev);
+	}
+	if (map & 2) {
+		dev = adap->sge.qs[1].netdev;
+
+		if (likely(__netif_rx_schedule_prep(dev)))
+			__netif_rx_schedule(dev);
+	}
 
 	spin_unlock(&q0->lock);
 	return IRQ_HANDLED;
@@ -2775,7 +2799,8 @@ static void sge_timer_cb(unsigned long data)
 {
 	spinlock_t *lock;
 	struct sge_qset *qs = (struct sge_qset *)data;
-	struct adapter *adap = qs->adap;
+	const struct port_info *pi = netdev_priv(qs->netdev);
+	struct adapter *adap = pi->adapter;
 
 	if (spin_trylock(&qs->txq[TXQ_ETH].lock)) {
 		reclaim_completed_tx(adap, &qs->txq[TXQ_ETH]);
@@ -2786,9 +2811,9 @@ static void sge_timer_cb(unsigned long data)
 		spin_unlock(&qs->txq[TXQ_OFLD].lock);
 	}
 	lock = (adap->flags & USING_MSIX) ? &qs->rspq.lock :
-					    &adap->sge.qs[0].rspq.lock;
+	    &adap->sge.qs[0].rspq.lock;
 	if (spin_trylock_irq(lock)) {
-		if (!napi_is_scheduled(&qs->napi)) {
+		if (!napi_is_scheduled(qs->netdev)) {
 			u32 status = t3_read_reg(adap, A_SG_RSPQ_FL_STATUS);
 
 			if (qs->fl[0].credits < qs->fl[0].size)
@@ -2822,9 +2847,12 @@ static void sge_timer_cb(unsigned long data)
  */
 void t3_update_qset_coalesce(struct sge_qset *qs, const struct qset_params *p)
 {
+	if (!qs->netdev)
+		return;
+
 	qs->rspq.holdoff_tmr = max(p->coalesce_usecs * 10, 1U);/* can't be 0 */
 	qs->rspq.polling = p->polling;
-	qs->napi.poll = p->polling ? napi_rx_handler : ofld_poll;
+	qs->netdev->poll = p->polling ? napi_rx_handler : ofld_poll;
 }
 
 /**
@@ -2844,7 +2872,7 @@ void t3_update_qset_coalesce(struct sge_qset *qs, const struct qset_params *p)
  */
 int t3_sge_alloc_qset(struct adapter *adapter, unsigned int id, int nports,
 		      int irq_vec_idx, const struct qset_params *p,
-		      int ntxq, struct net_device *dev)
+		      int ntxq, struct net_device *netdev)
 {
 	int i, avail, ret = -ENOMEM;
 	struct sge_qset *q = &adapter->sge.qs[id];
@@ -2978,11 +3006,17 @@ int t3_sge_alloc_qset(struct adapter *adapter, unsigned int id, int nports,
 	}
 
 	spin_unlock_irq(&adapter->sge.reg_lock);
-
-	q->adap = adapter;
-	q->netdev = dev;
+	q->netdev = netdev;
 	t3_update_qset_coalesce(q, p);
 
+ 	/*
+ 	 * We use atalk_ptr as a backpointer to a qset.  In case a device is
+ 	 * associated with multiple queue sets only the first one sets
+ 	 * atalk_ptr.
+ 	 */
+ 	if (netdev->atalk_ptr == NULL)
+ 		netdev->atalk_ptr = q;
+
 	init_lro_mgr(q, lro_mgr);
 
 	avail = refill_fl(adapter, &q->fl[0], q->fl[0].size,
