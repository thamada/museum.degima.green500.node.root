Place SQ wrid's first in wrid buffer, to eliminate an add
operation in the send datapath.

This keeps binary size constant, moving code from post send to
post receive: post send is a latency-sensitive operation, while
post receive is done beforehand, so it's not.  Additionally, a
generic ULP mixing send and RDMA does more post sends than post
receives (RDMA does not have a matching post receive).

Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
Signed-off-by: Michael S. Tsirkin <mst@mellanox.co.il>

---

While unlikely to give a large gain, this makes sense to me.
Please consider for 2.6.20.

---
 drivers/infiniband/hw/mthca/mthca_cq.c |    5 ++---
 drivers/infiniband/hw/mthca/mthca_qp.c |    8 ++++----
 2 files changed, 6 insertions(+), 7 deletions(-)

Index: ofed_kernel/drivers/infiniband/hw/mthca/mthca_cq.c
===================================================================
--- ofed_kernel.orig/drivers/infiniband/hw/mthca/mthca_cq.c
+++ ofed_kernel/drivers/infiniband/hw/mthca/mthca_cq.c
@@ -538,8 +538,7 @@ static inline int mthca_poll_one(struct 
 		wq = &(*cur_qp)->sq;
 		wqe_index = ((be32_to_cpu(cqe->wqe) - (*cur_qp)->send_wqe_offset)
 			     >> wq->wqe_shift);
-		entry->wr_id = (*cur_qp)->wrid[wqe_index +
-					       (*cur_qp)->rq.max];
+		entry->wr_id = (*cur_qp)->wrid[wqe_index];
 	} else if ((*cur_qp)->ibqp.srq) {
 		struct mthca_srq *srq = to_msrq((*cur_qp)->ibqp.srq);
 		u32 wqe = be32_to_cpu(cqe->wqe);
@@ -559,7 +558,7 @@ static inline int mthca_poll_one(struct 
 		 */
 		if (unlikely(wqe_index < 0))
 			wqe_index = wq->max - 1;
-		entry->wr_id = (*cur_qp)->wrid[wqe_index];
+		entry->wr_id = (*cur_qp)->wrid[wqe_index + (*cur_qp)->sq.max];
 	}
 
 	if (wq) {
Index: ofed_kernel/drivers/infiniband/hw/mthca/mthca_qp.c
===================================================================
--- ofed_kernel.orig/drivers/infiniband/hw/mthca/mthca_qp.c
+++ ofed_kernel/drivers/infiniband/hw/mthca/mthca_qp.c
@@ -1753,7 +1753,7 @@ int mthca_tavor_post_send(struct ib_qp *
 			size += sizeof (struct mthca_data_seg) / 16;
 		}
 
-		qp->wrid[ind + qp->rq.max] = wr->wr_id;
+		qp->wrid[ind] = wr->wr_id;
 
 		if (wr->opcode >= ARRAY_SIZE(mthca_opcode)) {
 			mthca_err(dev, "opcode invalid\n");
@@ -1869,7 +1869,7 @@ int mthca_tavor_post_receive(struct ib_q
 			size += sizeof (struct mthca_data_seg) / 16;
 		}
 
-		qp->wrid[ind] = wr->wr_id;
+		qp->wrid[ind + qp->sq.max] = wr->wr_id;
 
 		((struct mthca_next_seg *) prev_wqe)->ee_nds =
 			cpu_to_be32(MTHCA_NEXT_DBD | size);
@@ -2094,7 +2094,7 @@ int mthca_arbel_post_send(struct ib_qp *
 			size += sizeof (struct mthca_data_seg) / 16;
 		}
 
-		qp->wrid[ind + qp->rq.max] = wr->wr_id;
+		qp->wrid[ind] = wr->wr_id;
 
 		if (wr->opcode >= ARRAY_SIZE(mthca_opcode)) {
 			mthca_err(dev, "opcode invalid\n");
@@ -2207,7 +2207,7 @@ int mthca_arbel_post_receive(struct ib_q
 		if (i < qp->rq.max_gs)
 			mthca_set_data_seg_inval(wqe);
 
-		qp->wrid[ind] = wr->wr_id;
+		qp->wrid[ind + qp->sq.max] = wr->wr_id;
 
 		++ind;
 		if (unlikely(ind >= qp->rq.max))
