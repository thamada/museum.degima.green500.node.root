diff -up a/drivers/infiniband/hw/qib/qib.h b/drivers/infiniband/hw/qib/qib.h
--- a/drivers/infiniband/hw/qib/qib.h	2010-02-18 10:42:19.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib.h	2010-02-18 10:34:37.000000000 -0800
@@ -1,7 +1,8 @@
 #ifndef _QIB_KERNEL_H
 #define _QIB_KERNEL_H
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2003, 2004, 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -477,6 +478,19 @@ struct qib_sdma_state {
 	enum qib_sdma_events last_event;
 };
 
+struct xmit_wait {
+	struct timer_list timer;
+	u64 counter;
+	u8 flags;
+	struct cache {
+		u64 psxmitdata;
+		u64 psrcvdata;
+		u64 psxmitpkts;
+		u64 psrcvpkts;
+		u64 psxmitwait;
+	} counter_cache;
+};
+
 /*
  * The structure below encapsulates data relevant to a physical IB Port.
  * Current chips support only one such port, but the separation
@@ -604,6 +618,7 @@ struct qib_pportdata {
 	atomic_t led_override_timer_active;
 	/* Used to flash LEDs in override mode */
 	struct timer_list led_override_timer;
+	struct xmit_wait cong_stats;
 };
 
 /* Observers. Not to be taken lightly, possibly not to ship. */
@@ -745,7 +760,6 @@ struct qib_devdata {
 	void (*f_sdma_hw_clean_up)(struct qib_pportdata *);
 	void (*f_sdma_hw_start_up)(struct qib_pportdata *);
 	void (*f_sdma_init_early)(struct qib_pportdata *);
-	int (*f_sdma_flush_sends)(struct qib_pportdata *);
 	void (*f_set_cntr_sample)(struct qib_pportdata *, u32, u32);
 	void (*f_update_usrhead)(struct qib_ctxtdata *, u64, u32, u32);
 	u32 (*f_hdrqempty)(struct qib_ctxtdata *);
@@ -989,6 +1003,10 @@ struct qib_devdata {
 	struct qib_diag_client *diag_client;
 	spinlock_t qib_diag_trans_lock; /* protect diag observer ops */
 	struct diag_observer_list_elt *diag_observer_list;
+
+	u8 psxmitwait_supported;
+	/* cycle length of PS* counters in HW (in picoseconds) */
+	u16 psxmitwait_check_rate;
 };
 
 /* hol_state values */
@@ -1016,11 +1034,14 @@ struct qib_filedata {
 	unsigned subctxt;
 	unsigned tidcursor;
 	struct qib_user_sdma_queue *pq;
+	int rec_cpu_num; /* for cpu affinity; -1 if none */
 };
 
 extern struct list_head qib_dev_list;
 extern spinlock_t qib_devs_lock;
 extern struct qib_devdata *qib_lookup(int unit);
+extern u32 qib_cpulist_count;
+extern unsigned long *qib_cpulist;
 
 extern unsigned qib_wc_pat;
 int qib_init(struct qib_devdata *, int);
diff -up a/drivers/infiniband/hw/qib/qib_common.h b/drivers/infiniband/hw/qib/qib_common.h
--- a/drivers/infiniband/hw/qib/qib_common.h	2010-02-18 10:42:19.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_common.h	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2003, 2004, 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -278,7 +279,7 @@ struct qib_base_info {
  * may not be implemented; the user code must deal with this if it
  * cares, or it must abort after initialization reports the difference.
  */
-#define QIB_USER_SWMINOR 9
+#define QIB_USER_SWMINOR 10
 
 #define QIB_USER_SWVERSION ((QIB_USER_SWMAJOR << 16) | QIB_USER_SWMINOR)
 
@@ -354,7 +355,9 @@ struct qib_user_info {
 #define QIB_CMD_SDMA_COMPLETE   32      /* sdma completion counter request */
 /* 33 available, was a testing feature  */
 #define QIB_CMD_DISARM_BUFS     34      /* disarm send buffers w/ errors */
-#define QIB_CMD_ACK_EVENT     35	/* ack & clear bits */
+#define QIB_CMD_ACK_EVENT       35      /* ack & clear bits */
+#define QIB_CMD_CPUS_LIST       36      /* list of cpus allocated, for pinned
+					 * processes: qib_cpus_list */
 
 /*
  * QIB_CMD_ACK_EVENT obsoletes QIB_CMD_DISARM_BUFS, but we keep it for
@@ -392,7 +395,7 @@ struct qib_ctxt_info {
 	__u16 subctxt;          /* subctxt on unit assigned to caller */
 	__u16 num_ctxts;        /* number of ctxts available on unit */
 	__u16 num_subctxts;     /* number of subctxts opened on ctxt */
-	__u16 __pad;            /* pad size out to qword multple */
+	__u16 rec_cpu;          /* cpu # for affinity (ffff if none) */
 };
 
 struct qib_tid_info {
diff -up a/drivers/infiniband/hw/qib/qib_cq.c b/drivers/infiniband/hw/qib/qib_cq.c
--- a/drivers/infiniband/hw/qib/qib_cq.c	2010-02-18 10:42:19.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_cq.c	2010-02-18 10:34:16.000000000 -0800
@@ -175,7 +178,15 @@ static void send_complete(unsigned long
 	for (;;) {
 		u8 triggered = cq->triggered;
 
+		/*
+		 * IPoIB connected mode assumes the callback is from a
+		 * soft IRQ. We simulate this by blocking "bottom halves".
+		 * See the implementation for ipoib_cm_handle_tx_wc(),
+		 * netif_tx_lock_bh() and netif_tx_lock().
+		 */
+		local_bh_disable();
 		cq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context);
+		local_bh_enable();
 
 		if (cq->triggered == triggered)
 			return;
diff -up a/drivers/infiniband/hw/qib/qib_file_ops.c b/drivers/infiniband/hw/qib/qib_file_ops.c
--- a/drivers/infiniband/hw/qib/qib_file_ops.c	2010-02-18 10:42:19.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_file_ops.c	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2003, 2004, 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -1472,68 +1473,19 @@ bail:
 	return ret;
 }
 
-static int find_best_unit(struct file *fp, const struct qib_user_info *uinfo)
+static int get_a_ctxt(struct file *fp, const struct qib_user_info *uinfo)
 {
 	struct qib_pportdata *ppd;
-	int ret = 0, i, prefunit = -1, devmax;
+	int ret = 0, devmax;
 	int npresent, nup;
 	int ndev;
 	u32 port = uinfo->spu_port, ctxt;
 
 	devmax = qib_count_units(&npresent, &nup);
 
-	/*
-	 * This code is present to allow a knowledgeable person to
-	 * specify the layout of processes to processors before opening
-	 * this driver, and then we'll assign the process to the "closest"
-	 * QLogic_IB chip to that processor (we assume reasonable connectivity,
-	 * for now).  This code assumes that if affinity has been set
-	 * before this point, that at most one cpu is set; for now this
-	 * is reasonable.  I check for both cpus_empty() and cpus_full(),
-	 * in case some kernel variant sets none of the bits when no
-	 * affinity is set.
-	 * Some day we'll have to fix it up further to handle
-	 * a cpu subset.  Eventually this needs real topology
-	 * information.  There are issues with multi core numbering
-	 * as well.
-	 */
-	if (!cpus_empty(current->cpus_allowed) &&
-	    !cpus_full(current->cpus_allowed)) {
-		int ncpus = num_online_cpus(), curcpu = -1, nset = 0;
-
-		for (i = 0; i < ncpus; i++)
-			if (cpu_isset(i, current->cpus_allowed)) {
-				if (qib_debug & __QIB_VERBDBG)
-					qib_cdbg(PROC, "%s[%u] cpuaff %d/%d"
-						 " set\n", current->comm,
-						 current->pid, i, ncpus);
-				curcpu = i;
-				nset++;
-			}
-		if (curcpu != -1 && nset != ncpus) {
-			if (npresent) {
-				prefunit = curcpu / (ncpus / npresent);
-				qib_cdbg(PROC, "%s[%u] %d chips, %d cpus, "
-					 "%d cpus/chip, select unit %d\n",
-					 current->comm, current->pid,
-					 npresent, ncpus, ncpus / npresent,
-					 prefunit);
-			}
-		}
-	}
-
-	/*
-	 * user contexts start at 1, kernel port is 0
-	 * For now, we do round-robin access across all chips
-	 */
-	if (prefunit != -1) {
-		devmax = prefunit + 1;
-		qib_cdbg(PROC, "user wants unit %u\n", prefunit);
-	}
 	if (port)
 		qib_cdbg(PROC, "user wants IB port %u\n", port);
-recheck:
-	for (ndev = prefunit != -1 ? prefunit : 0; ndev < devmax; ndev++) {
+	for (ndev = 0; ndev < devmax; ndev++) {
 		struct qib_devdata *dd = qib_lookup(ndev);
 
 		/* device portion of usable() */
@@ -1545,8 +1497,8 @@ recheck:
 			if (port) {
 				if (port > dd->num_pports) {
 					qib_cdbg(PROC,
-						 "skip invalid unit %u:%u\n",
-						 ndev, port);
+						 "skip invalid unit:port "
+						 "%u:%u\n", ndev, port);
 					continue;
 				}
 				ppd = dd->pport + port - 1;
@@ -1581,19 +1533,6 @@ recheck:
 			qib_dbg("No contexts available (none initialized "
 				"and ready)\n");
 		} else {
-			if (prefunit != -1) {
-				/*
-				 * if had prefunit, but it wasn't available,
-				 * restart, and try all units
-				 */
-				qib_cdbg(PROC, "%s[%u] no contexts on "
-					 "prefunit %d, clear and re-check\n",
-					 current->comm, current->pid,
-					 prefunit);
-				devmax = qib_count_units(NULL, NULL);
-				prefunit = -1;
-				goto recheck;
-			}
 			ret = -EBUSY;
 			qib_dbg("All contexts busy\n");
 		}
@@ -1659,6 +1598,8 @@ static int qib_open(struct inode *in, st
 {
 	/* The real work is performed later in qib_assign_ctxt() */
 	fp->private_data = kzalloc(sizeof(struct qib_filedata), GFP_KERNEL);
+	if (fp->private_data) /* no cpu affinity by default */
+		((struct qib_filedata *)fp->private_data)->rec_cpu_num = -1;
 	return fp->private_data ? 0 : -ENOMEM;
 }
 
@@ -1711,13 +1652,13 @@ static int qib_assign_ctxt(struct file *
 	if (i_minor)
 		ret = find_free_ctxt(i_minor - 1, fp, uinfo);
 	else
-		ret = find_best_unit(fp, uinfo);
+		ret = get_a_ctxt(fp, uinfo);
 
 done_chk_sdma:
 	if (!ret) {
 		struct qib_filedata *fd = fp->private_data;
-		const struct qib_ctxtdata *rcd = fd->rcd;
-		const struct qib_devdata *dd = rcd->dd;
+		struct qib_ctxtdata *rcd = fd->rcd;
+		struct qib_devdata *dd = rcd->dd;
 
 		if (dd->flags & QIB_HAS_SEND_DMA) {
 			fd->pq = qib_user_sdma_queue_create(&dd->pcidev->dev,
@@ -1727,6 +1668,44 @@ done_chk_sdma:
 			if (!fd->pq)
 				ret = -ENOMEM;
 		}
+
+		/*
+		 * If process has NOT already set it's affinity, select and
+		 * reserve a processor for it, as a rendevous for all
+		 * users of the driver.  If they don't actually later
+		 * set affinity to this cpu, or set it to some other cpu,
+		 * it just means that sooner or later we don't recommend
+		 * a cpu, and let the scheduler do it's best.
+		 */
+		if (!ret && cpus_weight(current->cpus_allowed) >=
+		   qib_cpulist_count) {
+			int cpu;
+			cpu = find_first_zero_bit(qib_cpulist,
+						  qib_cpulist_count);
+			if (cpu == qib_cpulist_count)
+				qib_cdbg(PROC, "no cpus avail for affinity "
+					 "PID %u\n", current->pid);
+			else {
+				__set_bit(cpu, qib_cpulist);
+				fd->rec_cpu_num = cpu;
+				qib_cdbg(PROC, "allocate cpu %d PID %u unit%u"
+					 " ctxt%u:%u\n",
+					 fd->rec_cpu_num,
+					 current->pid, rcd->dd->unit,
+					 rcd->ctxt, subctxt_fp(fp));
+			}
+		} else if (cpus_weight(current->cpus_allowed) == 1 &&
+			test_bit(first_cpu(current->cpus_allowed),
+				   qib_cpulist))
+			qib_devinfo(dd->pcidev, "%s PID %u affinity "
+				    "set to cpu %d; already allocated\n",
+				    current->comm, current->pid,
+				    first_cpu(current->cpus_allowed));
+		else if (!ret)
+			qib_cdbg(PROC, "affinity set for %d cpus of %d, not "
+				 "allocating cpu\n",
+				 cpus_weight(current->cpus_allowed),
+				 qib_cpulist_count);
 	}
 
 	mutex_unlock(&qib_mutex);
@@ -1892,8 +1871,6 @@ static int qib_close(struct inode *in, s
 	unsigned ctxt;
 	pid_t pid;
 
-	qib_cdbg(VERBOSE, "close on dev %lx, private data %p\n",
-		 (long)in->i_rdev, fp->private_data);
 
 	mutex_lock(&qib_mutex);
 
@@ -1901,9 +1878,15 @@ static int qib_close(struct inode *in, s
 	fp->private_data = NULL;
 	rcd = fd->rcd;
 	if (!rcd) {
+		qib_cdbg(PROC, "close on dev %lx, private data %p, PID %u "
+			 "rcd NULL\n", (long)in->i_rdev, fp->private_data,
+			 current->pid);
 		mutex_unlock(&qib_mutex);
 		goto bail;
 	}
+	qib_cdbg(VERBOSE, "close dev %lx, private %p PID %u %u:%u:%u\n",
+		 (long)in->i_rdev, fp->private_data, current->pid,
+		 rcd->dd->unit, rcd->ctxt, rcd->subctxt_id);
 
 	dd = rcd->dd;
 
@@ -1916,6 +1899,13 @@ static int qib_close(struct inode *in, s
 		qib_user_sdma_queue_destroy(fd->pq);
 	}
 
+	if (fd->rec_cpu_num != -1) {
+		qib_cdbg(PROC, "mark cpu %d from unit%u ctxt%u:%u available\n",
+			fd->rec_cpu_num, dd->unit, rcd->ctxt,
+				 rcd->subctxt_id);
+		__clear_bit(fd->rec_cpu_num, qib_cpulist);
+	}
+
 	if (--rcd->cnt) {
 		/*
 		 * XXX If the master closes the context before the slave(s),
@@ -1970,6 +1960,7 @@ static int qib_close(struct inode *in, s
 		if (dd->pageshadow)
 			unlock_expected_tids(rcd);
 		qib_stats.sps_ctxts--;
+
 		qib_cdbg(PROC, "%s[%u] closed ctxt %u:%u\n",
 			 rcd->comm, pid, dd->unit, ctxt);
 	}
@@ -1982,21 +1973,25 @@ bail:
 	return ret;
 }
 
-static int qib_ctxt_info(struct qib_ctxtdata *rcd, u16 subctxt,
-			 struct qib_ctxt_info __user *uinfo)
+static int qib_ctxt_info(struct file *fp, struct qib_ctxt_info __user *uinfo)
 {
 	struct qib_ctxt_info info;
 	int ret;
 	size_t sz;
+	struct qib_ctxtdata *rcd = ctxt_fp(fp);
+	struct qib_filedata *fd;
+
+	fd = (struct qib_filedata *) fp->private_data;
 
 	info.num_active = qib_count_active_units();
 	info.unit = rcd->dd->unit;
 	info.port = rcd->ppd->port;
 	info.ctxt = rcd->ctxt;
-	info.subctxt = subctxt;
+	info.subctxt =  subctxt_fp(fp);
 	/* Number of user ctxts available for this device. */
 	info.num_ctxts = rcd->dd->cfgctxts - rcd->dd->first_user_ctxt;
 	info.num_subctxts = rcd->subctxt_cnt;
+	info.rec_cpu = fd->rec_cpu_num;
 	sz = sizeof(info);
 
 	if (copy_to_user(uinfo, &info, sz)) {
@@ -2287,8 +2282,7 @@ static ssize_t qib_write(struct file *fp
 		break;
 
 	case QIB_CMD_CTXT_INFO:
-		ret = qib_ctxt_info(rcd, subctxt_fp(fp),
-				    (struct qib_ctxt_info __user *)
+		ret = qib_ctxt_info(fp, (struct qib_ctxt_info __user *)
 				    (unsigned long) cmd.cmd.ctxt_info);
 		break;
 
@@ -2439,7 +2433,7 @@ int __init qib_dev_init(void)
 		goto done;
 	}
 
-	qib_class = class_create(THIS_MODULE, QIB_DRV_NAME);
+	qib_class = class_create(THIS_MODULE, "ipath");
 	if (IS_ERR(qib_class)) {
 		ret = PTR_ERR(qib_class);
 		printk(KERN_ERR QIB_DRV_NAME ": Could not create "
diff -up a/drivers/infiniband/hw/qib/qib_iba6120.c b/drivers/infiniband/hw/qib/qib_iba6120.c
--- a/drivers/infiniband/hw/qib/qib_iba6120.c	2010-02-18 10:42:19.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_iba6120.c	2010-02-18 10:34:37.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2003, 2004, 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -474,6 +475,8 @@ static inline u32 read_6120_creg32(const
 #define QLOGIC_IB_R_TAILUPD_SHIFT 31
 #define IBA6120_R_PKEY_DIS_SHIFT 30
 
+#define PBC_6120_VL15_SEND_CTRL (1ULL << 31) /* pbc; VL15; link_buf only */
+
 #define IBCBUSFRSPCPARITYERR HWE_MASK(IBCBusFromSPCParityErr)
 #define IBCBUSTOSPCPARITYERR HWE_MASK(IBCBusToSPCParityErr)
 
@@ -3445,19 +3448,67 @@ bail:
 	return ret;
 }
 
+/*
+ * For this chip, we want to use the same buffer every time
+ * when we are trying to bring the link up (they are always VL15
+ * packets).  At that link state the packet should always go out immediately
+ * (or at least be discarded at the tx interface if the link is down).
+ * If it doesn't, and the buffer isn't available, that means some other
+ * sender has gotten ahead of us, and is preventing our packet from going
+ * out.  In that case, we flush all packets, and try again.  If that still
+ * fails, we fail the request, and hope things work the next time around.
+ *
+ * We don't need very complicated heuristics on whether the packet had
+ * time to go out or not, since even at SDR 1X, it goes out in very short
+ * time periods, covered by the chip reads done here and as part of the
+ * flush.
+ */
+u32 __iomem *get_6120_link_buf(struct qib_pportdata *ppd, u32 *bnum)
+{
+	u32 __iomem *buf;
+	u32 lbuf = ppd->dd->piobcnt2k + ppd->dd->piobcnt4k - 1;
+
+	/*
+	 * always blip to get avail list updated, since it's almost
+	 * always needed, and is fairly cheap.
+	 */
+	sendctrl_6120_mod(ppd->dd->pport, QIB_SENDCTRL_AVAIL_BLIP);
+	qib_read_kreg64(ppd->dd, kr_scratch); /* extra chip flush */
+	buf = qib_getsendbuf_range(ppd->dd, bnum, lbuf, lbuf);
+	if (buf)
+		goto done;
+
+	sendctrl_6120_mod(ppd, QIB_SENDCTRL_DISARM_ALL | QIB_SENDCTRL_FLUSH |
+			  QIB_SENDCTRL_AVAIL_BLIP);
+	ppd->dd->upd_pio_shadow  = 1; /* update our idea of what's busy */
+	qib_read_kreg64(ppd->dd, kr_scratch); /* extra chip flush */
+	buf = qib_getsendbuf_range(ppd->dd, bnum, lbuf, lbuf);
+done:
+	return buf;
+}
+
+
 static u32 __iomem *qib_6120_getsendbuf(struct qib_pportdata *ppd, u64 pbc,
 					u32 *pbufnum)
 {
 	u32 first, last, plen = pbc & QIB_PBC_LENGTH_MASK;
 	struct qib_devdata *dd = ppd->dd;
+	u32 __iomem *buf;
 
-	if ((plen + 1) > dd->piosize2kmax_dwords)
-		first = dd->piobcnt2k;
-	else
-		first = 0;
-	/* last is the same for both, because we use 4k if all 2k busy */
-	last = dd->piobcnt2k + dd->piobcnt4k - 1;
-	return qib_getsendbuf_range(dd, pbufnum, first, last);
+	if (((pbc >> 32) & PBC_6120_VL15_SEND_CTRL) &&
+		!(ppd->lflags & (QIBL_IB_AUTONEG_INPROG | QIBL_LINKACTIVE)))
+		buf = get_6120_link_buf(ppd, pbufnum);
+	else {
+
+		if ((plen + 1) > dd->piosize2kmax_dwords)
+			first = dd->piobcnt2k;
+		else
+			first = 0;
+		/* try 4k if all 2k busy, so same last for both sizes */
+		last = dd->piobcnt2k + dd->piobcnt4k - 1;
+		buf = qib_getsendbuf_range(dd, pbufnum, first, last);
+	}
+	return buf;
 }
 
 static void dump_sdma_6120_state(struct qib_pportdata *ppd)
@@ -3491,10 +3542,14 @@ static void qib_sdma_set_6120_desc_cnt(s
 {
 }
 
+/*
+ * the pbc doesn't need a VL15 indicator, but we need it for link_buf.
+ * The chip ignores the bit if set.
+ */
 static u32 qib_6120_setpbc_control(struct qib_pportdata *ppd, u32 plen,
 				   u8 srate, u8 vl)
 {
-	return 0;
+	return vl == 15 ? PBC_6120_VL15_SEND_CTRL : 0;
 }
 
 static void qib_6120_initvl15_bufs(struct qib_devdata *dd)
diff -up a/drivers/infiniband/hw/qib/qib_iba7220.c b/drivers/infiniband/hw/qib/qib_iba7220.c
--- a/drivers/infiniband/hw/qib/qib_iba7220.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_iba7220.c	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2003, 2004, 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -474,6 +475,9 @@ static inline u32 read_7220_creg32(const
 #define QIB_TWSI_EEPROM_DEV 0xA2 /* All Production 7220 cards. */
 #define QIB_TWSI_TEMP_DEV 0x98
 
+/* HW counter clock is at 4nsec */
+#define QIB_7220_PSXMITWAIT_CHECK_RATE 4000
+
 #define IBA7220_R_INTRAVAIL_SHIFT 17
 #define IBA7220_R_PKEY_DIS_SHIFT 34
 #define IBA7220_R_TAILUPD_SHIFT 35
@@ -492,7 +496,7 @@ static inline u32 read_7220_creg32(const
 #define PBC_7220_VL15_SEND (1ULL << 63) /* pbc; VL15, no credit check */
 #define PBC_7220_VL15_SEND_CTRL (1ULL << 31) /* control version of same */
 
-#define AUTONEG_TRIES 3 /* sequential retries to negotiate DDR */
+#define AUTONEG_TRIES 5 /* sequential retries to negotiate DDR */
 
 /* packet rate matching delay multiplier */
 static u8 rate_to_delay[2][2] = {
@@ -864,28 +868,6 @@ static void qib_7220_sdma_hw_clean_up(st
 	ppd->dd->upd_pio_shadow  = 1; /* update our idea of what's busy */
 }
 
-/*
- * if we are in INIT, and not doing autoneg, and we haven't been able to
- * get a buffer, then flush all the buffers, in case we have something stuck
- * keeping us from getting packets sent.  It may take us a while before
- * we run out of buffers, but there isn't really any other safe way
- * to know that we are stuck.
- */
-static int qib_7220_sdma_flush_sends(struct qib_pportdata *ppd)
-{
-	int ret = 0;
-	if (ppd->dd->upd_pio_shadow && ((QIBL_LINKINIT ==
-		(ppd->lflags & (QIBL_IB_AUTONEG_INPROG | QIBL_LINKINIT))) ||
-		QIBL_LINKARMED == (ppd->lflags & (QIBL_IB_AUTONEG_INPROG |
-						  QIBL_LINKARMED)))) {
-		sendctrl_7220_mod(ppd, QIB_SENDCTRL_DISARM_ALL |
-				  QIB_SENDCTRL_FLUSH |
-				  QIB_SENDCTRL_AVAIL_BLIP);
-		qib_dbg("IB%u:%u flushed sends\n", ppd->dd->unit, ppd->port);
-		ret = 1;
-	}
-	return ret;
-}
 
 static void qib_sdma_7220_setlengen(struct qib_pportdata *ppd)
 {
@@ -3467,6 +3449,59 @@ static void qib_7220_xgxs_reset(struct q
 }
 
 /*
+ * For this chip, we want to use the same buffer every time
+ * when we are trying to bring the link up (they are always VL15
+ * packets).  At that link state the packet should always go out immediately
+ * (or at least be discarded at the tx interface if the link is down).
+ * If it doesn't, and the buffer isn't available, that means some other
+ * sender has gotten ahead of us, and is preventing our packet from going
+ * out.  In that case, we flush all packets, and try again.  If that still
+ * fails, we fail the request, and hope things work the next time around.
+ *
+ * We don't need very complicated heuristics on whether the packet had
+ * time to go out or not, since even at SDR 1X, it goes out in very short
+ * time periods, covered by the chip reads done here and as part of the
+ * flush.
+ */
+u32 __iomem *get_7220_link_buf(struct qib_pportdata *ppd, u32 *bnum)
+{
+	u32 __iomem *buf;
+	u32 lbuf = ppd->dd->cspec->lastbuf_for_pio;
+	int do_cleanup;
+	unsigned long flags;
+
+	/*
+	 * always blip to get avail list updated, since it's almost
+	 * always needed, and is fairly cheap.
+	 */
+	sendctrl_7220_mod(ppd->dd->pport, QIB_SENDCTRL_AVAIL_BLIP);
+	qib_read_kreg64(ppd->dd, kr_scratch); /* extra chip flush */
+	buf = qib_getsendbuf_range(ppd->dd, bnum, lbuf, lbuf);
+	if (buf)
+		goto done;
+
+	spin_lock_irqsave(&ppd->sdma_lock, flags);
+	if (ppd->sdma_state.current_state == qib_sdma_state_s20_idle &&
+	    ppd->sdma_state.current_state != qib_sdma_state_s00_hw_down) {
+		qib_cdbg(INIT, "sdma state %u, so set down, no buf\n",
+			ppd->sdma_state.current_state);
+		qib_sdma_process_event(ppd, qib_sdma_event_e00_go_hw_down);
+		do_cleanup = 0;
+	} else {
+		do_cleanup = 1;
+		qib_7220_sdma_hw_clean_up(ppd);
+	}
+	spin_unlock_irqrestore(&ppd->sdma_lock, flags);
+
+	if (do_cleanup) {
+		qib_read_kreg64(ppd->dd, kr_scratch); /* extra chip flush */
+		buf = qib_getsendbuf_range(ppd->dd, bnum, lbuf, lbuf);
+	}
+done:
+	return buf;
+}
+
+/*
  * This code for non-IBTA-compliant IB speed negotiation is only known to
  * work for the SDR to DDR transition, and only between an HCA and a switch
  * with recent firmware.  It is based on observed heuristics, rather than
@@ -3486,9 +3521,9 @@ static void autoneg_7220_sendpkt(struct
 	i = 0;
 	pbc = 7 + dcnt + 1; /* 7 dword header, dword data, icrc */
 	pbc |= PBC_7220_VL15_SEND;
-	while (!(piobuf = qib_7220_getsendbuf(ppd, pbc, &pnum))) {
-		if (i++ > 15) {
-			qib_dbg("Couldn't get pio buffer for send\n");
+	while (!(piobuf = get_7220_link_buf(ppd, &pnum))) {
+		if (i++ > 5) {
+			qib_dbg("Couldn't get buffer for send\n");
 			return;
 		}
 		udelay(2);
@@ -3744,10 +3779,10 @@ static int qib_7220_ib_updown(struct qib
 		if (!(ppd->lflags & QIBL_IB_AUTONEG_INPROG)) {
 			qib_cdbg(VERBOSE, "Setting RXEQ defaults\n");
 			qib_sd7220_presets(dd);
+			qib_cancel_sends(ppd); /* initial disarm, etc. */
 		}
 		/* this might better in qib_sd7220_presets() */
 		set_7220_relock_poll(dd, ibup);
-		qib_hol_down(ppd);
 	} else {
 		if (qib_compat_ddr_negotiate &&
 		    !(ppd->lflags & (QIBL_IB_AUTONEG_FAILED |
@@ -3822,7 +3857,6 @@ static int qib_7220_ib_updown(struct qib
 			    [(ibcs >> IBA7220_LINKWIDTH_SHIFT) & 1];
 
 			set_7220_relock_poll(dd, ibup);
-			qib_hol_init(ppd);
 		}
 	}
 
@@ -4259,6 +4293,9 @@ static int qib_init_7220_variables(struc
 	/* before full enable, no interrupts, no locking needed */
 	dd->sendctrl |= (updthresh & SYM_RMASK(SendCtrl, AvailUpdThld))
 			     << SYM_LSB(SendCtrl, AvailUpdThld);
+
+	dd->psxmitwait_supported = 1;
+	dd->psxmitwait_check_rate = QIB_7220_PSXMITWAIT_CHECK_RATE;
 bail:
 	return ret;
 }
@@ -4268,14 +4305,21 @@ static u32 __iomem *qib_7220_getsendbuf(
 {
 	u32 first, last, plen = pbc & QIB_PBC_LENGTH_MASK;
 	struct qib_devdata *dd = ppd->dd;
+	u32 __iomem *buf;
 
-	if ((plen + 1) > dd->piosize2kmax_dwords)
-		first = dd->piobcnt2k;
-	else
-		first = 0;
-	/* last is the same for both, because we use 4k if all 2k busy */
-	last = dd->cspec->lastbuf_for_pio;
-	return qib_getsendbuf_range(dd, pbufnum, first, last);
+	if (((pbc >> 32) & PBC_7220_VL15_SEND_CTRL) &&
+		!(ppd->lflags & (QIBL_IB_AUTONEG_INPROG | QIBL_LINKACTIVE)))
+		buf = get_7220_link_buf(ppd, pbufnum);
+	else {
+		if ((plen + 1) > dd->piosize2kmax_dwords)
+			first = dd->piobcnt2k;
+		else
+			first = 0;
+		/* try 4k if all 2k busy, so same last for both sizes */
+		last = dd->cspec->lastbuf_for_pio;
+		buf = qib_getsendbuf_range(dd, pbufnum, first, last);
+	}
+	return buf;
 }
 
 /* these 2 "counters" are really control registers, and are always RW */
@@ -4682,7 +4726,6 @@ struct qib_devdata *qib_init_iba7220_fun
 	dd->f_sdma_update_tail  = qib_sdma_update_7220_tail;
 	dd->f_sdma_hw_clean_up  = qib_7220_sdma_hw_clean_up;
 	dd->f_sdma_hw_start_up  = qib_7220_sdma_hw_start_up;
-	dd->f_sdma_flush_sends  = qib_7220_sdma_flush_sends;
 	dd->f_sdma_init_early   = qib_7220_sdma_init_early;
 	dd->f_sendctrl          = sendctrl_7220_mod;
 	dd->f_set_armlaunch     = qib_set_7220_armlaunch;
diff -up a/drivers/infiniband/hw/qib/qib_iba7322.c b/drivers/infiniband/hw/qib/qib_iba7322.c
--- a/drivers/infiniband/hw/qib/qib_iba7322.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_iba7322.c	2010-02-18 10:34:37.000000000 -0800
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2008, 2009, 2010 QLogic Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -69,7 +69,7 @@ static void qib_set_ib_7322_lstate(struc
 				   u16 linitcmd);
 static void dump_sdma_7322_state(struct qib_pportdata *);
 static void force_h1(struct qib_pportdata *);
-static void adj_qmh_serdes(struct qib_pportdata *);
+static void adj_tx_serdes(struct qib_pportdata *);
 static u32 qib_7322_setpbc_control(struct qib_pportdata *, u32, u8, u8);
 static void qib_7322_mini_pcs_reset(struct qib_pportdata *);
 
@@ -94,6 +94,11 @@ ushort qib_num_cfg_vls = 2;
 module_param_named(num_vls, qib_num_cfg_vls, ushort, S_IRUGO);
 MODULE_PARM_DESC(num_vls, "Set number of Virtual Lanes to use (1-8)");
 
+ushort qib_singleport;
+module_param_named(singleport, qib_singleport, ushort, S_IRUGO);
+MODULE_PARM_DESC(singleport, "Use only IB port 1; more per-port buffer space");
+
+
 /*
  * Setup QMH7342 receive and transmit parameters, necessary because
  * each bay, Mez connector, and IB port need different tuning, beyond
@@ -109,6 +114,12 @@ static unsigned dummy_qmh_params;
 module_param_call(qmh_serdes_setup, setup_qmh_params, param_get_uint,
 		  &dummy_qmh_params, S_IWUSR | S_IRUGO);
 
+/* similarly for QME7342, but it's simpler */
+static int setup_qme_params(const char *, struct kernel_param *);
+static unsigned dummy_qme_params;
+module_param_call(qme_serdes_setup, setup_qme_params, param_get_uint,
+		  &dummy_qme_params, S_IWUSR | S_IRUGO);
+
 #define BOARD_QME7342 5
 #define BOARD_QMH7342 6
 #define IS_QMH(dd) (SYM_FIELD((dd)->revision, Revision, BoardID) == \
@@ -175,6 +186,16 @@ module_param_call(qmh_serdes_setup, setu
 #define QIB_EEPROM_WEN_NUM 14
 #define QIB_TWSI_EEPROM_DEV 0xA2 /* All Production 7322 cards. */
 
+/* HW counter clock is at 4nsec */
+#define QIB_7322_PSXMITWAIT_CHECK_RATE 4000
+
+/* full speed IB port 1 only */
+#define PORT_SPD_CAP (QIB_IB_SDR | QIB_IB_DDR | QIB_IB_QDR)
+#define PORT_SPD_CAP_SHIFT 3
+
+/* full speed featuremask, both ports */
+#define DUAL_PORT_CAP (PORT_SPD_CAP | (PORT_SPD_CAP << PORT_SPD_CAP_SHIFT))
+
 /*
  * This file contains almost all the chip-specific register information and
  * access functions for the FAKED QLogic InfiniPath 7322 PCI-Express chip.
@@ -525,7 +546,7 @@ struct vendor_txdds_ent {
 #define SERDES_CHANS 4 /* yes, it's obvious, but one less magic number */
 
 #define H1_FORCE_VAL 8
-#define H1_FORCE_QME 7
+#define H1_FORCE_QME 1 /*  may be overridden via setup_qme_params() */
 #define H1_FORCE_QMH 7 /*  may be overridden via setup_qmh_params() */
 
 /*
@@ -552,7 +573,7 @@ static const struct txdds_ent qmh_qdr_tx
 
 static const struct txdds_ent qme_sdr_txdds =  { 11, 0,  4,  4 };
 static const struct txdds_ent qme_ddr_txdds =  {  7, 0,  2,  7 };
-static const struct txdds_ent qme_qdr_txdds =  {  0, 1, 12, 14 };
+static const struct txdds_ent qme_qdr_txdds =  {  0, 1, 12, 11 };
 
 struct qib_chippport_specific {
 	u64 __iomem *kpregbase;
@@ -1477,11 +1498,6 @@ static void qib_7322_sdma_hw_clean_up(st
 	__qib_sdma_process_event(ppd, qib_sdma_event_e50_hw_cleaned);
 }
 
-static int qib_7322_sdma_flush_sends(struct qib_pportdata *ppd)
-{
-	return 0; /* not needed on this chip */
-}
-
 static void qib_sdma_7322_setlengen(struct qib_pportdata *ppd)
 {
 	/*
@@ -1766,12 +1782,13 @@ static void handle_serdes_issues(struct
 		ibclt == IB_7322_LT_STATE_LINKUP))
 		force_h1(ppd);
 
-	if (IS_QMH(ppd->dd) && ppd->link_speed_enabled == QIB_IB_QDR &&
+	if ((IS_QMH(ppd->dd) || IS_QME(ppd->dd)) &&
+	    ppd->link_speed_enabled == QIB_IB_QDR &&
 	    (ibclt == IB_7322_LT_STATE_CFGTEST ||
 	     ibclt == IB_7322_LT_STATE_CFGENH ||
 	     (ibclt >= IB_7322_LT_STATE_POLLACTIVE &&
 	      ibclt <= IB_7322_LT_STATE_SLEEPQUIET)))
-		adj_qmh_serdes(ppd);
+			adj_tx_serdes(ppd);
 
 	if (!ppd->cpspec->qdr_dfe_on && ibclt != IB_7322_LT_STATE_LINKUP &&
 	    ibclt <= IB_7322_LT_STATE_SLEEPQUIET) {
@@ -3488,7 +3505,7 @@ static unsigned qib_7322_boardname(struc
 	/* Will need enumeration of board-types here */
 	char *n;
 	u32 boardid, namelen;
-	unsigned features = 0x3f;
+	unsigned features = DUAL_PORT_CAP;
 
 	boardid = SYM_FIELD(dd->revision, Revision, BoardID);
 
@@ -3499,7 +3516,7 @@ static unsigned qib_7322_boardname(struc
 	case 1:
 		n = "InfiniPath_QLE7340";
 		dd->flags |= QIB_HAS_QSFP;
-		features = 7;
+		features = PORT_SPD_CAP;
 		break;
 	case 2:
 		n = "InfiniPath_QLE7342";
@@ -3519,7 +3536,6 @@ static unsigned qib_7322_boardname(struc
 		break;
 	case BOARD_QME7342:
 		n = "InfiniPath_QME7342";
-		features = 0x24; /* BRINGUP; force QDR for now */
 		break;
 	case 15:
 		n = "InfiniPath_QLE7342_TEST";
@@ -3546,6 +3562,12 @@ static unsigned qib_7322_boardname(struc
 		 dd->majrev, dd->minrev,
 		 (unsigned)SYM_FIELD(dd->revision, Revision_R, SW));
 
+	if (features == DUAL_PORT_CAP && qib_singleport) {
+		qib_devinfo(dd->pcidev, "IB%u: Forced to single port mode"
+			    " by module parameter\n", dd->unit);
+		features = PORT_SPD_CAP;
+	}
+
 	return features;
 }
 
@@ -5556,10 +5578,11 @@ static int qib_7322_ib_updown(struct qib
 		if (!(ppd->lflags & (QIBL_IB_AUTONEG_FAILED |
 				     QIBL_IB_AUTONEG_INPROG)))
 			set_7322_ibspeed_fast(ppd, ppd->link_speed_enabled);
+		if (!(ppd->lflags & QIBL_IB_AUTONEG_INPROG))
+			qib_cancel_sends(ppd);
 		clr = read_7322_creg32_port(ppd, crp_iblinkdown);
 		if (clr == ppd->cpspec->iblnkdownsnap)
 			ppd->cpspec->iblnkdowndelta++;
-		qib_hol_down(ppd);
 	} else {
 		qib_cdbg(LINKVERB, "IB%u:%u Link up act_speed 0x%x\n",
 			 ppd->dd->unit, ppd->port, ppd->link_speed_active);
@@ -6295,9 +6318,8 @@ static int qib_init_7322_variables(struc
 
 	for (pidx = 0; pidx < NUM_IB_PORTS; ++pidx) {
 		struct qib_chippport_specific *cp = ppd->cpspec;
-		ppd->link_speed_supported = features &
-			(QIB_IB_SDR | QIB_IB_DDR | QIB_IB_QDR);
-		features >>= 3;
+		ppd->link_speed_supported = features & PORT_SPD_CAP;
+		features >>=  PORT_SPD_CAP_SHIFT;
 		if (!ppd->link_speed_supported) {
 			/* single port mode (7340, or configured) */
 			dd->skip_kctxt_mask |= 1 << pidx;
@@ -6385,23 +6407,26 @@ static int qib_init_7322_variables(struc
 				  autoneg_7322_work);
 		INIT_DELAYED_WORK(&cp->ipg_work, ipg_7322_work);
 
-		if (IS_QMH(ppd->dd)) {
+		if (IS_QMH(ppd->dd) || IS_QME(ppd->dd)) {
 			int i;
+			const struct txdds_ent *txdds;
+
+			txdds = IS_QMH(ppd->dd) ? &qmh_qdr_txdds :
+				&qme_qdr_txdds;
 
 			/*
 			 * set values in case link comes up
 			 * before table is written to driver.
 			 */
-			cp->h1_val = H1_FORCE_QMH;
+			cp->h1_val = IS_QMH(ppd->dd) ? H1_FORCE_QMH :
+				H1_FORCE_QME;
 			for (i = 0; i < SERDES_CHANS; i++) {
-				cp->amp[i] = qmh_qdr_txdds.amp;
-				cp->pre[i] = qmh_qdr_txdds.pre;
-				cp->mainv[i] = qmh_qdr_txdds.main;
-				cp->post[i] = qmh_qdr_txdds.post;
+				cp->amp[i] = txdds->amp;
+				cp->pre[i] = txdds->pre;
+				cp->mainv[i] = txdds->main;
+				cp->post[i] = txdds->post;
 			}
-		} else if (IS_QME(ppd->dd))
-			cp->h1_val = H1_FORCE_QME;
-		else
+		} else
 			cp->h1_val = H1_FORCE_VAL;
 
 		/* Avoid writes to chip for mini_init */
@@ -6508,6 +6533,8 @@ static int qib_init_7322_variables(struc
 			     << SYM_LSB(SendCtrl, AvailUpdThld)) |
 			SYM_MASK(SendCtrl, SendBufAvailPad64Byte);
 
+	dd->psxmitwait_supported = 1;
+	dd->psxmitwait_check_rate = QIB_7322_PSXMITWAIT_CHECK_RATE;
 bail:
 	if (!dd->ctxtcnt)
 		dd->ctxtcnt = 1; /* for other initialization code */
@@ -6850,16 +6877,17 @@ static void qib_7322_init_ctxt(struct qi
 static void qib_7322_txchk_change(struct qib_devdata *dd, u32 start,
 				  u32 len, u32 which, struct qib_ctxtdata *rcd)
 {
-	int i, last = start + len - 1, lastbuf = -1;
-	int lastr = last / BITS_PER_LONG;
+	int i;
+	const int last = start + len - 1;
+	const int lastr = last / BITS_PER_LONG;
 	u32 sleeps = 0;
 	int wait = rcd != NULL;
 	unsigned long flags;
 
 	while (wait) {
 		unsigned long shadow;
+		int cstart, previ = -1;
 
-		shadow = le64_to_cpu(*dd->pioavailregs_dma);
 		/*
 		 * when flipping from kernel to user, we can't change
 		 * the checking type if the buffer is allocated to the
@@ -6867,25 +6895,32 @@ static void qib_7322_txchk_change(struct
 		 * from close, and we have just disarm'ed all the
 		 * buffers.  All the kernel to kernel changes are also
 		 * OK.
-		 * Because we look at dma copy, wait a bit at start
-		 * for kernel users of buffers to have a change to
-		 * get them to go busy if they just did a pioget.
-		 * This is a hack, but...
 		 */
-		sendctrl_7322_mod(dd->pport, QIB_SENDCTRL_AVAIL_BLIP);
-		for (i = start; i <= last && !test_bit((2 * i) + 1, &shadow);
-			i++)
-			;
-		if (i > last)
+		for (cstart = start; cstart <= last; cstart++) {
+			i = ((2 * cstart) + QLOGIC_IB_SENDPIOAVAIL_BUSY_SHIFT)
+				/ BITS_PER_LONG;
+			if (i != previ) {
+				shadow = (unsigned long)
+					le64_to_cpu(dd->pioavailregs_dma[i]);
+				previ = i;
+			}
+			if (test_bit(((2 * cstart) +
+				      QLOGIC_IB_SENDPIOAVAIL_BUSY_SHIFT)
+				     % BITS_PER_LONG, &shadow))
+				break;
+		}
+
+		if (cstart > last)
 			break;
 
 		if (sleeps == QTXSLEEPS) {
 			qib_dbg("%u:ctxt%u buf %u busy after %u msec, "
 				"continue anyway\n", dd->unit, rcd->ctxt,
-				i, sleeps);
+				cstart, sleeps);
 			break;
 		}
-		lastbuf = i;
+		/* make sure we see an updated copy next time around */
+		sendctrl_7322_mod(dd->pport, QIB_SENDCTRL_AVAIL_BLIP);
 		sleeps++;
 		msleep(1);
 	}
@@ -7082,7 +7117,6 @@ struct qib_devdata *qib_init_iba7322_fun
 	dd->f_xgxs_reset        = qib_7322_mini_pcs_reset;
 	dd->f_sdma_hw_clean_up  = qib_7322_sdma_hw_clean_up;
 	dd->f_sdma_hw_start_up  = qib_7322_sdma_hw_start_up;
-	dd->f_sdma_flush_sends  = qib_7322_sdma_flush_sends;
 	dd->f_sdma_init_early   = qib_7322_sdma_init_early;
 	dd->f_writescratch      = writescratch;
 	dd->f_tempsense_rd	= qib_7322_tempsense_rd;
@@ -7220,9 +7254,17 @@ static const struct vendor_txdds_ent ven
 		{ 0x41, 0x50, 0x48 }, "584470004       ",
 		{  0,  0,  0,  8 }, {  0,  0,  0, 11 }, {  0,  1,  7, 15 },
 	},
+	{ /* Finisar 3m OM2 Optical */
+		{ 0x00, 0x90, 0x65 }, "FCBG410QB1C03-QL",
+		{  0,  0,  0,  3 }, {  0,  0,  0,  4 }, {  0,  0,  0, 13 },
+	},
 	{ /* Finisar 30m OM2 Optical */
-		{ 0x00, 0x90, 0x65 }, "CBL2-1003001    ",
-		{  0,  0,  0,  1 }, {  0,  0,  0,  5 }, {  0,  0,  0, 10 },
+		{ 0x00, 0x90, 0x65 }, "FCBG410QB1C30-QL",
+		{  0,  0,  0,  1 }, {  0,  0,  0,  5 }, {  0,  0,  0, 11 },
+	},
+	{ /* Finisar Default OM2 Optical */
+		{ 0x00, 0x90, 0x65 }, NULL,
+		{  0,  0,  0,  2 }, {  0,  0,  0,  5 }, {  0,  0,  0, 12 },
 	},
 	{ /* Gore 1m 30awg NoEq */
 		{ 0x00, 0x21, 0x77 }, "QSN3300-1       ",
@@ -7248,18 +7290,34 @@ static const struct vendor_txdds_ent ven
 		{ 0x00, 0x21, 0x77 }, "QSN7000-7       ",
 		{  0,  0,  0,  9 }, {  0,  0,  0, 11 }, {  0,  2,  6, 15 },
 	},
-	{ /* Gore 3m 26awg Eq */
-		{ 0x00, 0x21, 0x77 }, "QSN7600-3       ",
-		{ 11,  0,  3,  9 }, {  8,  0,  6,  1 }, {  0,  1, 15, 10 },
-	},
 	{ /* Gore 5m 26awg Eq */
 		{ 0x00, 0x21, 0x77 }, "QSN7600-5       ",
 		{  0,  0,  0,  8 }, {  0,  0,  0, 11 }, {  0,  1,  9, 13 },
 	},
 	{ /* Intersil 12m 24awg Active */
-		{ 0x00, 0x30, 0xB4 }, NULL,
+		{ 0x00, 0x30, 0xB4 }, "QLX4000CQSFP1224",
 		{  0,  0,  0,  2 }, {  0,  0,  0,  5 }, {  0,  3,  0,  9 },
 	},
+	{ /* Intersil 10m 28awg Active */
+		{ 0x00, 0x30, 0xB4 }, "QLX4000CQSFP1028",
+		{  0,  0,  0,  6 }, {  0,  0,  0,  4 }, {  0,  2,  0,  2 },
+	},
+	{ /* Intersil 7m 30awg Active */
+		{ 0x00, 0x30, 0xB4 }, "QLX4000CQSFP0730",
+		{  0,  0,  0,  6 }, {  0,  0,  0,  4 }, {  0,  1,  0,  3 },
+	},
+	{ /* Intersil 5m 32awg Active */
+		{ 0x00, 0x30, 0xB4 }, "QLX4000CQSFP0532",
+		{  0,  0,  0,  6 }, {  0,  0,  0,  6 }, {  0,  2,  0,  8 },
+	},
+	{ /* Intersil Default Active */
+		{ 0x00, 0x30, 0xB4 }, NULL,
+		{  0,  0,  0,  6 }, {  0,  0,  0,  5 }, {  0,  2,  0,  5 },
+	},
+	{ /* Luxtera 20m Active Optical */
+		{ 0x00, 0x25, 0x63 }, NULL,
+		{  0,  0,  0,  5 }, {  0,  0,  0,  8 }, {  0,  2,  0,  12 },
+	},
 	{ /* Molex 1M Cu loopback */
 		{ 0x00, 0x09, 0x3A }, "74763-0025      ",
 		{  2,  2,  6, 15 }, {  2,  2,  6, 15 }, {  2,  2,  6, 15 },
@@ -7330,6 +7388,22 @@ static const struct txdds_ent txdds_qdr[
 	{  0, 2,  9, 15 },	/* 16 dB */
 };
 
+static const struct txdds_ent *get_atten_table(const struct txdds_ent *txdds,
+					       unsigned atten)
+{
+	/*
+	 * The attenuation table starts at 2dB for entry 1,
+	 * with entry 0 being the loopback entry.
+	 */
+	if (atten <= 2)
+		atten = 1;
+	else if (atten > TXDDS_TABLE_SZ)
+		atten = TXDDS_TABLE_SZ - 1;
+	else
+		atten--;
+	return txdds + atten;
+}
+
 static void find_best_ent(struct qib_pportdata *ppd,
 			  const struct txdds_ent **sdr_dds,
 			  const struct txdds_ent **ddr_dds,
@@ -7365,14 +7439,9 @@ static void find_best_ent(struct qib_ppo
 	}
 
 	if (QSFP_HAS_ATTEN(qd->tech)) {
-		unsigned atten;
-
-		atten = (qd->atten[0] << 8) | qd->atten[1];
-		if (atten >= TXDDS_TABLE_SZ)
-			atten = TXDDS_TABLE_SZ - 1;
-		*sdr_dds = txdds_sdr + atten;
-		*ddr_dds = txdds_ddr + atten;
-		*qdr_dds = txdds_qdr + atten;
+		*sdr_dds = get_atten_table(txdds_sdr, qd->atten[0]);
+		*ddr_dds = get_atten_table(txdds_ddr, qd->atten[0]);
+		*qdr_dds = get_atten_table(txdds_qdr, qd->atten[1]);
 		return;
 	}
 
@@ -7567,8 +7636,8 @@ static int serdes_7322_init(struct qib_p
 	/* LoS filter select enabled */
 	ahb_mod(ppd->dd, IBSD(ppd->hw_pidx), 5, 9, 1 << 15, 1 << 15);
 
-	/* LoS target data = 4*/
-	ibsd_wr_allchans(ppd, 14, (4 << 3), BMASK(5, 3));
+	/* LoS target data = 2 */
+	ibsd_wr_allchans(ppd, 14, (2 << 3), BMASK(5, 3));
 
 	data = qib_read_kreg_port(ppd, krp_serdesctrl);
 	qib_write_kreg_port(ppd, krp_serdesctrl, data |
@@ -7643,7 +7712,7 @@ static void clock_man(struct qib_pportda
  * per-channel settings in the future, and that method only needs
  * to be done once.
  */
-static void adj_qmh_serdes(struct qib_pportdata *ppd)
+static void adj_tx_serdes(struct qib_pportdata *ppd)
 {
 	struct txdds_ent txdds;
 	u64 deemph;
@@ -7845,7 +7914,7 @@ static int setup_qmh_params(const char *
 				if (parm == ((5 * SERDES_CHANS) - 1)) {
 					/* At the end of a port, set params */
 					qib_dbg("Finished line, set params\n");
-					adj_qmh_serdes(ppd);
+					adj_tx_serdes(ppd);
 				}
 			}
 		}
@@ -7860,6 +7929,107 @@ bail:
 	return ret;
 }
 
+/*
+ * Similarly for QME7342, but the format is simpler, values are the
+ * same for all mez card positions in a blade (2 or 4 per blade), but
+ * are different for some blades vs others, and we don't need to
+ * specify different parameters for different serdes channels or different
+ * IB ports.
+ * Format is: h1 amp,pre,post,main
+ * Alternate format (so ports can be different): Pport# h1 amp,pre,post,main
+ */
+#define N_QME_FIELDS 5
+static int setup_qme_params(const char *str, struct kernel_param *kp)
+{
+	char *abuf, *v, *nv, *nvp;
+	struct qib_devdata *dd;
+	u32 vlen, nf, port = 0;
+	u8 h1, tx[4]; /* amp, pre, post, main */
+	int ret =  -EINVAL;
+	char *seplist;
+
+	vlen = strlen(str) + 1;
+	abuf = kmalloc(vlen, GFP_KERNEL);
+	if (!abuf) {
+		printk(KERN_INFO QIB_DRV_NAME
+		       "Unable to allocate QME param buffer; ignoring\n");
+		return 0;
+	}
+	strncpy(abuf, str, vlen);
+
+	v = abuf;
+	seplist = " \t";
+	h1 = H1_FORCE_QME; /* gcc can't figure out always set before used */
+
+	for (nf = 0; (nv = strsep(&v, seplist)); ) {
+		u32 val;
+
+		if (!*nv)
+			/* allow for multiple separators */
+			continue;
+
+		if (!nf && *nv == 'P') {
+			/* alternate format with port */
+			val = simple_strtoul(++nv, &nvp, 0);
+			if (nv == nvp || port >= NUM_IB_PORTS) {
+				printk(KERN_INFO QIB_DRV_NAME
+				       " %s: non-numeric port value (%s) "
+				       "ignoring rest\n", __func__, nv);
+				goto done;
+			}
+			port = val;
+			continue; /* without incrementing nf */
+		}
+		val = simple_strtoul(nv, &nvp, 0);
+		if (nv == nvp) {
+			printk(KERN_INFO QIB_DRV_NAME
+			       " %s: non-numeric value (%s) "
+			       "field #%u, ignoring rest\n", __func__,
+			       nv, nf);
+			goto done;
+		}
+		if (!nf) {
+			h1 = val;
+			seplist = ",";
+		} else
+			tx[nf - 1] = val;
+		if (++nf == N_QME_FIELDS) {
+			list_for_each_entry(dd, &qib_dev_list, list) {
+				int pidx, i;
+				if (!IS_QME(dd))
+					continue; /* only for QME cards */
+				for (pidx = 0; pidx < dd->num_pports; ++pidx) {
+					struct qib_pportdata *ppd;
+					ppd = &dd->pport[pidx];
+					if ((port && ppd->port != port) ||
+						!ppd->link_speed_supported)
+						continue;
+					ppd->cpspec->h1_val = h1;
+					for (i = 0; i < SERDES_CHANS; i++) {
+						ppd->cpspec->amp[i] = tx[0];
+						ppd->cpspec->pre[i] = tx[1];
+						ppd->cpspec->post[i] = tx[2];
+						ppd->cpspec->mainv[i] = tx[3];
+					}
+					qib_cdbg(INIT, "IB%u:%u set h1=%u "
+						 "amp=%u pre=%u post=%u "
+						 "main=%u\n", dd->unit,
+						 ppd->port, h1, tx[0], tx[1],
+						 tx[2], tx[3]);
+					adj_tx_serdes(ppd);
+				}
+			}
+			ret = 0;
+			goto done;
+		}
+	}
+	printk(KERN_INFO QIB_DRV_NAME
+	       " %s: Only %u of %u fields provided, skipping\n",
+	       __func__, nf, N_QME_FIELDS);
+done:
+	return ret;
+}
+
 #define SJA_EN SYM_MASK(SPC_JTAG_ACCESS_REG, SPC_JTAG_ACCESS_EN)
 #define BISTEN_LSB SYM_LSB(SPC_JTAG_ACCESS_REG, bist_en)
 
Only in b/drivers/infiniband/hw/qib: qib_iba7322.c.orig
diff -up a/drivers/infiniband/hw/qib/qib_init.c b/drivers/infiniband/hw/qib/qib_init.c
--- a/drivers/infiniband/hw/qib/qib_init.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_init.c	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2003, 2004, 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -85,6 +86,8 @@ struct workqueue_struct *qib_wq;
 static void verify_interrupt(unsigned long);
 
 static struct idr qib_unit_table;
+u32 qib_cpulist_count;
+unsigned long *qib_cpulist;
 
 /* set number of contexts we'll actually use */
 void qib_set_ctxtcnt(struct qib_devdata *dd)
@@ -982,7 +985,20 @@ struct qib_devdata *qib_alloc_devdata(st
 			      "Could not allocate unit ID: error %d\n", -ret);
 		ib_dealloc_device(&dd->verbs_dev.ibdev);
 		dd = ERR_PTR(ret);
+		goto bail;
+	}
+
+	if (!qib_cpulist_count) {
+		u32 count = num_online_cpus();
+		qib_cpulist = kzalloc(BITS_TO_LONGS(count) *
+				  sizeof(long), GFP_KERNEL);
+		if (qib_cpulist)
+			qib_cpulist_count = count;
+		else
+			qib_early_err(&pdev->dev, "Could not alloc cpulist "
+				      "info, cpu affinity might be wrong\n");
 	}
+
 bail:
 	return dd;
 }
@@ -1142,6 +1158,9 @@ static void __exit qlogic_ib_cleanup(voi
 
 	destroy_workqueue(qib_wq);
 
+	qib_cpulist_count = 0;
+	kfree(qib_cpulist);
+
 	idr_destroy(&qib_unit_table);
 	qib_trace_fini();
 	qib_dev_cleanup();
diff -up a/drivers/infiniband/hw/qib/qib_intr.c b/drivers/infiniband/hw/qib/qib_intr.c
--- a/drivers/infiniband/hw/qib/qib_intr.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_intr.c	2010-02-18 10:34:37.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2003, 2004, 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -128,6 +129,7 @@ void handle_e_ibstatuschanged(struct qib
 			spin_unlock_irqrestore(&ppd->lflags_lock, flags);
 		} else if (ltstate == IB_PHYSPORTSTATE_LINKUP) {
 			/* active, but not active defered */
+			qib_hol_up(ppd); /* useful only for 6120 now */
 			*ppd->statusp |=
 				QIB_STATUS_IB_READY | QIB_STATUS_IB_CONF;
 			spin_lock_irqsave(&ppd->lflags_lock, flags);
@@ -140,7 +142,6 @@ void handle_e_ibstatuschanged(struct qib
 					qib_sdma_event_e30_go_running);
 			signal_ib_event(ppd, IB_EVENT_PORT_ACTIVE);
 			dd->f_setextled(ppd, 1);
-			qib_hol_up(ppd);
 		}
 	} else { /* down */
 		if (ppd->lflags & QIBL_LINKACTIVE)
diff -up a/drivers/infiniband/hw/qib/qib_mad.c b/drivers/infiniband/hw/qib/qib_mad.c
--- a/drivers/infiniband/hw/qib/qib_mad.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_mad.c	2010-02-18 10:34:37.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -638,6 +639,7 @@ static int subn_set_portinfo(struct ib_s
 	u8 lse;
 	u8 state;
 	u8 vls;
+	u8 msl;
 	u16 lstate;
 	int ret, ore, mtu;
 	u32 port_num = be32_to_cpu(smp->attr_mod);
@@ -678,15 +680,23 @@ static int subn_set_portinfo(struct ib_s
 	}
 
 	smlid = be16_to_cpu(pip->sm_lid);
+	msl = pip->neighbormtu_mastersmsl & 0xF;
 	/* Must be a valid unicast LID address. */
 	if (smlid == 0 || smlid >= QIB_MULTICAST_LID_BASE)
 		goto err;
-	if (smlid != ibp->sm_lid) {
+	if (smlid != ibp->sm_lid || msl != ibp->sm_sl) {
 		spin_lock_irqsave(&ibp->lock, flags);
-		if (ibp->sm_ah)
-			ibp->sm_ah->attr.dlid = smlid;
+		if (ibp->sm_ah) {
+			if (smlid != ibp->sm_lid)
+				ibp->sm_ah->attr.dlid = smlid;
+			if (msl != ibp->sm_sl)
+				ibp->sm_ah->attr.sl = msl;
+		}
 		spin_unlock_irqrestore(&ibp->lock, flags);
-		ibp->sm_lid = smlid;
+		if (smlid != ibp->sm_lid)
+			ibp->sm_lid = smlid;
+		if (msl != ibp->sm_sl)
+			ibp->sm_sl = msl;
 		event.event = IB_EVENT_SM_CHANGE;
 		ib_dispatch_event(&event);
 	}
@@ -741,8 +751,6 @@ static int subn_set_portinfo(struct ib_s
 		goto err;
 	qib_set_mtu(ppd, mtu);
 
-	ibp->sm_sl = pip->neighbormtu_mastersmsl & 0xF;
-
 	/* Set operational VLs */
 	vls = (pip->operationalvl_pei_peo_fpi_fpo >> 4) & 0xF;
 	if (vls) {
@@ -1113,10 +1121,12 @@ static int subn_trap_repress(struct ib_s
 	return IB_MAD_RESULT_SUCCESS | IB_MAD_RESULT_CONSUMED;
 }
 
-static int pma_get_classportinfo(struct ib_perf *pmp)
+static int pma_get_classportinfo(struct ib_perf *pmp,
+				 struct ib_device *ibdev)
 {
 	struct ib_pma_classportinfo *p =
 		(struct ib_pma_classportinfo *)pmp->data;
+	struct qib_devdata *dd = dd_from_ibdev(ibdev);
 
 	memset(pmp->data, 0, sizeof(pmp->data));
 
@@ -1128,6 +1138,11 @@ static int pma_get_classportinfo(struct
 	p->class_version = 1;
 	p->cap_mask = IB_PMA_CLASS_CAP_EXT_WIDTH;
 	/*
+	 * Set the most significant bit of CM2 to indicate support for
+	 * congestion statistics
+	 */
+	p->reserved[0] = dd->psxmitwait_supported << 7;
+	/*
 	 * Expected response time is 4.096 usec. * 2^18 == 1.073741824 sec.
 	 */
 	p->resp_time_value = 18;
@@ -1183,7 +1198,7 @@ static int pma_set_portsamplescontrol(st
 	struct qib_ibport *ibp = to_iport(ibdev, port);
 	struct qib_pportdata *ppd = ppd_from_ibp(ibp);
 	unsigned long flags;
-	u8 status;
+	u8 status, xmit_flags;
 	int ret;
 
 	if (pmp->attr_mod != 0 || p->port_select != port) {
@@ -1193,8 +1208,14 @@ static int pma_set_portsamplescontrol(st
 	}
 
 	spin_lock_irqsave(&ibp->lock, flags);
+
+	/* Port Sampling code owns the PS* HW counters */
+	xmit_flags = ppd->cong_stats.flags;
+	ppd->cong_stats.flags = IB_PMA_CONG_HW_CONTROL_SAMPLE;
 	status = dd->f_portcntr(ppd, QIBPORTCNTR_PSSTAT);
-	if (status == IB_PMA_SAMPLE_STATUS_DONE) {
+	if (status == IB_PMA_SAMPLE_STATUS_DONE ||
+	    (status == IB_PMA_SAMPLE_STATUS_RUNNING &&
+	     xmit_flags == IB_PMA_CONG_HW_CONTROL_TIMER)) {
 		ibp->pma_sample_start = be32_to_cpu(p->sample_start);
 		ibp->pma_sample_interval = be32_to_cpu(p->sample_interval);
 		ibp->pma_tag = be16_to_cpu(p->tag);
@@ -1242,6 +1263,60 @@ static u64 get_counter(struct qib_ibport
 	return ret;
 }
 
+/* This function assumes that the xmit_wait lock is already held */
+static u64 xmit_wait_get_value_delta(struct qib_pportdata *ppd)
+{
+	u32 delta;
+
+	delta = get_counter(&ppd->ibport_data, ppd,
+			    IB_PMA_PORT_XMIT_WAIT);
+	return ppd->cong_stats.counter + delta;
+}
+
+static void cache_hw_sample_counters(struct qib_pportdata *ppd)
+{
+	struct qib_ibport *ibp = &ppd->ibport_data;
+
+	ppd->cong_stats.counter_cache.psxmitdata =
+		get_counter(ibp, ppd, IB_PMA_PORT_XMIT_DATA);
+	ppd->cong_stats.counter_cache.psrcvdata =
+		get_counter(ibp, ppd, IB_PMA_PORT_RCV_DATA);
+	ppd->cong_stats.counter_cache.psxmitpkts =
+		get_counter(ibp, ppd, IB_PMA_PORT_XMIT_PKTS);
+	ppd->cong_stats.counter_cache.psrcvpkts =
+		get_counter(ibp, ppd, IB_PMA_PORT_RCV_PKTS);
+	ppd->cong_stats.counter_cache.psxmitwait =
+		get_counter(ibp, ppd, IB_PMA_PORT_XMIT_WAIT);
+}
+
+static u64 get_cache_hw_sample_counters(struct qib_pportdata *ppd,
+					__be16 sel)
+{
+	u64 ret;
+
+	switch (sel) {
+	case IB_PMA_PORT_XMIT_DATA:
+		ret = ppd->cong_stats.counter_cache.psxmitdata;
+		break;
+	case IB_PMA_PORT_RCV_DATA:
+		ret = ppd->cong_stats.counter_cache.psrcvdata;
+		break;
+	case IB_PMA_PORT_XMIT_PKTS:
+		ret = ppd->cong_stats.counter_cache.psxmitpkts;
+		break;
+	case IB_PMA_PORT_RCV_PKTS:
+		ret = ppd->cong_stats.counter_cache.psrcvpkts;
+		break;
+	case IB_PMA_PORT_XMIT_WAIT:
+		ret = ppd->cong_stats.counter_cache.psxmitwait;
+		break;
+	default:
+		ret = 0;
+	}
+
+	return ret;
+}
+
 static int pma_get_portsamplesresult(struct ib_perf *pmp,
 				     struct ib_device *ibdev, u8 port)
 {
@@ -1258,12 +1333,24 @@ static int pma_get_portsamplesresult(str
 	memset(pmp->data, 0, sizeof(pmp->data));
 	spin_lock_irqsave(&ibp->lock, flags);
 	p->tag = cpu_to_be16(ibp->pma_tag);
-	status = dd->f_portcntr(ppd, QIBPORTCNTR_PSSTAT);
-	p->sample_status = cpu_to_be16(status);
+	if (ppd->cong_stats.flags == IB_PMA_CONG_HW_CONTROL_TIMER)
+		p->sample_status = IB_PMA_SAMPLE_STATUS_DONE;
+	else {
+		status = dd->f_portcntr(ppd, QIBPORTCNTR_PSSTAT);
+		p->sample_status = cpu_to_be16(status);
+		if (status == IB_PMA_SAMPLE_STATUS_DONE) {
+			cache_hw_sample_counters(ppd);
+			ppd->cong_stats.counter =
+				xmit_wait_get_value_delta(ppd);
+			dd->f_set_cntr_sample(ppd,
+					      QIB_CONG_TIMER_PSINTERVAL, 0);
+			ppd->cong_stats.flags = IB_PMA_CONG_HW_CONTROL_TIMER;
+		}
+	}
 	for (i = 0; i < ARRAY_SIZE(ibp->pma_counter_select); i++)
-		p->counter[i] = (status != IB_PMA_SAMPLE_STATUS_DONE) ? 0 :
-		    cpu_to_be32(
-			get_counter(ibp, ppd, ibp->pma_counter_select[i]));
+		p->counter[i] = cpu_to_be32(
+			get_cache_hw_sample_counters(
+				ppd, ibp->pma_counter_select[i]));
 	spin_unlock_irqrestore(&ibp->lock, flags);
 
 	return reply((struct ib_smp *) pmp);
@@ -1278,19 +1365,35 @@ static int pma_get_portsamplesresult_ext
 	struct qib_devdata *dd = dd_from_dev(dev);
 	struct qib_ibport *ibp = to_iport(ibdev, port);
 	struct qib_pportdata *ppd = ppd_from_ibp(ibp);
+	unsigned long flags;
 	u8 status;
 	int i;
 
+	/* Port Sampling code owns the PS* HW counters */
 	memset(pmp->data, 0, sizeof(pmp->data));
+	spin_lock_irqsave(&ibp->lock, flags);
 	p->tag = cpu_to_be16(ibp->pma_tag);
-	status = dd->f_portcntr(ppd, QIBPORTCNTR_PSSTAT);
-	p->sample_status = cpu_to_be16(status);
-	/* 64 bits */
-	p->extended_width = cpu_to_be32(0x80000000);
+	if (ppd->cong_stats.flags == IB_PMA_CONG_HW_CONTROL_TIMER)
+		p->sample_status = IB_PMA_SAMPLE_STATUS_DONE;
+	else {
+		status = dd->f_portcntr(ppd, QIBPORTCNTR_PSSTAT);
+		p->sample_status = cpu_to_be16(status);
+		/* 64 bits */
+		p->extended_width = cpu_to_be32(0x80000000);
+		if (status == IB_PMA_SAMPLE_STATUS_DONE) {
+			cache_hw_sample_counters(ppd);
+			ppd->cong_stats.counter =
+				xmit_wait_get_value_delta(ppd);
+			dd->f_set_cntr_sample(ppd,
+					      QIB_CONG_TIMER_PSINTERVAL, 0);
+			ppd->cong_stats.flags = IB_PMA_CONG_HW_CONTROL_TIMER;
+		}
+	}
 	for (i = 0; i < ARRAY_SIZE(ibp->pma_counter_select); i++)
-		p->counter[i] = (status != IB_PMA_SAMPLE_STATUS_DONE) ? 0 :
-		    cpu_to_be64(
-			get_counter(ibp, ppd, ibp->pma_counter_select[i]));
+		p->counter[i] = cpu_to_be64(
+			get_cache_hw_sample_counters(
+				ppd, ibp->pma_counter_select[i]));
+	spin_unlock_irqrestore(&ibp->lock, flags);
 
 	return reply((struct ib_smp *) pmp);
 }
@@ -1393,6 +1496,120 @@ static int pma_get_portcounters(struct i
 	return reply((struct ib_smp *) pmp);
 }
 
+static int pma_get_portcounters_cong(struct ib_perf *pmp,
+				     struct ib_device *ibdev, u8 port)
+{
+	/* Congestion PMA packets start at offset 24 not 64 */
+	struct ib_pma_portcounters_cong *p =
+		(struct ib_pma_portcounters_cong *)pmp->reserved;
+	struct qib_verbs_counters cntrs;
+	struct qib_ibport *ibp = to_iport(ibdev, port);
+	struct qib_pportdata *ppd = ppd_from_ibp(ibp);
+	struct qib_devdata *dd = dd_from_ppd(ppd);
+	u32 port_select = cpu_to_be32(pmp->attr_mod) & 0xFF;
+	u64 xmit_wait_counter;
+	unsigned long flags;
+
+	/*
+	 * This check is performed only in the GET method because the
+	 * SET method ends up calling this anyway.
+	 */
+	if (!dd->psxmitwait_supported)
+		pmp->status |= IB_SMP_UNSUP_METH_ATTR;
+	if ((!(ibp->port_cap_flags & IB_PMA_CLASS_CAP_ALLPORTSELECT) &&
+	     port_select == 0xFF) || port_select != port)
+		pmp->status |= IB_SMP_INVALID_FIELD;
+
+	qib_get_counters(ppd, &cntrs);
+	spin_lock_irqsave(&ppd->ibport_data.lock, flags);
+	xmit_wait_counter = xmit_wait_get_value_delta(ppd);
+	spin_unlock_irqrestore(&ppd->ibport_data.lock, flags);
+
+	/* Adjust counters for any resets done. */
+	cntrs.symbol_error_counter -= ibp->z_symbol_error_counter;
+	cntrs.link_error_recovery_counter -=
+		ibp->z_link_error_recovery_counter;
+	cntrs.link_downed_counter -= ibp->z_link_downed_counter;
+	cntrs.port_rcv_errors -= ibp->z_port_rcv_errors;
+	cntrs.port_rcv_remphys_errors -=
+		ibp->z_port_rcv_remphys_errors;
+	cntrs.port_xmit_discards -= ibp->z_port_xmit_discards;
+	cntrs.local_link_integrity_errors -=
+		ibp->z_local_link_integrity_errors;
+	cntrs.excessive_buffer_overrun_errors -=
+		ibp->z_excessive_buffer_overrun_errors;
+	cntrs.vl15_dropped -= ibp->z_vl15_dropped;
+	cntrs.vl15_dropped += ibp->n_vl15_dropped;
+	cntrs.port_xmit_data -= ibp->z_port_xmit_data;
+	cntrs.port_rcv_data -= ibp->z_port_rcv_data;
+	cntrs.port_xmit_packets -= ibp->z_port_xmit_packets;
+	cntrs.port_rcv_packets -= ibp->z_port_rcv_packets;
+
+	memset(pmp->reserved, 0, sizeof(pmp->reserved) +
+	       sizeof(pmp->data));
+
+	/*
+	 * Set top 3 bits to indicate interval in picoseconds in
+	 * remaining bits.
+	 */
+	p->port_check_rate =
+		cpu_to_be16((QIB_XMIT_RATE_PICO << 13) |
+			    (dd->psxmitwait_check_rate &
+			     ~(QIB_XMIT_RATE_PICO << 13)));
+	p->port_adr_events = cpu_to_be64(0);
+	p->port_xmit_wait = cpu_to_be64(xmit_wait_counter);
+	p->port_xmit_data = cpu_to_be64(cntrs.port_xmit_data);
+	p->port_rcv_data = cpu_to_be64(cntrs.port_rcv_data);
+	p->port_xmit_packets =
+		cpu_to_be64(cntrs.port_xmit_packets);
+	p->port_rcv_packets =
+		cpu_to_be64(cntrs.port_rcv_packets);
+	if (cntrs.symbol_error_counter > 0xFFFFUL)
+		p->symbol_error_counter = cpu_to_be16(0xFFFF);
+	else
+		p->symbol_error_counter =
+			cpu_to_be16(
+				(u16)cntrs.symbol_error_counter);
+	if (cntrs.link_error_recovery_counter > 0xFFUL)
+		p->link_error_recovery_counter = 0xFF;
+	else
+		p->link_error_recovery_counter =
+			(u8)cntrs.link_error_recovery_counter;
+	if (cntrs.link_downed_counter > 0xFFUL)
+		p->link_downed_counter = 0xFF;
+	else
+		p->link_downed_counter =
+			(u8)cntrs.link_downed_counter;
+	if (cntrs.port_rcv_errors > 0xFFFFUL)
+		p->port_rcv_errors = cpu_to_be16(0xFFFF);
+	else
+		p->port_rcv_errors =
+			cpu_to_be16((u16) cntrs.port_rcv_errors);
+	if (cntrs.port_rcv_remphys_errors > 0xFFFFUL)
+		p->port_rcv_remphys_errors = cpu_to_be16(0xFFFF);
+	else
+		p->port_rcv_remphys_errors =
+			cpu_to_be16(
+				(u16)cntrs.port_rcv_remphys_errors);
+	if (cntrs.port_xmit_discards > 0xFFFFUL)
+		p->port_xmit_discards = cpu_to_be16(0xFFFF);
+	else
+		p->port_xmit_discards =
+			cpu_to_be16((u16)cntrs.port_xmit_discards);
+	if (cntrs.local_link_integrity_errors > 0xFUL)
+		cntrs.local_link_integrity_errors = 0xFUL;
+	if (cntrs.excessive_buffer_overrun_errors > 0xFUL)
+		cntrs.excessive_buffer_overrun_errors = 0xFUL;
+	p->lli_ebor_errors = (cntrs.local_link_integrity_errors << 4) |
+		cntrs.excessive_buffer_overrun_errors;
+	if (cntrs.vl15_dropped > 0xFFFFUL)
+		p->vl15_dropped = cpu_to_be16(0xFFFF);
+	else
+		p->vl15_dropped = cpu_to_be16((u16)cntrs.vl15_dropped);
+
+	return reply((struct ib_smp *)pmp);
+}
+
 static int pma_get_portcounters_ext(struct ib_perf *pmp,
 				    struct ib_device *ibdev, u8 port)
 {
@@ -1495,6 +1712,58 @@ static int pma_set_portcounters(struct i
 	return pma_get_portcounters(pmp, ibdev, port);
 }
 
+static int pma_set_portcounters_cong(struct ib_perf *pmp,
+				     struct ib_device *ibdev, u8 port)
+{
+	struct qib_ibport *ibp = to_iport(ibdev, port);
+	struct qib_pportdata *ppd = ppd_from_ibp(ibp);
+	struct qib_devdata *dd = dd_from_ppd(ppd);
+	struct qib_verbs_counters cntrs;
+	u32 counter_select =
+		(cpu_to_be32(pmp->attr_mod) >> 24) & 0xFF;
+	int ret = 0;
+	unsigned long flags;
+
+	qib_get_counters(ppd, &cntrs);
+	/* Get counter values before we save them */
+	ret = pma_get_portcounters_cong(pmp, ibdev, port);
+
+	if (counter_select & IB_PMA_SEL_CONG_XMIT) {
+		spin_lock_irqsave(&ppd->ibport_data.lock, flags);
+		ppd->cong_stats.counter = 0;
+		dd->f_set_cntr_sample(ppd, QIB_CONG_TIMER_PSINTERVAL,
+				      0x0);
+		spin_unlock_irqrestore(&ppd->ibport_data.lock, flags);
+	}
+	if (counter_select & IB_PMA_SEL_CONG_PORT_DATA) {
+		ibp->z_port_xmit_data = cntrs.port_xmit_data;
+		ibp->z_port_rcv_data = cntrs.port_rcv_data;
+		ibp->z_port_xmit_packets = cntrs.port_xmit_packets;
+		ibp->z_port_rcv_packets = cntrs.port_rcv_packets;
+	}
+	if (counter_select & IB_PMA_SEL_CONG_ALL) {
+		ibp->z_symbol_error_counter =
+			cntrs.symbol_error_counter;
+		ibp->z_link_error_recovery_counter =
+			cntrs.link_error_recovery_counter;
+		ibp->z_link_downed_counter =
+			cntrs.link_downed_counter;
+		ibp->z_port_rcv_errors = cntrs.port_rcv_errors;
+		ibp->z_port_rcv_remphys_errors =
+			cntrs.port_rcv_remphys_errors;
+		ibp->z_port_xmit_discards =
+			cntrs.port_xmit_discards;
+		ibp->z_local_link_integrity_errors =
+			cntrs.local_link_integrity_errors;
+		ibp->z_excessive_buffer_overrun_errors =
+			cntrs.excessive_buffer_overrun_errors;
+		ibp->n_vl15_dropped = 0;
+		ibp->z_vl15_dropped = cntrs.vl15_dropped;
+	}
+
+	return ret;
+}
+
 static int pma_set_portcounters_ext(struct ib_perf *pmp,
 				    struct ib_device *ibdev, u8 port)
 {
@@ -1702,7 +1971,7 @@ static int process_perf(struct ib_device
 	case IB_MGMT_METHOD_GET:
 		switch (pmp->attr_id) {
 		case IB_PMA_CLASS_PORT_INFO:
-			ret = pma_get_classportinfo(pmp);
+			ret = pma_get_classportinfo(pmp, ibdev);
 			goto bail;
 		case IB_PMA_PORT_SAMPLES_CONTROL:
 			ret = pma_get_portsamplescontrol(pmp, ibdev, port);
@@ -1719,6 +1988,9 @@ static int process_perf(struct ib_device
 		case IB_PMA_PORT_COUNTERS_EXT:
 			ret = pma_get_portcounters_ext(pmp, ibdev, port);
 			goto bail;
+		case IB_PMA_PORT_COUNTERS_CONG:
+			ret = pma_get_portcounters_cong(pmp, ibdev, port);
+			goto bail;
 		default:
 			pmp->status |= IB_SMP_UNSUP_METH_ATTR;
 			ret = reply((struct ib_smp *) pmp);
@@ -1736,6 +2008,9 @@ static int process_perf(struct ib_device
 		case IB_PMA_PORT_COUNTERS_EXT:
 			ret = pma_set_portcounters_ext(pmp, ibdev, port);
 			goto bail;
+		case IB_PMA_PORT_COUNTERS_CONG:
+			ret = pma_set_portcounters_cong(pmp, ibdev, port);
+			goto bail;
 		default:
 			pmp->status |= IB_SMP_UNSUP_METH_ATTR;
 			ret = reply((struct ib_smp *) pmp);
@@ -1810,6 +2085,30 @@ static void send_handler(struct ib_mad_a
 	ib_free_send_mad(mad_send_wc->send_buf);
 }
 
+static void xmit_wait_timer_func(unsigned long opaque)
+{
+	struct qib_pportdata *ppd = (struct qib_pportdata *)opaque;
+	struct qib_devdata *dd = dd_from_ppd(ppd);
+	unsigned long flags;
+	u8 status;
+
+	spin_lock_irqsave(&ppd->ibport_data.lock, flags);
+	if (ppd->cong_stats.flags == IB_PMA_CONG_HW_CONTROL_SAMPLE) {
+		status = dd->f_portcntr(ppd, QIBPORTCNTR_PSSTAT);
+		if (status == IB_PMA_SAMPLE_STATUS_DONE) {
+			/* save counter cache */
+			cache_hw_sample_counters(ppd);
+			ppd->cong_stats.flags = IB_PMA_CONG_HW_CONTROL_TIMER;
+		} else
+			goto done;
+	}
+	ppd->cong_stats.counter = xmit_wait_get_value_delta(ppd);
+	dd->f_set_cntr_sample(ppd, QIB_CONG_TIMER_PSINTERVAL, 0x0);
+done:
+	spin_unlock_irqrestore(&ppd->ibport_data.lock, flags);
+	mod_timer(&ppd->cong_stats.timer, jiffies + HZ);
+}
+
 int qib_create_agents(struct qib_ibdev *dev)
 {
 	struct qib_devdata *dd = dd_from_dev(dev);
@@ -1827,6 +2126,16 @@ int qib_create_agents(struct qib_ibdev *
 			ret = PTR_ERR(agent);
 			goto err;
 		}
+
+		/* Initialize xmit_wait structure */
+		dd->pport[p].cong_stats.counter = 0;
+		init_timer(&dd->pport[p].cong_stats.timer);
+		dd->pport[p].cong_stats.timer.function = xmit_wait_timer_func;
+		dd->pport[p].cong_stats.timer.data =
+			(unsigned long)(&dd->pport[p]);
+		dd->pport[p].cong_stats.timer.expires = 0;
+		add_timer(&dd->pport[p].cong_stats.timer);
+
 		ibp->send_agent = agent;
 	}
 
@@ -1863,5 +2172,7 @@ void qib_free_agents(struct qib_ibdev *d
 			ib_destroy_ah(&ibp->sm_ah->ibah);
 			ibp->sm_ah = NULL;
 		}
+		if (dd->pport[p].cong_stats.timer.data)
+			del_timer_sync(&dd->pport[p].cong_stats.timer);
 	}
 }
Only in b/drivers/infiniband/hw/qib: qib_mad.c.orig
diff -up a/drivers/infiniband/hw/qib/qib_mad.h b/drivers/infiniband/hw/qib/qib_mad.h
--- a/drivers/infiniband/hw/qib/qib_mad.h	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_mad.h	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -182,6 +183,7 @@ struct ib_vl_weight_elem {
 /*
  * PMA class portinfo capability mask bits
  */
+#define IB_PMA_CLASS_CAP_ALLPORTSELECT  cpu_to_be16(1 << 8)
 #define IB_PMA_CLASS_CAP_EXT_WIDTH      cpu_to_be16(1 << 9)
 #define IB_PMA_CLASS_CAP_XMIT_WAIT      cpu_to_be16(1 << 12)
 
@@ -191,6 +193,7 @@ struct ib_vl_weight_elem {
 #define IB_PMA_PORT_COUNTERS            cpu_to_be16(0x0012)
 #define IB_PMA_PORT_COUNTERS_EXT        cpu_to_be16(0x001D)
 #define IB_PMA_PORT_SAMPLES_RESULT_EXT  cpu_to_be16(0x001E)
+#define IB_PMA_PORT_COUNTERS_CONG       cpu_to_be16(0xFF00)
 
 struct ib_perf {
 	u8 base_version;
@@ -280,6 +283,39 @@ struct ib_pma_portcounters {
 	__be32 port_rcv_packets;
 } __attribute__ ((packed));
 
+struct ib_pma_portcounters_cong {
+	u8 reserved;
+	u8 reserved1;
+	__be16 port_check_rate;
+	__be16 symbol_error_counter;
+	u8 link_error_recovery_counter;
+	u8 link_downed_counter;
+	__be16 port_rcv_errors;
+	__be16 port_rcv_remphys_errors;
+	__be16 port_rcv_switch_relay_errors;
+	__be16 port_xmit_discards;
+	u8 port_xmit_constraint_errors;
+	u8 port_rcv_constraint_errors;
+	u8 reserved2;
+	u8 lli_ebor_errors;    /* 4, 4, bits */
+	__be16 reserved3;
+	__be16 vl15_dropped;
+	__be64 port_xmit_data;
+	__be64 port_rcv_data;
+	__be64 port_xmit_packets;
+	__be64 port_rcv_packets;
+	__be64 port_xmit_wait;
+	__be64 port_adr_events;
+} __attribute__ ((packed));
+
+#define IB_PMA_CONG_HW_CONTROL_TIMER            0x00
+#define IB_PMA_CONG_HW_CONTROL_SAMPLE           0x01
+
+#define QIB_XMIT_RATE_UNSUPPORTED               0x0
+#define QIB_XMIT_RATE_PICO                      0x7
+/* number of 4nsec cycles equaling 2secs */
+#define QIB_CONG_TIMER_PSINTERVAL               0x1DCD64EC
+
 #define IB_PMA_SEL_SYMBOL_ERROR                 cpu_to_be16(0x0001)
 #define IB_PMA_SEL_LINK_ERROR_RECOVERY          cpu_to_be16(0x0002)
 #define IB_PMA_SEL_LINK_DOWNED                  cpu_to_be16(0x0004)
@@ -294,6 +330,11 @@ struct ib_pma_portcounters {
 #define IB_PMA_SEL_PORT_XMIT_PACKETS            cpu_to_be16(0x4000)
 #define IB_PMA_SEL_PORT_RCV_PACKETS             cpu_to_be16(0x8000)
 
+#define IB_PMA_SEL_CONG_ALL                     0x01
+#define IB_PMA_SEL_CONG_PORT_DATA               0x02
+#define IB_PMA_SEL_CONG_XMIT                    0x04
+#define IB_PMA_SEL_CONG_ROUTING                 0x08
+
 struct ib_pma_portcounters_ext {
 	u8 reserved;
 	u8 port_select;
diff -up a/drivers/infiniband/hw/qib/qib_qp.c b/drivers/infiniband/hw/qib/qib_qp.c
--- a/drivers/infiniband/hw/qib/qib_qp.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_qp.c	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -391,6 +392,9 @@ static void clear_mr_refs(struct qib_qp
 {
 	unsigned n;
 
+	if (test_and_clear_bit(QIB_R_REWIND_SGE, &qp->r_aflags))
+		qp->r_sge = qp->s_rdma_read_sge;
+
 	while (qp->r_sge.num_sge) {
 		atomic_dec(&qp->r_sge.sge.mr->refcount);
 		if (--qp->r_sge.num_sge)
diff -up a/drivers/infiniband/hw/qib/qib_sdma.c b/drivers/infiniband/hw/qib/qib_sdma.c
--- a/drivers/infiniband/hw/qib/qib_sdma.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_sdma.c	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2007, 2008, 2009, 2010 QLogic Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -789,6 +789,15 @@ void __qib_sdma_process_event(struct qib
 		switch (event) {
 		case qib_sdma_event_e00_go_hw_down:
 			break;
+		case qib_sdma_event_e30_go_running:
+			/*
+			 * If down, but running requested (usually result
+			 * of link up, then we need to start up.
+			 * This can happen when hw down is requested while
+			 * bringing the link up with traffic active on
+			 * 7220, e.g. */
+			ss->go_s99_running = 1;
+			/* fall through and start dma engine */
 		case qib_sdma_event_e10_go_hw_start:
 			/* This reference means the state machine is started */
 			sdma_get(&ppd->sdma_state);
@@ -797,8 +806,6 @@ void __qib_sdma_process_event(struct qib
 			break;
 		case qib_sdma_event_e20_hw_started:
 			break;
-		case qib_sdma_event_e30_go_running:
-			break;
 		case qib_sdma_event_e40_sw_cleaned:
 			sdma_sw_tear_down(ppd);
 			break;
@@ -872,14 +879,6 @@ void __qib_sdma_process_event(struct qib
 		case qib_sdma_event_e60_hw_halted:
 			break;
 		case qib_sdma_event_e70_go_idle:
-			/*
-			 * Some chips need sends flushed after we go idle
-			 * in some conditions, in case there are stuck sends
-			 * after hw cleanup
-			 */
-			if (ppd->dd->f_sdma_flush_sends(ppd))
-				sdma_set_state(ppd,
-					qib_sdma_state_s40_hw_clean_up_wait);
 			break;
 		case qib_sdma_event_e7220_err_halted:
 			break;
diff -up a/drivers/infiniband/hw/qib/qib_tx.c b/drivers/infiniband/hw/qib/qib_tx.c
--- a/drivers/infiniband/hw/qib/qib_tx.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_tx.c	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2008, 2009, 2010 QLogic Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -173,10 +173,13 @@ static int find_ctxt(struct qib_devdata
 void qib_disarm_piobufs_set(struct qib_devdata *dd, unsigned long *mask,
 			    unsigned cnt)
 {
-	struct qib_pportdata *ppd;
+	struct qib_pportdata *ppd, *pppd[dd->num_pports];
 	unsigned i;
 	unsigned long flags;
 
+	for (i = 0; i < dd->num_pports; i++)
+		pppd[i] = NULL;
+
 	for (i = 0; i < cnt; i++) {
 		int which;
 		if (!test_bit(i, mask))
@@ -187,9 +190,7 @@ void qib_disarm_piobufs_set(struct qib_d
 		 */
 		ppd = is_sdma_buf(dd, i);
 		if (ppd) {
-			/* should do only once per port */
-			qib_cdbg(ERRPKT, "buf %u sdma; do cancel_sends\n", i);
-			qib_cancel_sends(ppd);
+			pppd[ppd->port] = ppd;
 			continue;
 		}
 		/*
@@ -210,6 +211,14 @@ void qib_disarm_piobufs_set(struct qib_d
 		qib_cdbg(ERRPKT, "disarm buf %u %s\n", i, which ?
 			 "now" : "later");
 	}
+
+	/* do cancel_sends once per port that had sdma piobufs in error */
+	for (i = 0; i < dd->num_pports; i++)
+		if (pppd[i]) {
+			qib_cdbg(ERRPKT, "IB%u:%u sdma bufs; do cancel\n",
+				 dd->unit, pppd[i]->port);
+			qib_cancel_sends(pppd[i]);
+		}
 }
 
 /**
@@ -509,9 +518,7 @@ void qib_cancel_sends(struct qib_pportda
 			spin_unlock_irqrestore(&dd->uctxt_lock, flags);
 	}
 
-	if (dd->flags & QIB_HAS_SEND_DMA)
-		qib_sdma_process_event(ppd, qib_sdma_event_e70_go_idle);
-	else
+	if (!(dd->flags & QIB_HAS_SEND_DMA))
 		dd->f_sendctrl(ppd, QIB_SENDCTRL_DISARM_ALL |
 				    QIB_SENDCTRL_FLUSH);
 }
diff -up a/drivers/infiniband/hw/qib/qib_uc.c b/drivers/infiniband/hw/qib/qib_uc.c
--- a/drivers/infiniband/hw/qib/qib_uc.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_uc.c	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -225,26 +226,6 @@ unlock:
 	return ret;
 }
 
-static void fix_mr_refcount(struct qib_qp *qp)
-{
-	unsigned i;
-
-	if (qp->r_sge.num_sge == qp->s_rdma_read_sge.num_sge)
-		return;
-	while (qp->r_sge.num_sge) {
-		atomic_dec(&qp->r_sge.sge.mr->refcount);
-		if (--qp->r_sge.num_sge)
-			qp->r_sge.sge = *qp->r_sge.sg_list++;
-	}
-	for (i = 0; i < qp->s_rdma_read_sge.num_sge; i++) {
-		struct qib_sge *sge = i ?
-			&qp->s_rdma_read_sge.sg_list[i - 1] :
-			&qp->s_rdma_read_sge.sge;
-
-		atomic_inc(&sge->mr->refcount);
-	}
-}
-
 /**
  * qib_uc_rcv - handle an incoming UC packet
  * @ibp: the port the packet came in on
@@ -372,11 +353,9 @@ inv:
 	case OP(SEND_ONLY):
 	case OP(SEND_ONLY_WITH_IMMEDIATE):
 send_first:
-		if (qp->r_flags & QIB_R_REUSE_SGE) {
-			qp->r_flags &= ~QIB_R_REUSE_SGE;
-			fix_mr_refcount(qp);
+		if (test_and_clear_bit(QIB_R_REWIND_SGE, &qp->r_aflags))
 			qp->r_sge = qp->s_rdma_read_sge;
-		} else {
+		else {
 			ret = qib_get_rwqe(qp, 0);
 			if (ret < 0)
 				goto op_err;
@@ -384,6 +363,8 @@ send_first:
 				ibp->n_pkt_drops++;
 				goto runlock;
 			}
+			/* qp->s_rdma_read_sge will be the owner
+			   of the mr references. */
 			qp->s_rdma_read_sge = qp->r_sge;
 		}
 		qp->r_rcv_len = 0;
@@ -395,17 +376,17 @@ send_first:
 	case OP(SEND_MIDDLE):
 		/* Check for invalid length PMTU or posted rwqe len. */
 		if (unlikely(tlen != (hdrsize + pmtu + 4))) {
-			qp->r_flags |= QIB_R_REUSE_SGE;
+			set_bit(QIB_R_REWIND_SGE, &qp->r_aflags);
 			ibp->n_pkt_drops++;
 			goto runlock;
 		}
 		qp->r_rcv_len += pmtu;
 		if (unlikely(qp->r_rcv_len > qp->r_len)) {
-			qp->r_flags |= QIB_R_REUSE_SGE;
+			set_bit(QIB_R_REWIND_SGE, &qp->r_aflags);
 			ibp->n_pkt_drops++;
 			goto runlock;
 		}
-		qib_copy_sge(&qp->r_sge, data, pmtu, 1);
+		qib_copy_sge(&qp->r_sge, data, pmtu, 0);
 		break;
 
 	case OP(SEND_LAST_WITH_IMMEDIATE):
@@ -421,7 +402,7 @@ send_last:
 		/* Check for invalid length. */
 		/* XXX LAST len should be >= 1 */
 		if (unlikely(tlen < (hdrsize + pad + 4))) {
-			qp->r_flags |= QIB_R_REUSE_SGE;
+			set_bit(QIB_R_REWIND_SGE, &qp->r_aflags);
 			ibp->n_pkt_drops++;
 			goto runlock;
 		}
@@ -429,17 +410,18 @@ send_last:
 		tlen -= (hdrsize + pad + 4);
 		wc.byte_len = tlen + qp->r_rcv_len;
 		if (unlikely(wc.byte_len > qp->r_len)) {
-			qp->r_flags |= QIB_R_REUSE_SGE;
+			set_bit(QIB_R_REWIND_SGE, &qp->r_aflags);
 			ibp->n_pkt_drops++;
 			goto runlock;
 		}
 		wc.opcode = IB_WC_RECV;
 last_imm:
-		qib_copy_sge(&qp->r_sge, data, tlen, 1);
-		while (qp->r_sge.num_sge) {
-			atomic_dec(&qp->r_sge.sge.mr->refcount);
-			if (--qp->r_sge.num_sge)
-				qp->r_sge.sge = *qp->r_sge.sg_list++;
+		qib_copy_sge(&qp->r_sge, data, tlen, 0);
+		while (qp->s_rdma_read_sge.num_sge) {
+			atomic_dec(&qp->s_rdma_read_sge.sge.mr->refcount);
+			if (--qp->s_rdma_read_sge.num_sge)
+				qp->s_rdma_read_sge.sge =
+					*qp->s_rdma_read_sge.sg_list++;
 		}
 		wc.wr_id = qp->r_wr_id;
 		wc.status = IB_WC_SUCCESS;
@@ -526,9 +508,7 @@ rdma_last_imm:
 			ibp->n_pkt_drops++;
 			goto runlock;
 		}
-		if (qp->r_flags & QIB_R_REUSE_SGE)
-			qp->r_flags &= ~QIB_R_REUSE_SGE;
-		else {
+		if (!test_and_clear_bit(QIB_R_REWIND_SGE, &qp->r_aflags)) {
 			ret = qib_get_rwqe(qp, 1);
 			if (ret < 0)
 				goto op_err;
diff -up a/drivers/infiniband/hw/qib/qib_user_sdma.c b/drivers/infiniband/hw/qib/qib_user_sdma.c
--- a/drivers/infiniband/hw/qib/qib_user_sdma.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_user_sdma.c	2010-02-18 10:34:16.000000000 -0800
@@ -49,6 +49,8 @@
 #define QIB_USER_SDMA_MIN_HEADER_LENGTH 64
 /* expected size of headers (for dma_pool) */
 #define QIB_USER_SDMA_EXP_HEADER_LENGTH 64
+/* attempt to drain the queue for 5secs */
+#define QIB_USER_SDMA_DRAIN_TIMEOUT 500
 
 struct qib_user_sdma_pkt {
 	u8 naddr;               /* dimension of addr (1..3) ... */
@@ -637,7 +639,7 @@ void qib_user_sdma_queue_drain(struct qi
 	if (!pq)
 		return;
 
-	for (i = 0; i < 500; i++) {
+	for (i = 0; i < QIB_USER_SDMA_DRAIN_TIMEOUT; i++) {
 		mutex_lock(&pq->lock);
 		if (list_empty(&pq->sent)) {
 			mutex_unlock(&pq->lock);
diff -up a/drivers/infiniband/hw/qib/qib_verbs.c b/drivers/infiniband/hw/qib/qib_verbs.c
--- a/drivers/infiniband/hw/qib/qib_verbs.c	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_verbs.c	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -1816,18 +1817,23 @@ static int qib_modify_port(struct ib_dev
 static int qib_query_gid(struct ib_device *ibdev, u8 port,
 			 int index, union ib_gid *gid)
 {
-	struct qib_ibport *ibp = to_iport(ibdev, port);
-	struct qib_pportdata *ppd = ppd_from_ibp(ibp);
+	struct qib_devdata *dd = dd_from_ibdev(ibdev);
 	int ret = 0;
 
-	gid->global.subnet_prefix = ibp->gid_prefix;
-
-	if (index == 0)
-		gid->global.interface_id = ppd->guid;
-	else if (index < QIB_GUIDS_PER_PORT)
-		gid->global.interface_id = ibp->guids[index - 1];
-	else
+	if (!port || port > dd->num_pports)
 		ret = -EINVAL;
+	else {
+		struct qib_ibport *ibp = to_iport(ibdev, port);
+		struct qib_pportdata *ppd = ppd_from_ibp(ibp);
+
+		gid->global.subnet_prefix = ibp->gid_prefix;
+		if (index == 0)
+			gid->global.interface_id = ppd->guid;
+		else if (index < QIB_GUIDS_PER_PORT)
+			gid->global.interface_id = ibp->guids[index - 1];
+		else
+			ret = -EINVAL;
+	}
 
 	return ret;
 }
@@ -2174,17 +2180,18 @@ int qib_register_ib_device(struct qib_de
 	INIT_LIST_HEAD(&dev->dmawait);
 	INIT_LIST_HEAD(&dev->txwait);
 	INIT_LIST_HEAD(&dev->memwait);
-	INIT_LIST_HEAD(&dev->rnrwait);
 	INIT_LIST_HEAD(&dev->txreq_free);
 
-	dev->pio_hdrs = dma_alloc_coherent(&dd->pcidev->dev,
-					   ppd->sdma_descq_cnt *
-					   sizeof(struct qib_pio_header),
-					   &dev->pio_hdrs_phys,
-					   GFP_KERNEL);
-	if (!dev->pio_hdrs) {
-		ret = -ENOMEM;
-		goto err_hdrs;
+	if (ppd->sdma_descq_cnt) {
+		dev->pio_hdrs = dma_alloc_coherent(&dd->pcidev->dev,
+						ppd->sdma_descq_cnt *
+						sizeof(struct qib_pio_header),
+						&dev->pio_hdrs_phys,
+						GFP_KERNEL);
+		if (!dev->pio_hdrs) {
+			ret = -ENOMEM;
+			goto err_hdrs;
+		}
 	}
 
 	for (i = 0; i < ppd->sdma_descq_cnt; i++) {
@@ -2288,8 +2295,7 @@ int qib_register_ib_device(struct qib_de
 	ibdev->mmap = qib_mmap;
 	ibdev->dma_ops = &qib_dma_mapping_ops;
 
-	snprintf(ibdev->node_desc, sizeof(ibdev->node_desc),
-		 QIB_IDSTR " %s", init_utsname()->nodename);
+	snprintf(ibdev->node_desc, sizeof(ibdev->node_desc), "@");
 
 	ret = ib_register_device(ibdev);
 	if (ret)
@@ -2318,9 +2324,11 @@ err_tx:
 		tx = list_entry(l, struct qib_verbs_txreq, txreq.list);
 		kfree(tx);
 	}
-	dma_free_coherent(&dd->pcidev->dev,
-			  ppd->sdma_descq_cnt * sizeof(struct qib_pio_header),
-			  dev->pio_hdrs, dev->pio_hdrs_phys);
+	if (ppd->sdma_descq_cnt)
+		dma_free_coherent(&dd->pcidev->dev,
+				  ppd->sdma_descq_cnt *
+					sizeof(struct qib_pio_header),
+				  dev->pio_hdrs, dev->pio_hdrs_phys);
 err_hdrs:
 	free_pages((unsigned long) dev->lk_table.table, get_order(lk_tab_size));
 err_lk:
@@ -2352,8 +2360,6 @@ void qib_unregister_ib_device(struct qib
 		qib_dev_err(dd, "txwait list not empty!\n");
 	if (!list_empty(&dev->memwait))
 		qib_dev_err(dd, "memwait list not empty!\n");
-	if (!list_empty(&dev->rnrwait))
-		qib_dev_err(dd, "rnrwait list not empty!\n");
 	if (dev->dma_mr)
 		qib_dev_err(dd, "DMA MR not NULL!\n");
 
@@ -2372,10 +2378,11 @@ void qib_unregister_ib_device(struct qib
 		tx = list_entry(l, struct qib_verbs_txreq, txreq.list);
 		kfree(tx);
 	}
-	dma_free_coherent(&dd->pcidev->dev,
-			  dd->pport->sdma_descq_cnt *
-				  sizeof(struct qib_pio_header),
-			  dev->pio_hdrs, dev->pio_hdrs_phys);
+	if (dd->pport->sdma_descq_cnt)
+		dma_free_coherent(&dd->pcidev->dev,
+				  dd->pport->sdma_descq_cnt *
+					sizeof(struct qib_pio_header),
+				  dev->pio_hdrs, dev->pio_hdrs_phys);
 	lk_tab_size = dev->lk_table.max * sizeof(*dev->lk_table.table);
 	free_pages((unsigned long) dev->lk_table.table,
 		   get_order(lk_tab_size));
diff -up a/drivers/infiniband/hw/qib/qib_verbs.h b/drivers/infiniband/hw/qib/qib_verbs.h
--- a/drivers/infiniband/hw/qib/qib_verbs.h	2010-02-18 10:42:20.000000000 -0800
+++ b/drivers/infiniband/hw/qib/qib_verbs.h	2010-02-18 10:34:16.000000000 -0800
@@ -1,5 +1,6 @@
 /*
- * Copyright (c) 2006, 2007, 2008, 2009 QLogic Corporation. All rights reserved.
+ * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
+ * All rights reserved.
  * Copyright (c) 2005, 2006 PathScale, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -504,6 +505,7 @@ struct qib_qp {
  * Atomic bit definitions for r_aflags.
  */
 #define QIB_R_WRID_VALID        0
+#define QIB_R_REWIND_SGE        1
 
 /*
  * Bit definitions for r_flags.
@@ -720,7 +722,6 @@ struct qib_ibdev {
 	struct qib_pio_header *pio_hdrs;
 	dma_addr_t pio_hdrs_phys;
 	/* list of QPs waiting for RNR timer */
-	struct list_head rnrwait;
 	spinlock_t pending_lock; /* protect wait lists, PMA counters, etc. */
 	unsigned qp_table_size; /* size of the hash table */
 	spinlock_t qpt_lock;
